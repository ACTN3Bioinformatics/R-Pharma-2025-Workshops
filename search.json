[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About This Project",
    "section": "",
    "text": "This repository provides comprehensive documentation of the R/Pharma 2025 conference, including detailed workshop summaries, presentation insights, trend analysis, and a complete catalog of tools and packages discussed.\n\n\n\n19 Workshop Pages - Detailed documentation with prerequisites, learning outcomes, and resources\n30+ Presentations - Summaries from 8 Europe/US sessions and Asia/Pacific presentations\nTrend Analysis - Key themes, future directions, and industry insights\nTools Catalog - A-Z reference of all mentioned packages and technologies\nCareer Guidance - Skills, strategies, and opportunities for R professionals in pharma"
  },
  {
    "objectID": "about.html#overview",
    "href": "about.html#overview",
    "title": "About This Project",
    "section": "",
    "text": "This repository provides comprehensive documentation of the R/Pharma 2025 conference, including detailed workshop summaries, presentation insights, trend analysis, and a complete catalog of tools and packages discussed.\n\n\n\n19 Workshop Pages - Detailed documentation with prerequisites, learning outcomes, and resources\n30+ Presentations - Summaries from 8 Europe/US sessions and Asia/Pacific presentations\nTrend Analysis - Key themes, future directions, and industry insights\nTools Catalog - A-Z reference of all mentioned packages and technologies\nCareer Guidance - Skills, strategies, and opportunities for R professionals in pharma"
  },
  {
    "objectID": "about.html#purpose",
    "href": "about.html#purpose",
    "title": "About This Project",
    "section": "Purpose",
    "text": "Purpose\nThis project was created to:\n\nPreserve conference knowledge for future reference\nHelp attendees review and reinforce learning\nShare insights with those who couldnâ€™t attend\nShowcase industry trends to recruiters and hiring managers\nBuild a resource for the R pharma community"
  },
  {
    "objectID": "about.html#technology-stack",
    "href": "about.html#technology-stack",
    "title": "About This Project",
    "section": "Technology Stack",
    "text": "Technology Stack\nThis site is built using modern web technologies:\n\nQuarto - Scientific publishing system\nGitHub Pages - Free hosting\nGitHub Actions - Automated deployment\nCustom CSS - R blue branding (#276DC2)\nResponsive design - Mobile-friendly layout"
  },
  {
    "objectID": "about.html#about-the-author",
    "href": "about.html#about-the-author",
    "title": "About This Project",
    "section": "About the Author",
    "text": "About the Author\n\n\n\nSzymon Myrta\nIâ€™m passionate about applying data science and bioinformatics to advance pharmaceutical research and precision medicine. This repository reflects my commitment to continuous learning, knowledge sharing, and contributing to the open-source R community in pharma.\nInterests:\n\n8+ years of experience in bioinformatics in pharmaceutical and biotech settings\nNGS data analysis (RNAseq, scRNA-seq, ChIP-seq, TCR/BCR-seq, WES/WGS etc.) [Bioconductor, Seurat, edgeR, DESeq2, limma]\nFunctional genomics data analysis (CRISPR / ORF overexpression screens)\nStrong R programming skills, including development of R packages and Shiny apps\nDeveloper of NGS data analysis pipelines and reproducible research workflows and documentation (RMarkdown, Quarto, Snakemake, Bash, Git, CI/CD, Docker)\nData visualization and interpretation of results [ggplot2, Shiny, ComplexHeatmap]\nBackground in computational biology, cancer genomics, immuno-oncology\nCo-author of multiple peer-reviewed scientific publications in top-tier journals\nInterested in multi-omics data integration, precision medicine, AI-powered analyses\n\nConnect with me:\n  \n\n\nR/Pharma 2025 Workshop Certificates\nI actively participated in 11 workshops at the R/Pharma 2025 conference, gaining hands-on experience with cutting-edge tools and methodologies:\n\n\n  \n\nGetting Started with LLM APIs in R\n\n\n\n  \n\nHands on with Cardinal: Clinical Reporting\n\n\n\n  \n\nIntroduction to Building R Packages\n\n\n\n  \n\nPolars: Blazing Fast Python Framework\n\n\n\n  \n\nData Validation with pointblank\n\n\n\n  \n\nDebugging Stan Programs\n\n\n\n  \n\nClinical Trial Design with rpact\n\n\n\n  \n\nR Validation Discussion\n\n\n\n  \n\nCDISC Dataset-JSON in R & Python\n\n\n\n  \n\nSDTM Programming with sdtm.oak\n\n\n\n  \n\nPython for Clinical Study Reports\n\n\n\nTopics covered: AI/LLM Integration, Clinical Reporting, Package Development, Data Validation, Bayesian Methods, CDISC Standards, and Python for Pharma"
  },
  {
    "objectID": "about.html#acknowledgments",
    "href": "about.html#acknowledgments",
    "title": "About This Project",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nR/Pharma 2025 Conference:\n\nConference organizers and speakers\nWorkshop instructors who shared their expertise\nPresentation authors and their organizations\n\nOpen-Source Community:\n\nPharmaverse initiative\nR Validation Hub\nPosit PBC\nAll package developers"
  },
  {
    "objectID": "about.html#contributing",
    "href": "about.html#contributing",
    "title": "About This Project",
    "section": "Contributing",
    "text": "Contributing\nFound an error or have suggestions for improvements?\n\nOpen an issue on GitHub\nSubmit a pull request with corrections or enhancements\nShare feedback via LinkedIn\n\nAll contributions are welcome and appreciated!"
  },
  {
    "objectID": "about.html#license",
    "href": "about.html#license",
    "title": "About This Project",
    "section": "License",
    "text": "License\nThis project is licensed under the MIT License.\nContent is based on publicly available information from the R/Pharma 2025 conference. All credit for original materials goes to the respective authors and organizations."
  },
  {
    "objectID": "about.html#future-plans",
    "href": "about.html#future-plans",
    "title": "About This Project",
    "section": "Future Plans",
    "text": "Future Plans\nThis repository will continue to evolve:\n\nâœ… Complete all workshop documentation\nâœ… Add presentation summaries\nğŸ”„ Include video links (when available)"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About This Project",
    "section": "Contact",
    "text": "Contact\nFor questions, feedback, or collaboration opportunities:\n\nGitHub Issues: Report an issue\nLinkedIn: Message me\nEmail: Via LinkedIn"
  },
  {
    "objectID": "about.html#support-this-project",
    "href": "about.html#support-this-project",
    "title": "About This Project",
    "section": "Support This Project",
    "text": "Support This Project\nIf you find this resource valuable:\n\nâ­ Star the repository on GitHub\nğŸ”— Share with your network\nğŸ’¬ Provide feedback for improvements\nğŸ¤ Contribute documentation or corrections\n\n\nThank you for visiting! I hope this resource helps you in your R pharma journey.\n\nLast updated: November 2025\nRepository: github.com/ACTN3Bioinformatics/R-Pharma-2025-Workshops\nWebsite: actn3bioinformatics.github.io/R-Pharma-2025-Workshops"
  },
  {
    "objectID": "workshops/specialized/teal-framework.html",
    "href": "workshops/specialized/teal-framework.html",
    "title": "From Data to Insights: A Hands-On Workshop with {teal}",
    "section": "",
    "text": "Intermediate Shiny Interactive Apps\n{teal} is an innovative open-source R-Shiny framework that has transformed clinical trial data analysis and visualization. This hands-on workshop, built on the latest {teal} 1.0 release, will progressively cover practical topics for building interactive applications that deliver insights faster while promoting efficiency, transparency, and reproducibility.\n\n\n\nğŸ¯ {teal} fundamentals - Core concepts and architecture\nğŸ§© Building blocks - Modules and components\nğŸ“Š Clinical applications - Real-world use cases\nğŸ”§ Hands-on exercises - Step-by-step building\nğŸš€ Best practices - Production-ready apps",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "From Data to Insights: A Hands-On Workshop with {teal}"
    ]
  },
  {
    "objectID": "workshops/specialized/teal-framework.html#overview",
    "href": "workshops/specialized/teal-framework.html#overview",
    "title": "From Data to Insights: A Hands-On Workshop with {teal}",
    "section": "",
    "text": "Intermediate Shiny Interactive Apps\n{teal} is an innovative open-source R-Shiny framework that has transformed clinical trial data analysis and visualization. This hands-on workshop, built on the latest {teal} 1.0 release, will progressively cover practical topics for building interactive applications that deliver insights faster while promoting efficiency, transparency, and reproducibility.\n\n\n\nğŸ¯ {teal} fundamentals - Core concepts and architecture\nğŸ§© Building blocks - Modules and components\nğŸ“Š Clinical applications - Real-world use cases\nğŸ”§ Hands-on exercises - Step-by-step building\nğŸš€ Best practices - Production-ready apps",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "From Data to Insights: A Hands-On Workshop with {teal}"
    ]
  },
  {
    "objectID": "workshops/specialized/teal-framework.html#prerequisites",
    "href": "workshops/specialized/teal-framework.html#prerequisites",
    "title": "From Data to Insights: A Hands-On Workshop with {teal}",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRequired Knowledge:\n\nBasic R programming\nUnderstanding of clinical trial data (CDISC helpful)\nNo prior Shiny or {teal} experience required!\n\nAll R users welcome - from beginners to experienced developers",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "From Data to Insights: A Hands-On Workshop with {teal}"
    ]
  },
  {
    "objectID": "workshops/specialized/teal-framework.html#key-tools",
    "href": "workshops/specialized/teal-framework.html#key-tools",
    "title": "From Data to Insights: A Hands-On Workshop with {teal}",
    "section": "Key Tools",
    "text": "Key Tools\n\n{teal}\n\n\n{teal.modules.clinical}\n\n\n{teal.data}\n\n\nShiny",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "From Data to Insights: A Hands-On Workshop with {teal}"
    ]
  },
  {
    "objectID": "workshops/specialized/teal-framework.html#workshop-materials",
    "href": "workshops/specialized/teal-framework.html#workshop-materials",
    "title": "From Data to Insights: A Hands-On Workshop with {teal}",
    "section": "Workshop Materials",
    "text": "Workshop Materials\n\n\n\n\n\n\nNoteResources\n\n\n\nGitHub Workshop: github.com/pharmaverse/tealworkshop-rinpharma2025\n{teal} Website: insightsengineering.github.io/teal",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "From Data to Insights: A Hands-On Workshop with {teal}"
    ]
  },
  {
    "objectID": "workshops/specialized/teal-framework.html#what-is-teal",
    "href": "workshops/specialized/teal-framework.html#what-is-teal",
    "title": "From Data to Insights: A Hands-On Workshop with {teal}",
    "section": "What is {teal}?",
    "text": "What is {teal}?\n\nFramework Overview\n{teal} provides a modular framework for building interactive data exploration applications:\n\nğŸ§© Modular - Plug-and-play components\nğŸ”„ Reactive - Built on Shiny\nğŸ“Š Clinical-focused - Pharma use cases\nâœ… Validated - GxP-ready\nğŸŒ Pharmaverse - Part of ecosystem\n\n\n\nKey Features\nData Management:\n\nMultiple dataset support\nFiltering and subsetting\nData relationships\nReproducible states\n\nModules:\n\nPre-built clinical modules\nCustom module creation\nModule composition\nShared state management\n\nReproducibility:\n\nCode generation\nSession state export\nReport generation\nAudit trails",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "From Data to Insights: A Hands-On Workshop with {teal}"
    ]
  },
  {
    "objectID": "workshops/specialized/teal-framework.html#the-teal-1.0-release",
    "href": "workshops/specialized/teal-framework.html#the-teal-1.0-release",
    "title": "From Data to Insights: A Hands-On Workshop with {teal}",
    "section": "The {teal} 1.0 Release",
    "text": "The {teal} 1.0 Release\n\nWhatâ€™s New\n\nâœ¨ Stable API\nğŸ“¦ Improved module system\nğŸ¨ Better UI/UX\nğŸ“š Enhanced documentation\nğŸ”§ Performance improvements\n\n\n\nEcosystem\nCore packages:\n\n{teal} - Framework core\n{teal.data} - Data management\n{teal.code} - Code tracking\n{teal.modules.clinical} - Clinical modules\n{teal.modules.general} - General modules",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "From Data to Insights: A Hands-On Workshop with {teal}"
    ]
  },
  {
    "objectID": "workshops/specialized/teal-framework.html#building-your-first-teal-app",
    "href": "workshops/specialized/teal-framework.html#building-your-first-teal-app",
    "title": "From Data to Insights: A Hands-On Workshop with {teal}",
    "section": "Building Your First {teal} App",
    "text": "Building Your First {teal} App\n\nMinimal Example\nlibrary(teal)\n\n# Create app\napp &lt;- init(\n  data = teal_data(ADSL = pharmaverseadam::adsl),\n  modules = modules(\n    tm_data_table(\"Data Table\"),\n    tm_variable_browser(\"Variable Browser\")\n  )\n)\n\n# Run\nshinyApp(app$ui, app$server)\n\n\nWith Clinical Modules\nlibrary(teal)\nlibrary(teal.modules.clinical)\n\n# Prepare data\ndata &lt;- teal_data(\n  ADSL = pharmaverseadam::adsl,\n  ADAE = pharmaverseadam::adae,\n  ADTTE = pharmaverseadam::adtte\n)\n\n# Build app\napp &lt;- init(\n  data = data,\n  modules = modules(\n    # Demographics\n    tm_t_summary(\n      label = \"Demographics Table\",\n      dataname = \"ADSL\",\n      arm_var = choices_selected(\"ARM\", \"ARM\"),\n      summarize_vars = choices_selected(\n        c(\"AGE\", \"SEX\", \"RACE\"),\n        c(\"AGE\", \"SEX\")\n      )\n    ),\n    \n    # Adverse Events\n    tm_t_events_summary(\n      label = \"Adverse Events\",\n      dataname = \"ADAE\",\n      arm_var = choices_selected(\"ARM\", \"ARM\"),\n      flag_var_anl = choices_selected(\"ANL01FL\", \"ANL01FL\")\n    ),\n    \n    # Kaplan-Meier\n    tm_g_km(\n      label = \"Survival Analysis\",\n      dataname = \"ADTTE\",\n      arm_var = choices_selected(\"ARM\", \"ARM\"),\n      paramcd = choices_selected(\"PARAMCD\", \"OS\")\n    )\n  ),\n  title = \"Clinical Trial Explorer\"\n)\n\nshinyApp(app$ui, app$server)",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "From Data to Insights: A Hands-On Workshop with {teal}"
    ]
  },
  {
    "objectID": "workshops/specialized/teal-framework.html#workshop-exercises",
    "href": "workshops/specialized/teal-framework.html#workshop-exercises",
    "title": "From Data to Insights: A Hands-On Workshop with {teal}",
    "section": "Workshop Exercises",
    "text": "Workshop Exercises\n\nExercise 1: Basic App (30 min)\nGoal: Create simple data exploration app\nTasks:\n\nLoad ADSL dataset\nAdd data table module\nAdd variable browser\nRun and explore\n\n\n\nExercise 2: Demographics Module (45 min)\nGoal: Build demographics summary\nTasks:\n\nConfigure demographics module\nSelect variables to summarize\nAdd treatment arm comparison\nGenerate table\n\n\n\nExercise 3: Adverse Events (45 min)\nGoal: Create AE summary app\nTasks:\n\nLoad ADAE dataset\nConfigure AE module\nFilter by severity\nAdd system organ class view\n\n\n\nExercise 4: Survival Analysis (45 min)\nGoal: Add Kaplan-Meier plots\nTasks:\n\nLoad ADTTE dataset\nConfigure KM module\nAdd risk tables\nCustomize plot appearance\n\n\n\nExercise 5: Complete Clinical App (60 min)\nGoal: Combine everything\nTasks:\n\nMultiple modules\nData filtering\nCustom styling\nCode generation\nExport capabilities",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "From Data to Insights: A Hands-On Workshop with {teal}"
    ]
  },
  {
    "objectID": "workshops/specialized/teal-framework.html#advanced-topics",
    "href": "workshops/specialized/teal-framework.html#advanced-topics",
    "title": "From Data to Insights: A Hands-On Workshop with {teal}",
    "section": "Advanced Topics",
    "text": "Advanced Topics\n\nCustom Modules\nCreate your own {teal} modules:\ntm_my_custom_module &lt;- function(label, dataname) {\n  module(\n    label = label,\n    server = function(input, output, session, data) {\n      # Your server logic\n    },\n    ui = function(id) {\n      # Your UI\n    }\n  )\n}\n\n\nData Filtering\nGlobal filters across all modules:\napp &lt;- init(\n  data = data,\n  modules = modules(...),\n  filter = teal_slices(\n    teal_slice(dataname = \"ADSL\", varname = \"SAFFL\", selected = \"Y\"),\n    teal_slice(dataname = \"ADAE\", varname = \"AESER\", selected = \"Y\")\n  )\n)\n\n\nCode Generation\nUsers can extract R code for reproducibility:\n# Built-in \"Show R Code\" button\n# Generates exact code to reproduce current view\n# Can be run independently",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "From Data to Insights: A Hands-On Workshop with {teal}"
    ]
  },
  {
    "objectID": "workshops/specialized/teal-framework.html#real-world-applications",
    "href": "workshops/specialized/teal-framework.html#real-world-applications",
    "title": "From Data to Insights: A Hands-On Workshop with {teal}",
    "section": "Real-World Applications",
    "text": "Real-World Applications\n\nClinical Study Reports\n\nInteractive data review\nAd-hoc analyses\nQuality checks\nStakeholder presentations\n\n\n\nSafety Monitoring\n\nReal-time AE tracking\nSignal detection\nDose-escalation decisions\nDSMB reports\n\n\n\nEfficacy Analysis\n\nEndpoint visualization\nSubgroup exploration\nSensitivity analyses\nForest plots\n\n\n\nExploratory Analysis\n\nHypothesis generation\nBiomarker discovery\nPopulation characterization\nCovariate relationships",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "From Data to Insights: A Hands-On Workshop with {teal}"
    ]
  },
  {
    "objectID": "workshops/specialized/teal-framework.html#benefits-of-teal",
    "href": "workshops/specialized/teal-framework.html#benefits-of-teal",
    "title": "From Data to Insights: A Hands-On Workshop with {teal}",
    "section": "Benefits of {teal}",
    "text": "Benefits of {teal}\n\nFor Statisticians\n\nâœ… Fast exploration without coding\nâœ… Reproducible analyses\nâœ… Code generation\nâœ… Publication-ready outputs\n\n\n\nFor Programmers\n\nâœ… Modular development\nâœ… Reusable components\nâœ… Validated framework\nâœ… Extensible architecture\n\n\n\nFor Organizations\n\nâœ… Consistency across studies\nâœ… Reduced development time\nâœ… GxP compliance ready\nâœ… Lower training burden",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "From Data to Insights: A Hands-On Workshop with {teal}"
    ]
  },
  {
    "objectID": "workshops/specialized/teal-framework.html#deployment",
    "href": "workshops/specialized/teal-framework.html#deployment",
    "title": "From Data to Insights: A Hands-On Workshop with {teal}",
    "section": "Deployment",
    "text": "Deployment\n\nLocal\n# Run locally\nshinyApp(app$ui, app$server)\n\n\nPosit Connect\n# Deploy to Connect\nrsconnect::deployApp(\n  appDir = \"path/to/app\",\n  appTitle = \"Clinical Trial Explorer\"\n)\n\n\nDocker\nPackage as Docker container for enterprise deployment.",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "From Data to Insights: A Hands-On Workshop with {teal}"
    ]
  },
  {
    "objectID": "workshops/specialized/teal-framework.html#best-practices",
    "href": "workshops/specialized/teal-framework.html#best-practices",
    "title": "From Data to Insights: A Hands-On Workshop with {teal}",
    "section": "Best Practices",
    "text": "Best Practices\n\nDesign\n\nKeep modules focused\nUse meaningful labels\nProvide helpful tooltips\nTest with real users\n\n\n\nPerformance\n\nPre-process data when possible\nUse reactive expressions wisely\nImplement caching\nMonitor memory usage\n\n\n\nValidation\n\nDocument module behavior\nTest edge cases\nVersion control\nMaintain change log",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "From Data to Insights: A Hands-On Workshop with {teal}"
    ]
  },
  {
    "objectID": "workshops/specialized/teal-framework.html#learning-outcomes",
    "href": "workshops/specialized/teal-framework.html#learning-outcomes",
    "title": "From Data to Insights: A Hands-On Workshop with {teal}",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nâœ… Understand {teal} architecture\nâœ… Build basic {teal} applications\nâœ… Use clinical modules effectively\nâœ… Create custom modules\nâœ… Implement data filtering\nâœ… Deploy production-ready apps\nâœ… Ensure reproducibility",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "From Data to Insights: A Hands-On Workshop with {teal}"
    ]
  },
  {
    "objectID": "workshops/specialized/teal-framework.html#resources-1",
    "href": "workshops/specialized/teal-framework.html#resources-1",
    "title": "From Data to Insights: A Hands-On Workshop with {teal}",
    "section": "Resources",
    "text": "Resources\nDocumentation:\n\n{teal} website with vignettes\nModule reference guides\nAPI documentation\n\nCommunity:\n\nPharmaverse Slack (#teal channel)\nGitHub discussions\nMonthly office hours\n\nExamples:\n\nGallery of {teal} apps\nTemplate repositories\nUse case studies",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "From Data to Insights: A Hands-On Workshop with {teal}"
    ]
  },
  {
    "objectID": "workshops/specialized/teal-framework.html#next-steps",
    "href": "workshops/specialized/teal-framework.html#next-steps",
    "title": "From Data to Insights: A Hands-On Workshop with {teal}",
    "section": "Next Steps",
    "text": "Next Steps\nAfter the workshop:\n\nPractice - Build an app for your data\nExplore - Try different modules\nCustomize - Create custom modules\nDeploy - Share with your team\nContribute - Join pharmaverse community\n\n\n\nSimilar Workshops\n\nCardinal: Clinical Reporting - Related to TFLs\nHPC Cluster with Shiny - Advanced Shiny\n\n\n\nRelated Presentations\n\nadmiralâ€™s Journey to Stability - Related pharmaverse package\n\n\n\nNext Steps\n\nFor validation: Validating Shiny Apps presentation\nCareer skills: Shiny Development\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "From Data to Insights: A Hands-On Workshop with {teal}"
    ]
  },
  {
    "objectID": "workshops/specialized/sdtm-oak.html",
    "href": "workshops/specialized/sdtm-oak.html",
    "title": "SDTM Programming in R using {sdtm.oak}",
    "section": "",
    "text": "Intermediate SDTM CDISC\n{sdtm.oak} is an EDC and data standard-agnostic solution designed to empower pharmaceutical programmers to develop CDISC SDTM datasets using R. The package offers a modular programming framework with reusable algorithms that can potentially automate SDTM creation based on standard specifications.\n\n\n\nğŸŒ³ {sdtm.oak} fundamentals - V0.1 package overview\nğŸ”§ Modular programming - Reusable algorithm approach\nğŸ“Š SDTM creation - From raw data to standard domains\nğŸ“ EDC-agnostic design - Works with any data source\nğŸ¯ Automation potential - Standards-based generation",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "SDTM Programming in R using {sdtm.oak}"
    ]
  },
  {
    "objectID": "workshops/specialized/sdtm-oak.html#overview",
    "href": "workshops/specialized/sdtm-oak.html#overview",
    "title": "SDTM Programming in R using {sdtm.oak}",
    "section": "",
    "text": "Intermediate SDTM CDISC\n{sdtm.oak} is an EDC and data standard-agnostic solution designed to empower pharmaceutical programmers to develop CDISC SDTM datasets using R. The package offers a modular programming framework with reusable algorithms that can potentially automate SDTM creation based on standard specifications.\n\n\n\nğŸŒ³ {sdtm.oak} fundamentals - V0.1 package overview\nğŸ”§ Modular programming - Reusable algorithm approach\nğŸ“Š SDTM creation - From raw data to standard domains\nğŸ“ EDC-agnostic design - Works with any data source\nğŸ¯ Automation potential - Standards-based generation",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "SDTM Programming in R using {sdtm.oak}"
    ]
  },
  {
    "objectID": "workshops/specialized/sdtm-oak.html#prerequisites",
    "href": "workshops/specialized/sdtm-oak.html#prerequisites",
    "title": "SDTM Programming in R using {sdtm.oak}",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRequired Knowledge:\n\nIntermediate R programming\nCDISC SDTM standards familiarity\nClinical trial data structure understanding\n\nHelpful:\n\nExperience with SDTM programming\nKnowledge of EDC systems",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "SDTM Programming in R using {sdtm.oak}"
    ]
  },
  {
    "objectID": "workshops/specialized/sdtm-oak.html#key-package",
    "href": "workshops/specialized/sdtm-oak.html#key-package",
    "title": "SDTM Programming in R using {sdtm.oak}",
    "section": "Key Package",
    "text": "Key Package\n\n{sdtm.oak}\n\n\nPharmaverse",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "SDTM Programming in R using {sdtm.oak}"
    ]
  },
  {
    "objectID": "workshops/specialized/sdtm-oak.html#workshop-materials",
    "href": "workshops/specialized/sdtm-oak.html#workshop-materials",
    "title": "SDTM Programming in R using {sdtm.oak}",
    "section": "Workshop Materials",
    "text": "Workshop Materials\n\n\n\n\n\n\nNoteResources\n\n\n\nWorkshop Slides: pharmaverse.github.io/rinpharma-SDTM-workshop\nR Environment: Provided for workshop participants (no pre-installation needed)",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "SDTM Programming in R using {sdtm.oak}"
    ]
  },
  {
    "objectID": "workshops/specialized/sdtm-oak.html#package-philosophy",
    "href": "workshops/specialized/sdtm-oak.html#package-philosophy",
    "title": "SDTM Programming in R using {sdtm.oak}",
    "section": "Package Philosophy",
    "text": "Package Philosophy\n\nEDC-Agnostic Approach\nWorks with data from any source:\n\nâœ… Medidata Rave\nâœ… Oracle Clinical\nâœ… Veeva Vault\nâœ… Custom EDC systems\nâœ… CSV files\n\n\n\nReusable Algorithms\nInstead of custom code for each study:\n\nDefine once, use many times\nStandards-based transformations\nMetadata-driven approach\n\n\n\nModular Framework\nBreak SDTM creation into components:\n\nData reading\nDomain mapping\nVariable derivation\nStandardization",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "SDTM Programming in R using {sdtm.oak}"
    ]
  },
  {
    "objectID": "workshops/specialized/sdtm-oak.html#key-features",
    "href": "workshops/specialized/sdtm-oak.html#key-features",
    "title": "SDTM Programming in R using {sdtm.oak}",
    "section": "Key Features",
    "text": "Key Features\n\n1. Domain Creation\nlibrary(sdtm.oak)\n\n# Create Demographics (DM) domain\ndm &lt;- create_dm(\n  raw_data = raw_subjects,\n  spec = dm_specification\n)\n\n\n2. Variable Mapping\nAutomatic mapping from raw to SDTM:\n\nUSUBJID derivation\nDate standardization\nControlled terminology\nUnit conversions\n\n\n\n3. Validation\nBuilt-in checks for:\n\nRequired variables\nData types\nValue constraints\nSDTM conformance\n\n\n\n4. Metadata-Driven\nUse SDTM specifications as input:\n\nExcel specification sheets\nStandard templates\nReusable across studies",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "SDTM Programming in R using {sdtm.oak}"
    ]
  },
  {
    "objectID": "workshops/specialized/sdtm-oak.html#workshop-content",
    "href": "workshops/specialized/sdtm-oak.html#workshop-content",
    "title": "SDTM Programming in R using {sdtm.oak}",
    "section": "Workshop Content",
    "text": "Workshop Content\n\nModule 1: Package Introduction\n\n{sdtm.oak} architecture\nInstallation and setup\nKey functions overview\nIntegration with pharmaverse\n\n\n\nModule 2: Simple Domain Creation\nHands-on: Demographics (DM)\nCreate DM domain from raw subject data:\n\nSubject identifiers\nDemographics variables\nDates formatting\nControlled terminology\n\n\n\nModule 3: Complex Domains\nHands-on: Adverse Events (AE)\n\nMultiple records per subject\nStart/end dates\nSeverity grading\nRelationship coding\n\n\n\nModule 4: Automation\n\nUsing specifications to drive creation\nBatch processing multiple domains\nQuality checks\nDocumentation generation",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "SDTM Programming in R using {sdtm.oak}"
    ]
  },
  {
    "objectID": "workshops/specialized/sdtm-oak.html#practical-example",
    "href": "workshops/specialized/sdtm-oak.html#practical-example",
    "title": "SDTM Programming in R using {sdtm.oak}",
    "section": "Practical Example",
    "text": "Practical Example\nlibrary(sdtm.oak)\nlibrary(dplyr)\n\n# Load raw data\nraw_ae &lt;- read_raw_data(\"adverse_events.csv\")\n\n# Define SDTM specification\nae_spec &lt;- read_specification(\"SDTM_AE_spec.xlsx\")\n\n# Create AE domain\nae &lt;- raw_ae %&gt;%\n  create_sdtm_domain(\n    domain = \"AE\",\n    spec = ae_spec,\n    mappings = list(\n      USUBJID = derive_usubjid(SUBJID, SITEID),\n      AESTDTC = format_date(AE_START_DATE),\n      AEENDTC = format_date(AE_END_DATE),\n      AEDECOD = standardize_term(AE_TERM, dictionary = \"MedDRA\"),\n      AESEV = map_severity(AE_SEVERITY)\n    )\n  ) %&gt;%\n  validate_domain(\"AE\")\n\n# Export to XPT\nwrite_xpt(ae, \"ae.xpt\")",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "SDTM Programming in R using {sdtm.oak}"
    ]
  },
  {
    "objectID": "workshops/specialized/sdtm-oak.html#benefits",
    "href": "workshops/specialized/sdtm-oak.html#benefits",
    "title": "SDTM Programming in R using {sdtm.oak}",
    "section": "Benefits",
    "text": "Benefits\n\nFor Programmers\n\nLess custom coding - Reuse algorithms\nFaster development - Metadata-driven\nFewer errors - Automated validation\nBetter documentation - Standards-based\n\n\n\nFor Organizations\n\nConsistency across studies\nEfficiency gains\nQuality improvements\nCompliance with CDISC\n\n\n\nFor Industry\n\nOpen-source collaboration\nShared algorithms\nBest practices dissemination",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "SDTM Programming in R using {sdtm.oak}"
    ]
  },
  {
    "objectID": "workshops/specialized/sdtm-oak.html#learning-outcomes",
    "href": "workshops/specialized/sdtm-oak.html#learning-outcomes",
    "title": "SDTM Programming in R using {sdtm.oak}",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nâœ… Understand {sdtm.oak} architecture\nâœ… Create simple SDTM domains\nâœ… Handle complex domain mappings\nâœ… Use specifications to drive automation\nâœ… Validate SDTM datasets\nâœ… Integrate with pharmaverse ecosystem",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "SDTM Programming in R using {sdtm.oak}"
    ]
  },
  {
    "objectID": "workshops/specialized/sdtm-oak.html#future-roadmap",
    "href": "workshops/specialized/sdtm-oak.html#future-roadmap",
    "title": "SDTM Programming in R using {sdtm.oak}",
    "section": "Future Roadmap",
    "text": "Future Roadmap\nPlanned Features:\n\nMore domain templates\nEnhanced automation\nIntegration with {admiral}\nSpecification validator\nMachine learning-assisted mapping",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "SDTM Programming in R using {sdtm.oak}"
    ]
  },
  {
    "objectID": "workshops/specialized/sdtm-oak.html#getting-help",
    "href": "workshops/specialized/sdtm-oak.html#getting-help",
    "title": "SDTM Programming in R using {sdtm.oak}",
    "section": "Getting Help",
    "text": "Getting Help\n\nGitHub Issues: Report bugs and requests\nPharmaverse Slack: #sdtm-oak channel\nDocumentation: Package website\nExamples: Vignettes and demos\n\n\n\nSimilar Workshops\n\ndatasetjson - Modern CDISC formats\nBuilding R Packages - Package structure\n\n\n\nRelated Presentations\n\nMosaic: ARS-Driven Automation - SDTM to TFLs\n\n\n\nNext Steps\n\nAfter SDTM: Learn Cardinal for TFLs\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "SDTM Programming in R using {sdtm.oak}"
    ]
  },
  {
    "objectID": "workshops/development-validation/r-validation-discussion.html",
    "href": "workshops/development-validation/r-validation-discussion.html",
    "title": "R Validation Discussion: Metric Repos for Open Quality Assessment",
    "section": "",
    "text": "Advanced Validation GxP\nThe R Validation Hub has built a Metric Repository - a pre-built database of metrics to support the software validation process. This discussion workshop explores the industry outlook for this open validation database and gathers community input on standardization and implementation.\n\n\n\nğŸ—„ï¸ Metric Repository overview\nğŸ”§ Standardizing compute environments\nğŸ“Š Quality standards for packages\nğŸ¢ In-house package validation\nğŸ”® Future of R validation",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "R Validation Discussion: Metric Repos for Open Quality Assessment"
    ]
  },
  {
    "objectID": "workshops/development-validation/r-validation-discussion.html#overview",
    "href": "workshops/development-validation/r-validation-discussion.html#overview",
    "title": "R Validation Discussion: Metric Repos for Open Quality Assessment",
    "section": "",
    "text": "Advanced Validation GxP\nThe R Validation Hub has built a Metric Repository - a pre-built database of metrics to support the software validation process. This discussion workshop explores the industry outlook for this open validation database and gathers community input on standardization and implementation.\n\n\n\nğŸ—„ï¸ Metric Repository overview\nğŸ”§ Standardizing compute environments\nğŸ“Š Quality standards for packages\nğŸ¢ In-house package validation\nğŸ”® Future of R validation",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "R Validation Discussion: Metric Repos for Open Quality Assessment"
    ]
  },
  {
    "objectID": "workshops/development-validation/r-validation-discussion.html#key-packages",
    "href": "workshops/development-validation/r-validation-discussion.html#key-packages",
    "title": "R Validation Discussion: Metric Repos for Open Quality Assessment",
    "section": "Key Packages",
    "text": "Key Packages\n\n{riskmetric}\n\n\n{riskassessment}\n\n\n{riskscore}\n\n\n{val.meter}\n\n\n{val.criterion}",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "R Validation Discussion: Metric Repos for Open Quality Assessment"
    ]
  },
  {
    "objectID": "workshops/development-validation/r-validation-discussion.html#workshop-links",
    "href": "workshops/development-validation/r-validation-discussion.html#workshop-links",
    "title": "R Validation Discussion: Metric Repos for Open Quality Assessment",
    "section": "Workshop Links",
    "text": "Workshop Links\n\n\n\n\n\n\nNoteResources\n\n\n\nRegulatory Repo WG: github.com/pharmaR/regulatory-r-repo-wg\nval.meter: github.com/pharmaR/val.meter\nval.criterion: github.com/pharmaR/val.criterion",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "R Validation Discussion: Metric Repos for Open Quality Assessment"
    ]
  },
  {
    "objectID": "workshops/development-validation/r-validation-discussion.html#key-questions",
    "href": "workshops/development-validation/r-validation-discussion.html#key-questions",
    "title": "R Validation Discussion: Metric Repos for Open Quality Assessment",
    "section": "Key Questions",
    "text": "Key Questions\n\n1. Environment Standardization\nHow do we standardize compute environments to make metrics useful across organizations?\n\n\n2. Quality Thresholds\nWhat should we do when a package doesnâ€™t meet our standards?\n\n\n3. In-House Packages\nHow can organizations supplement the database with validation of internal packages?\n\n\n4. Industry Adoption\nWhat barriers prevent adoption of shared validation databases?",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "R Validation Discussion: Metric Repos for Open Quality Assessment"
    ]
  },
  {
    "objectID": "workshops/development-validation/r-validation-discussion.html#metric-repository-concept",
    "href": "workshops/development-validation/r-validation-discussion.html#metric-repository-concept",
    "title": "R Validation Discussion: Metric Repos for Open Quality Assessment",
    "section": "Metric Repository Concept",
    "text": "Metric Repository Concept\nlibrary(riskmetric)\nlibrary(riskscore)\n\n# Assess package risk\npkg_ref &lt;- pkg_ref(\"dplyr\")\npkg_risk &lt;- pkg_assess(pkg_ref)\n\n# Calculate risk score\nscore &lt;- riskscore(pkg_risk)\n\n# Store in shared database\n# Organizations can query and contribute",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "R Validation Discussion: Metric Repos for Open Quality Assessment"
    ]
  },
  {
    "objectID": "workshops/development-validation/r-validation-discussion.html#learning-outcomes",
    "href": "workshops/development-validation/r-validation-discussion.html#learning-outcomes",
    "title": "R Validation Discussion: Metric Repos for Open Quality Assessment",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nâœ… Understand metric repository approach\nâœ… Contribute to standardization efforts\nâœ… Plan validation strategy for your organization\nâœ… Network with industry validation experts\n\n\nSimilar Workshops\n\nBuilding R Packages - Create packages to validate\npointblank - Data validation\n\n\n\nNext Steps\n\nIndustry standards: Validation Maturity trends\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "R Validation Discussion: Metric Repos for Open Quality Assessment"
    ]
  },
  {
    "objectID": "workshops/ai-llm/llm-clinical-data-privacy.html",
    "href": "workshops/ai-llm/llm-clinical-data-privacy.html",
    "title": "Integrating LLM using R Shiny for Clinical Data Review",
    "section": "",
    "text": "Intermediate AI/LLM Shiny Data Privacy\nThe pharmaceutical industry is shifting from traditional SAS-based workflows toward the open-source R ecosystem. This workshop presents {DataChat}, an innovative R Shiny application that enables users to â€œchat with dataâ€ through a conversational interface while maintaining strict compliance with data privacy requirements and statistical validity standards.\n\n\n\nğŸ›¡ï¸ Data privacy in LLM applications\nğŸ’¬ Conversational interfaces for clinical data\nğŸ” RAG (Retrieval-Augmented Generation) for pharma domain\nâœ… Statistical validity in AI-generated results\nğŸ¯ User-friendly design for non-programmers",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Integrating LLM using R Shiny for Clinical Data Review"
    ]
  },
  {
    "objectID": "workshops/ai-llm/llm-clinical-data-privacy.html#overview",
    "href": "workshops/ai-llm/llm-clinical-data-privacy.html#overview",
    "title": "Integrating LLM using R Shiny for Clinical Data Review",
    "section": "",
    "text": "Intermediate AI/LLM Shiny Data Privacy\nThe pharmaceutical industry is shifting from traditional SAS-based workflows toward the open-source R ecosystem. This workshop presents {DataChat}, an innovative R Shiny application that enables users to â€œchat with dataâ€ through a conversational interface while maintaining strict compliance with data privacy requirements and statistical validity standards.\n\n\n\nğŸ›¡ï¸ Data privacy in LLM applications\nğŸ’¬ Conversational interfaces for clinical data\nğŸ” RAG (Retrieval-Augmented Generation) for pharma domain\nâœ… Statistical validity in AI-generated results\nğŸ¯ User-friendly design for non-programmers",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Integrating LLM using R Shiny for Clinical Data Review"
    ]
  },
  {
    "objectID": "workshops/ai-llm/llm-clinical-data-privacy.html#prerequisites",
    "href": "workshops/ai-llm/llm-clinical-data-privacy.html#prerequisites",
    "title": "Integrating LLM using R Shiny for Clinical Data Review",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRequired Knowledge:\n\nIntermediate R and Shiny\nBasic understanding of clinical trial data structures\nFamiliarity with data privacy regulations (GDPR, HIPAA)\n\nTechnical Setup:\n\nR/RStudio with Shiny\nAccess to sample clinical datasets",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Integrating LLM using R Shiny for Clinical Data Review"
    ]
  },
  {
    "objectID": "workshops/ai-llm/llm-clinical-data-privacy.html#key-packages-tools",
    "href": "workshops/ai-llm/llm-clinical-data-privacy.html#key-packages-tools",
    "title": "Integrating LLM using R Shiny for Clinical Data Review",
    "section": "Key Packages & Tools",
    "text": "Key Packages & Tools\n\n{ellmer}\n\n\n{shinychat}\n\n\n{ragnar}\n\n\n{shiny}\n\n\nInternal statistical tools",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Integrating LLM using R Shiny for Clinical Data Review"
    ]
  },
  {
    "objectID": "workshops/ai-llm/llm-clinical-data-privacy.html#the-challenge",
    "href": "workshops/ai-llm/llm-clinical-data-privacy.html#the-challenge",
    "title": "Integrating LLM using R Shiny for Clinical Data Review",
    "section": "The Challenge",
    "text": "The Challenge\nTraditional R Shiny applications for clinical data often require:\n\nğŸ“š Strong understanding of data structures (SDTM, ADaM)\nğŸ–±ï¸ Familiarity with complex UI components (dropdowns, filters)\nğŸ’» Programming knowledge for data exploration\n\nThis creates barriers for clinical reviewers, physicians, and medical writers who need to access insights but lack technical expertise.",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Integrating LLM using R Shiny for Clinical Data Review"
    ]
  },
  {
    "objectID": "workshops/ai-llm/llm-clinical-data-privacy.html#the-solution-datachat",
    "href": "workshops/ai-llm/llm-clinical-data-privacy.html#the-solution-datachat",
    "title": "Integrating LLM using R Shiny for Clinical Data Review",
    "section": "The Solution: {DataChat}",
    "text": "The Solution: {DataChat}\nAn AI-powered conversational interface that allows natural language interaction with clinical data while ensuring:\n\nğŸ”’ Data never leaves the secure environment\nâœ… Statistical calculations are validated\nğŸ“Š Results are reproducible and auditable\nğŸ‘¥ Accessible to non-technical users",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Integrating LLM using R Shiny for Clinical Data Review"
    ]
  },
  {
    "objectID": "workshops/ai-llm/llm-clinical-data-privacy.html#architecture-overview",
    "href": "workshops/ai-llm/llm-clinical-data-privacy.html#architecture-overview",
    "title": "Integrating LLM using R Shiny for Clinical Data Review",
    "section": "Architecture Overview",
    "text": "Architecture Overview\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚          User Interface (Shiny)                 â”‚\nâ”‚  \"Show me adverse events for patients &gt;65\"      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚       LLM Orchestration ({ellmer})              â”‚\nâ”‚  â€¢ Intent classification                        â”‚\nâ”‚  â€¢ Tool selection                               â”‚\nâ”‚  â€¢ Response generation                          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚                   â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ RAG System   â”‚   â”‚ Statistical     â”‚\nâ”‚ ({ragnar})   â”‚   â”‚ Tools           â”‚\nâ”‚              â”‚   â”‚ (validated)     â”‚\nâ”‚ â€¢ Document   â”‚   â”‚ â€¢ Summaries     â”‚\nâ”‚   retrieval  â”‚   â”‚ â€¢ Plots         â”‚\nâ”‚ â€¢ Context    â”‚   â”‚ â€¢ Models        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Integrating LLM using R Shiny for Clinical Data Review"
    ]
  },
  {
    "objectID": "workshops/ai-llm/llm-clinical-data-privacy.html#key-features",
    "href": "workshops/ai-llm/llm-clinical-data-privacy.html#key-features",
    "title": "Integrating LLM using R Shiny for Clinical Data Review",
    "section": "Key Features",
    "text": "Key Features\n\n1. Conversational Data Exploration\nNatural language queries like:\n\nâ€œWhatâ€™s the average age of patients in the treatment arm?â€\nâ€œShow me serious adverse events by system organ classâ€\nâ€œCompare baseline demographics between armsâ€\n\n\n\n2. RAG for Domain Knowledge\n{ragnar} provides retrieval-augmented generation capabilities:\nlibrary(ragnar)\n\n# Create vector database from study documents\nvector_db &lt;- ragnar_db() %&gt;%\n  add_documents(\n    path = \"study_protocols/\",\n    chunk_size = 500\n  )\n\n# Query with context\ncontext &lt;- vector_db$search(\n  query = user_question,\n  top_k = 5\n)\n\n\n3. Privacy-Preserving Design\nCritical Privacy Features:\n\nâœ… On-premise deployment - No data sent to external APIs\nâœ… Local LLMs supported - Can use llama.cpp or similar\nâœ… Query sanitization - Remove PII before processing\nâœ… Audit logging - Track all data access\nâœ… Role-based access - Control data visibility\n\n\n\n4. Statistical Validity\nEnsuring Accurate Results:\n\nAll statistical calculations use validated R functions\nLLM suggests approach, validated code executes\nResults include confidence intervals and p-values\nAutomatic flagging of statistical assumptions\nHuman review required for critical decisions",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Integrating LLM using R Shiny for Clinical Data Review"
    ]
  },
  {
    "objectID": "workshops/ai-llm/llm-clinical-data-privacy.html#workshop-content",
    "href": "workshops/ai-llm/llm-clinical-data-privacy.html#workshop-content",
    "title": "Integrating LLM using R Shiny for Clinical Data Review",
    "section": "Workshop Content",
    "text": "Workshop Content\n\nModule 1: Setting Up Secure LLM Integration\n\nConfiguring {ellmer} for private deployments\nLocal vs.Â cloud LLM considerations\nAPI security and authentication\n\n\n\nModule 2: Building the Conversational Interface\nUsing {shinychat} for user interaction:\nlibrary(shinychat)\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  chat_ui(\"clinical_chat\")\n)\n\nserver &lt;- function(input, output, session) {\n  chat &lt;- chat_server(\"clinical_chat\",\n    system_prompt = \"You are a clinical data assistant.\n                     Only answer questions about the loaded study data.\n                     Never make up information.\",\n    tools = list(\n      summarize_demographics,\n      plot_adverse_events,\n      query_database\n    )\n  )\n}\n\n\nModule 3: Implementing RAG\nDomain-specific context retrieval:\n\nIndexing study protocols and SAPs\nMedical terminology databases\nPrevious study reports\n\n\n\nModule 4: Privacy Controls\nPractical Implementation:\n# Anonymize data before LLM processing\nsanitize_query &lt;- function(query, data) {\n  # Remove patient identifiers\n  query &lt;- remove_pii(query)\n  \n  # Check for sensitive fields\n  if (contains_sensitive_terms(query)) {\n    return(list(\n      allowed = FALSE,\n      message = \"Query contains sensitive information\"\n    ))\n  }\n  \n  # Log for audit\n  log_query(query, user_id = session$user)\n  \n  return(list(allowed = TRUE, query = query))\n}\n\n\nModule 5: Validation Strategy\nEnsuring Reliability:\n\nTool validation - Each statistical function tested independently\nResponse validation - LLM output checked against expected format\nUser verification - Results shown with source data\nExpert review - Critical decisions flagged for human oversight",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Integrating LLM using R Shiny for Clinical Data Review"
    ]
  },
  {
    "objectID": "workshops/ai-llm/llm-clinical-data-privacy.html#use-cases-in-pharma",
    "href": "workshops/ai-llm/llm-clinical-data-privacy.html#use-cases-in-pharma",
    "title": "Integrating LLM using R Shiny for Clinical Data Review",
    "section": "Use Cases in Pharma",
    "text": "Use Cases in Pharma\n\n1. Clinical Review Meetings\n\nQuick ad-hoc analyses during discussions\nExploration of safety signals\nSubgroup identification\n\n\n\n2. Medical Writing\n\nExtracting statistics for CSR\nVerifying data consistency\nGenerating descriptive text\n\n\n\n3. Safety Monitoring\n\nDSMB data reviews\nAdverse event trending\nSafety signal detection\n\n\n\n4. Regulatory Queries\n\nRapid response to agency questions\nData subsetting and analysis\nDocumentation generation",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Integrating LLM using R Shiny for Clinical Data Review"
    ]
  },
  {
    "objectID": "workshops/ai-llm/llm-clinical-data-privacy.html#privacy-compliance",
    "href": "workshops/ai-llm/llm-clinical-data-privacy.html#privacy-compliance",
    "title": "Integrating LLM using R Shiny for Clinical Data Review",
    "section": "Privacy Compliance",
    "text": "Privacy Compliance\n\nGDPR Considerations\n\nâœ… Data minimization\nâœ… Purpose limitation\nâœ… Right to explanation (audit logs)\nâœ… Data encryption at rest and in transit\n\n\n\nHIPAA Compliance\n\nâœ… Access controls\nâœ… Audit trails\nâœ… De-identification support\nâœ… Business associate agreements (if using cloud LLMs)\n\n\n\n21 CFR Part 11\n\nâœ… Electronic signatures\nâœ… Audit trails\nâœ… System validation\nâœ… Controlled access",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Integrating LLM using R Shiny for Clinical Data Review"
    ]
  },
  {
    "objectID": "workshops/ai-llm/llm-clinical-data-privacy.html#validation-approach",
    "href": "workshops/ai-llm/llm-clinical-data-privacy.html#validation-approach",
    "title": "Integrating LLM using R Shiny for Clinical Data Review",
    "section": "Validation Approach",
    "text": "Validation Approach\n\nIQ (Installation Qualification)\n\nEnvironment setup documentation\nVersion control\nAccess controls verification\n\n\n\nOQ (Operational Qualification)\n\nTest each statistical tool independently\nVerify LLM response formatting\nConfirm privacy controls function\n\n\n\nPQ (Performance Qualification)\n\nEnd-to-end testing with real scenarios\nUser acceptance testing\nPerformance benchmarking",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Integrating LLM using R Shiny for Clinical Data Review"
    ]
  },
  {
    "objectID": "workshops/ai-llm/llm-clinical-data-privacy.html#learning-outcomes",
    "href": "workshops/ai-llm/llm-clinical-data-privacy.html#learning-outcomes",
    "title": "Integrating LLM using R Shiny for Clinical Data Review",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of this workshop, you will be able to:\nâœ… Design privacy-preserving LLM applications\nâœ… Implement RAG for pharmaceutical domain knowledge\nâœ… Build conversational interfaces with {shinychat}\nâœ… Ensure statistical validity in AI-generated results\nâœ… Deploy compliant AI solutions in regulated environments\nâœ… Create user-friendly tools for non-technical stakeholders",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Integrating LLM using R Shiny for Clinical Data Review"
    ]
  },
  {
    "objectID": "workshops/ai-llm/llm-clinical-data-privacy.html#demo-application",
    "href": "workshops/ai-llm/llm-clinical-data-privacy.html#demo-application",
    "title": "Integrating LLM using R Shiny for Clinical Data Review",
    "section": "Demo Application",
    "text": "Demo Application\nWorkshop includes hands-on work with {DataChat} demo:\n\nSample CDISC SDTM/ADaM datasets\nPre-configured LLM (local or API)\nExample queries and workflows\nPrivacy controls demonstration",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Integrating LLM using R Shiny for Clinical Data Review"
    ]
  },
  {
    "objectID": "workshops/ai-llm/llm-clinical-data-privacy.html#best-practices",
    "href": "workshops/ai-llm/llm-clinical-data-privacy.html#best-practices",
    "title": "Integrating LLM using R Shiny for Clinical Data Review",
    "section": "Best Practices",
    "text": "Best Practices\n\nDoâ€™s âœ…\n\nAlways validate statistical outputs\nLog all data access for audit\nUse validated tools for calculations\nImplement role-based access control\nTest privacy controls thoroughly\n\n\n\nDonâ€™ts âŒ\n\nNever send raw clinical data to external APIs (unless approved)\nDonâ€™t rely solely on LLM for critical decisions\nAvoid exposing PII in queries\nDonâ€™t skip validation documentation\nNever deploy without proper testing",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Integrating LLM using R Shiny for Clinical Data Review"
    ]
  },
  {
    "objectID": "workshops/ai-llm/llm-clinical-data-privacy.html#future-directions",
    "href": "workshops/ai-llm/llm-clinical-data-privacy.html#future-directions",
    "title": "Integrating LLM using R Shiny for Clinical Data Review",
    "section": "Future Directions",
    "text": "Future Directions\n\nIntegration with electronic data capture (EDC) systems\nMulti-lingual support for global trials\nAdvanced visualization capabilities\nAutomated report generation\nReal-time safety monitoring",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Integrating LLM using R Shiny for Clinical Data Review"
    ]
  },
  {
    "objectID": "workshops/ai-llm/llm-clinical-data-privacy.html#additional-resources",
    "href": "workshops/ai-llm/llm-clinical-data-privacy.html#additional-resources",
    "title": "Integrating LLM using R Shiny for Clinical Data Review",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nCDISC standards: cdisc.org\nFDA guidance on AI/ML: fda.gov\nPrivacy regulations: GDPR, HIPAA guidelines\n\n\n\n\n\n\n\nWarningImportant Note\n\n\n\nThis workshop demonstrates privacy-preserving approaches but should not be considered legal or regulatory advice. Always consult with your organizationâ€™s legal, compliance, and IT security teams before deploying AI applications with clinical data.\n\n\n\n\nSimilar Workshops\n\nGetting Started with LLM APIs - LLM basics\npointblank: Data Validation - Data quality for AI\n\n\n\nRelated Presentations\n\nIntegrating LLM for Clinical Data Review - DataChat presentation\n\n\n\nNext Steps\n\nFor validation: See pointblank workshop\nIndustry trends: AI Revolution analysis\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Integrating LLM using R Shiny for Clinical Data Review"
    ]
  },
  {
    "objectID": "workshops/ai-llm/getting-started-llm-apis.html",
    "href": "workshops/ai-llm/getting-started-llm-apis.html",
    "title": "Getting Started with LLM APIs in R",
    "section": "",
    "text": "Beginner Friendly AI/LLM Shiny\nLLMs are transforming how we write code, build tools, and analyze data, but getting started with directly working with LLM APIs can feel daunting. This workshop introduces participants to programming with LLM APIs in R using {ellmer}, an open-source package that makes it easy to work with LLMs from R.\n\n\n\nğŸ“¡ Calling LLMs from R - Basic API integration and response handling\nğŸ¯ System Prompt Design - Crafting effective prompts for specific tasks\nğŸ”§ Tool Calling - Enabling LLMs to execute R functions\nğŸ’¬ Building Chatbots - Creating interactive conversational interfaces",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Getting Started with LLM APIs in R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/getting-started-llm-apis.html#overview",
    "href": "workshops/ai-llm/getting-started-llm-apis.html#overview",
    "title": "Getting Started with LLM APIs in R",
    "section": "",
    "text": "Beginner Friendly AI/LLM Shiny\nLLMs are transforming how we write code, build tools, and analyze data, but getting started with directly working with LLM APIs can feel daunting. This workshop introduces participants to programming with LLM APIs in R using {ellmer}, an open-source package that makes it easy to work with LLMs from R.\n\n\n\nğŸ“¡ Calling LLMs from R - Basic API integration and response handling\nğŸ¯ System Prompt Design - Crafting effective prompts for specific tasks\nğŸ”§ Tool Calling - Enabling LLMs to execute R functions\nğŸ’¬ Building Chatbots - Creating interactive conversational interfaces",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Getting Started with LLM APIs in R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/getting-started-llm-apis.html#prerequisites",
    "href": "workshops/ai-llm/getting-started-llm-apis.html#prerequisites",
    "title": "Getting Started with LLM APIs in R",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRequired Knowledge:\n\nBasic R familiarity\nNo AI or machine learning background needed\n\nSetup:\n\nR environment with internet access\nAPI keys (will be provided during workshop)",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Getting Started with LLM APIs in R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/getting-started-llm-apis.html#key-packages-tools",
    "href": "workshops/ai-llm/getting-started-llm-apis.html#key-packages-tools",
    "title": "Getting Started with LLM APIs in R",
    "section": "Key Packages & Tools",
    "text": "Key Packages & Tools\n\n{ellmer}\n\n\n{shinychat}\n\n\nOpenAI API\n\n\nAnthropic API",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Getting Started with LLM APIs in R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/getting-started-llm-apis.html#workshop-content",
    "href": "workshops/ai-llm/getting-started-llm-apis.html#workshop-content",
    "title": "Getting Started with LLM APIs in R",
    "section": "Workshop Content",
    "text": "Workshop Content\n\n1. Introduction to LLM APIs\nUnderstanding how to interact with large language models programmatically:\n\nAPI authentication and configuration\nRequest/response structure\nToken management and costs\nError handling and best practices\n\n\n\n2. The {ellmer} Package\n{ellmer} provides a unified interface for working with multiple LLM providers:\nlibrary(ellmer)\n\n# Connect to an LLM\nchat &lt;- chat_openai(\n  model = \"gpt-4\",\n  system_prompt = \"You are a helpful R programming assistant.\"\n)\n\n# Send a message\nresponse &lt;- chat$chat(\"How do I read a CSV file in R?\")\n\n\n3. System Prompt Engineering\nLearn to design effective system prompts that guide LLM behavior:\n\nDefining role and expertise\nSetting tone and style\nProviding context and constraints\nExamples of good vs.Â bad prompts\n\n\n\n4. Tool Calling\nEnable LLMs to execute R functions and interact with your data:\n\nDefining tool schemas\nRegistering R functions as tools\nHandling tool execution\nMulti-turn conversations with tools\n\nExample use case: LLM that can read files, perform calculations, and generate plots.\n\n\n5. Building Basic Chatbots\nCreate interactive conversational applications:\n\nUsing {shinychat} for UI\nManaging conversation state\nStreaming responses\nAdding context and memory",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Getting Started with LLM APIs in R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/getting-started-llm-apis.html#practical-applications-in-pharma",
    "href": "workshops/ai-llm/getting-started-llm-apis.html#practical-applications-in-pharma",
    "title": "Getting Started with LLM APIs in R",
    "section": "Practical Applications in Pharma",
    "text": "Practical Applications in Pharma\n\nğŸ“Š Data exploration assistants - Natural language queries on clinical data\nğŸ“ Report generation - Automated narrative generation from analysis results\nğŸ” Code review helpers - Explain complex statistical code\nğŸ“š Documentation assistants - Generate function documentation",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Getting Started with LLM APIs in R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/getting-started-llm-apis.html#workshop-materials",
    "href": "workshops/ai-llm/getting-started-llm-apis.html#workshop-materials",
    "title": "Getting Started with LLM APIs in R",
    "section": "Workshop Materials",
    "text": "Workshop Materials\n\n\n\n\n\n\nNoteResources\n\n\n\nWorkshop Link: https://skaltman.github.io/r-pharma-llm/\nGitHub: https://github.com/posit-dev/ellmer\nInstructor: Sara Altman is a Data Science Educator at Posit PBC, focusing on making AI tools accessible to R users.",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Getting Started with LLM APIs in R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/getting-started-llm-apis.html#example-simple-llm-powered-data-assistant",
    "href": "workshops/ai-llm/getting-started-llm-apis.html#example-simple-llm-powered-data-assistant",
    "title": "Getting Started with LLM APIs in R",
    "section": "Example: Simple LLM-Powered Data Assistant",
    "text": "Example: Simple LLM-Powered Data Assistant\nlibrary(ellmer)\nlibrary(dplyr)\n\n# Create a chat interface with tools\nchat &lt;- chat_openai(\n  model = \"gpt-4\",\n  system_prompt = \"You are a data analysis assistant. \n                   Use the provided tools to answer questions about data.\"\n)\n\n# Register tools\nchat &lt;- chat %&gt;%\n  register_tool(\n    \"summarize_data\",\n    function(data) {\n      summary(data)\n    },\n    description = \"Get summary statistics of a dataset\"\n  )\n\n# Use it\nresponse &lt;- chat$chat(\n  \"Can you summarize the mtcars dataset?\",\n  data = mtcars\n)",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Getting Started with LLM APIs in R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/getting-started-llm-apis.html#learning-outcomes",
    "href": "workshops/ai-llm/getting-started-llm-apis.html#learning-outcomes",
    "title": "Getting Started with LLM APIs in R",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of this workshop, you will be able to:\nâœ… Set up and configure LLM API connections in R\nâœ… Design effective system prompts for specific tasks\nâœ… Implement tool calling to extend LLM capabilities\nâœ… Build basic chatbot interfaces with {shinychat}\nâœ… Understand best practices for LLM integration in pharma workflows",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Getting Started with LLM APIs in R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/getting-started-llm-apis.html#next-steps",
    "href": "workshops/ai-llm/getting-started-llm-apis.html#next-steps",
    "title": "Getting Started with LLM APIs in R",
    "section": "Next Steps",
    "text": "Next Steps\nAfter this workshop, consider:\n\nGuided Tour to Building LLM-Based Tooling - More advanced MCP and agentic systems\nIntegrating LLM with Clinical Data - Privacy and validation considerations\nExplore {ellmer} documentation for advanced features",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Getting Started with LLM APIs in R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/getting-started-llm-apis.html#additional-resources",
    "href": "workshops/ai-llm/getting-started-llm-apis.html#additional-resources",
    "title": "Getting Started with LLM APIs in R",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nellmer documentation: posit-dev.github.io/ellmer\nOpenAI API reference: platform.openai.com/docs\nPrompt engineering guide: Anthropic prompt engineering\n\n\n\nSimilar Workshops\n\nGuided Tour to Building LLM-Based Tooling - Advanced enterprise AI implementation\nIntegrating LLM with Clinical Data Review - Privacy-preserving AI applications\n\n\n\nRelated Presentations\n\nPractical AI for Data Science - Real-world use cases\nGenAI in Production - Moving beyond prototypes\n\n\n\nTools & Resources\n\n{ellmer} in Tools Catalog - Complete package reference\n{shinychat} - Chatbot interface details\n\n\n\nNext Steps\n\nIf you liked this: Try Guided Tour to LLM Tooling for enterprise patterns\nFor Shiny integration: See Integrating LLM with Clinical Data\nCareer impact: AI/LLM Skills are in high demand\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "Getting Started with LLM APIs in R"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/debugging-stan.html",
    "href": "workshops/statistical-methods/debugging-stan.html",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "",
    "text": "Beginner Friendly Machine Learning Classification\nDive into classification with R and the elegant tidymodels framework! Learn to build, evaluate, and refine machine learning models that predict categorical outcomes through practical exercises including a wine classification challenge.\n\n\n\nğŸ§¹ Data preprocessing with {recipes}\nğŸ“Š Train/test splitting with {rsample}\nğŸ¤– Model fitting with {parsnip}\nğŸ“ˆ Performance evaluation with {yardstick}\nğŸ¯ Complete ML workflow from data to predictions",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/debugging-stan.html#overview",
    "href": "workshops/statistical-methods/debugging-stan.html#overview",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "",
    "text": "Beginner Friendly Machine Learning Classification\nDive into classification with R and the elegant tidymodels framework! Learn to build, evaluate, and refine machine learning models that predict categorical outcomes through practical exercises including a wine classification challenge.\n\n\n\nğŸ§¹ Data preprocessing with {recipes}\nğŸ“Š Train/test splitting with {rsample}\nğŸ¤– Model fitting with {parsnip}\nğŸ“ˆ Performance evaluation with {yardstick}\nğŸ¯ Complete ML workflow from data to predictions",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/debugging-stan.html#prerequisites",
    "href": "workshops/statistical-methods/debugging-stan.html#prerequisites",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRequired Knowledge:\n\nBasic R programming\nUnderstanding of classification concepts\nFamiliarity with dplyr helpful\n\nNo Prior Experience Needed:\n\nMachine learning\ntidymodels framework",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/debugging-stan.html#key-packages",
    "href": "workshops/statistical-methods/debugging-stan.html#key-packages",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "Key Packages",
    "text": "Key Packages\n\n{tidymodels}\n\n\n{recipes}\n\n\n{parsnip}\n\n\n{rsample}\n\n\n{yardstick}\n\n\n{tune}",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/debugging-stan.html#the-tidymodels-ecosystem",
    "href": "workshops/statistical-methods/debugging-stan.html#the-tidymodels-ecosystem",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "The tidymodels Ecosystem",
    "text": "The tidymodels Ecosystem\n\nCore Packages\n{rsample} - Data splitting and resampling\nlibrary(tidymodels)\n\n# Split data\ndata_split &lt;- initial_split(data, prop = 0.75, strata = outcome)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n{recipes} - Feature engineering\nrecipe_spec &lt;- recipe(outcome ~ ., data = train_data) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors())\n{parsnip} - Model specification\nmodel_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n{workflows} - Combine recipe + model\nworkflow_fit &lt;- workflow() %&gt;%\n  add_recipe(recipe_spec) %&gt;%\n  add_model(model_spec) %&gt;%\n  fit(data = train_data)\n{yardstick} - Performance metrics\npredictions %&gt;%\n  metrics(truth = outcome, estimate = .pred_class)",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/debugging-stan.html#workshop-content",
    "href": "workshops/statistical-methods/debugging-stan.html#workshop-content",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "Workshop Content",
    "text": "Workshop Content\n\n1. Data Preparation\n\nLoading and exploring data\nHandling missing values\nFeature selection\nTrain/test splitting with stratification\n\n\n\n2. Preprocessing with Recipes\nCommon Steps:\n\nstep_normalize() - Scale numeric features\nstep_dummy() - Create dummy variables\nstep_impute_*() - Handle missing data\nstep_pca() - Dimensionality reduction\nstep_interact() - Feature interactions\n\n\n\n3. Model Building\nClassification Algorithms:\n\nLogistic regression\nDecision trees\nRandom forests\nSupport vector machines\nNeural networks\n\nConsistent Interface:\n# Same syntax for different models!\nlog_spec &lt;- logistic_reg() %&gt;% set_engine(\"glm\")\nrf_spec &lt;- rand_forest() %&gt;% set_engine(\"ranger\")\nsvm_spec &lt;- svm_rbf() %&gt;% set_engine(\"kernlab\")\n\n\n4. Model Evaluation\nClassification Metrics:\n\nAccuracy\nPrecision/Recall\nF1 Score\nROC AUC\nConfusion matrix\n\n\n\n5. Hyperparameter Tuning\n# Define tuning grid\nrf_spec &lt;- rand_forest(\n  mtry = tune(),\n  trees = tune(),\n  min_n = tune()\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# Cross-validation\ncv_folds &lt;- vfold_cv(train_data, v = 5)\n\n# Tune\ntune_results &lt;- tune_grid(\n  workflow() %&gt;% add_recipe(recipe_spec) %&gt;% add_model(rf_spec),\n  resamples = cv_folds,\n  grid = 25\n)\n\n# Select best\nbest_params &lt;- select_best(tune_results, metric = \"roc_auc\")",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/debugging-stan.html#hands-on-wine-classification",
    "href": "workshops/statistical-methods/debugging-stan.html#hands-on-wine-classification",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "Hands-On: Wine Classification",
    "text": "Hands-On: Wine Classification\nChallenge: Predict wine quality (Good/Bad) based on chemical properties.\nSteps:\n\nExplore wine dataset\nCreate preprocessing recipe\nTry multiple classification algorithms\nEvaluate and compare models\nSelect best performer\nMake final predictions\n\nExample Workflow:\n# Load data\ndata(wine_quality)\n\n# Split\nwine_split &lt;- initial_split(wine_quality, strata = quality)\nwine_train &lt;- training(wine_split)\nwine_test &lt;- testing(wine_split)\n\n# Recipe\nwine_recipe &lt;- recipe(quality ~ ., data = wine_train) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_corr(all_numeric_predictors(), threshold = 0.9)\n\n# Model\nrf_model &lt;- rand_forest(trees = 1000) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\n# Workflow\nwine_wf &lt;- workflow() %&gt;%\n  add_recipe(wine_recipe) %&gt;%\n  add_model(rf_model)\n\n# Fit\nwine_fit &lt;- wine_wf %&gt;% fit(data = wine_train)\n\n# Predict\nwine_pred &lt;- wine_fit %&gt;%\n  predict(wine_test) %&gt;%\n  bind_cols(wine_test)\n\n# Evaluate\nwine_pred %&gt;%\n  metrics(truth = quality, estimate = .pred_class)",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/debugging-stan.html#practical-applications-in-pharma",
    "href": "workshops/statistical-methods/debugging-stan.html#practical-applications-in-pharma",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "Practical Applications in Pharma",
    "text": "Practical Applications in Pharma\n\nClinical Trials\n\nPatient risk stratification\nResponder identification\nAdverse event prediction\nSite selection\n\n\n\nDrug Discovery\n\nCompound classification\nActivity prediction\nToxicity screening\n\n\n\nOperations\n\nTrial enrollment prediction\nResource allocation\nQuality control",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/debugging-stan.html#best-practices",
    "href": "workshops/statistical-methods/debugging-stan.html#best-practices",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "Best Practices",
    "text": "Best Practices\n\nâœ… Doâ€™s\n\nAlways use train/test split\nStratify on outcome variable\nUse cross-validation for tuning\nCheck for data leakage\nDocument preprocessing steps\n\n\n\nâŒ Donâ€™ts\n\nDonâ€™t peek at test set during development\nDonâ€™t overfit with too many features\nDonâ€™t ignore class imbalance\nDonâ€™t forget to normalize/scale\nDonâ€™t skip exploratory analysis",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/debugging-stan.html#learning-outcomes",
    "href": "workshops/statistical-methods/debugging-stan.html#learning-outcomes",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nâœ… Preprocess data with {recipes}\nâœ… Split data properly with {rsample}\nâœ… Fit classification models with {parsnip}\nâœ… Evaluate performance with {yardstick}\nâœ… Tune hyperparameters systematically\nâœ… Build complete ML pipelines",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/debugging-stan.html#beyond-the-workshop",
    "href": "workshops/statistical-methods/debugging-stan.html#beyond-the-workshop",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "Beyond the Workshop",
    "text": "Beyond the Workshop\nNext Steps:\n\nExplore other algorithms (XGBoost, neural nets)\nLearn advanced feature engineering\nStudy model interpretation (SHAP, LIME)\nPractice with real datasets\n\nResources:\n\ntidymodels.org\nTidy Modeling with R (book)\ntidymodels tag on Stack Overflow\n\n\n\nSimilar Workshops\n\nBayesian Survival Models - Applied Stan usage\nrpact Trial Design - Frequentist alternative\n\n\n\nRelated Presentations\n\nBayesERtools - Production Bayesian package\n\n\n\nNext Steps\n\nApply skills: Bayesian Survival workshop\nCareer path: Bayesian Methods expertise\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/r-classification-tidymodels.html",
    "href": "workshops/statistical-methods/r-classification-tidymodels.html",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "",
    "text": "Beginner Friendly Machine Learning Classification\nDive into classification with R and the elegant tidymodels framework! Learn to build, evaluate, and refine machine learning models that predict categorical outcomes through practical exercises including a wine classification challenge.\n\n\n\nğŸ§¹ Data preprocessing with {recipes}\nğŸ“Š Train/test splitting with {rsample}\nğŸ¤– Model fitting with {parsnip}\nğŸ“ˆ Performance evaluation with {yardstick}\nğŸ¯ Complete ML workflow from data to predictions",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/r-classification-tidymodels.html#overview",
    "href": "workshops/statistical-methods/r-classification-tidymodels.html#overview",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "",
    "text": "Beginner Friendly Machine Learning Classification\nDive into classification with R and the elegant tidymodels framework! Learn to build, evaluate, and refine machine learning models that predict categorical outcomes through practical exercises including a wine classification challenge.\n\n\n\nğŸ§¹ Data preprocessing with {recipes}\nğŸ“Š Train/test splitting with {rsample}\nğŸ¤– Model fitting with {parsnip}\nğŸ“ˆ Performance evaluation with {yardstick}\nğŸ¯ Complete ML workflow from data to predictions",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/r-classification-tidymodels.html#prerequisites",
    "href": "workshops/statistical-methods/r-classification-tidymodels.html#prerequisites",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRequired Knowledge:\n\nBasic R programming\nUnderstanding of classification concepts\nFamiliarity with dplyr helpful\n\nNo Prior Experience Needed:\n\nMachine learning\ntidymodels framework",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/r-classification-tidymodels.html#key-packages",
    "href": "workshops/statistical-methods/r-classification-tidymodels.html#key-packages",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "Key Packages",
    "text": "Key Packages\n\n{tidymodels}\n\n\n{recipes}\n\n\n{parsnip}\n\n\n{rsample}\n\n\n{yardstick}\n\n\n{tune}",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/r-classification-tidymodels.html#the-tidymodels-ecosystem",
    "href": "workshops/statistical-methods/r-classification-tidymodels.html#the-tidymodels-ecosystem",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "The tidymodels Ecosystem",
    "text": "The tidymodels Ecosystem\n\nCore Packages\n{rsample} - Data splitting and resampling\nlibrary(tidymodels)\n\n# Split data\ndata_split &lt;- initial_split(data, prop = 0.75, strata = outcome)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n{recipes} - Feature engineering\nrecipe_spec &lt;- recipe(outcome ~ ., data = train_data) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors())\n{parsnip} - Model specification\nmodel_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n{workflows} - Combine recipe + model\nworkflow_fit &lt;- workflow() %&gt;%\n  add_recipe(recipe_spec) %&gt;%\n  add_model(model_spec) %&gt;%\n  fit(data = train_data)\n{yardstick} - Performance metrics\npredictions %&gt;%\n  metrics(truth = outcome, estimate = .pred_class)",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/r-classification-tidymodels.html#workshop-content",
    "href": "workshops/statistical-methods/r-classification-tidymodels.html#workshop-content",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "Workshop Content",
    "text": "Workshop Content\n\n1. Data Preparation\n\nLoading and exploring data\nHandling missing values\nFeature selection\nTrain/test splitting with stratification\n\n\n\n2. Preprocessing with Recipes\nCommon Steps:\n\nstep_normalize() - Scale numeric features\nstep_dummy() - Create dummy variables\nstep_impute_*() - Handle missing data\nstep_pca() - Dimensionality reduction\nstep_interact() - Feature interactions\n\n\n\n3. Model Building\nClassification Algorithms:\n\nLogistic regression\nDecision trees\nRandom forests\nSupport vector machines\nNeural networks\n\nConsistent Interface:\n# Same syntax for different models!\nlog_spec &lt;- logistic_reg() %&gt;% set_engine(\"glm\")\nrf_spec &lt;- rand_forest() %&gt;% set_engine(\"ranger\")\nsvm_spec &lt;- svm_rbf() %&gt;% set_engine(\"kernlab\")\n\n\n4. Model Evaluation\nClassification Metrics:\n\nAccuracy\nPrecision/Recall\nF1 Score\nROC AUC\nConfusion matrix\n\n\n\n5. Hyperparameter Tuning\n# Define tuning grid\nrf_spec &lt;- rand_forest(\n  mtry = tune(),\n  trees = tune(),\n  min_n = tune()\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# Cross-validation\ncv_folds &lt;- vfold_cv(train_data, v = 5)\n\n# Tune\ntune_results &lt;- tune_grid(\n  workflow() %&gt;% add_recipe(recipe_spec) %&gt;% add_model(rf_spec),\n  resamples = cv_folds,\n  grid = 25\n)\n\n# Select best\nbest_params &lt;- select_best(tune_results, metric = \"roc_auc\")",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/r-classification-tidymodels.html#hands-on-wine-classification",
    "href": "workshops/statistical-methods/r-classification-tidymodels.html#hands-on-wine-classification",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "Hands-On: Wine Classification",
    "text": "Hands-On: Wine Classification\nChallenge: Predict wine quality (Good/Bad) based on chemical properties.\nSteps:\n\nExplore wine dataset\nCreate preprocessing recipe\nTry multiple classification algorithms\nEvaluate and compare models\nSelect best performer\nMake final predictions\n\nExample Workflow:\n# Load data\ndata(wine_quality)\n\n# Split\nwine_split &lt;- initial_split(wine_quality, strata = quality)\nwine_train &lt;- training(wine_split)\nwine_test &lt;- testing(wine_split)\n\n# Recipe\nwine_recipe &lt;- recipe(quality ~ ., data = wine_train) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_corr(all_numeric_predictors(), threshold = 0.9)\n\n# Model\nrf_model &lt;- rand_forest(trees = 1000) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\n# Workflow\nwine_wf &lt;- workflow() %&gt;%\n  add_recipe(wine_recipe) %&gt;%\n  add_model(rf_model)\n\n# Fit\nwine_fit &lt;- wine_wf %&gt;% fit(data = wine_train)\n\n# Predict\nwine_pred &lt;- wine_fit %&gt;%\n  predict(wine_test) %&gt;%\n  bind_cols(wine_test)\n\n# Evaluate\nwine_pred %&gt;%\n  metrics(truth = quality, estimate = .pred_class)",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/r-classification-tidymodels.html#practical-applications-in-pharma",
    "href": "workshops/statistical-methods/r-classification-tidymodels.html#practical-applications-in-pharma",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "Practical Applications in Pharma",
    "text": "Practical Applications in Pharma\n\nClinical Trials\n\nPatient risk stratification\nResponder identification\nAdverse event prediction\nSite selection\n\n\n\nDrug Discovery\n\nCompound classification\nActivity prediction\nToxicity screening\n\n\n\nOperations\n\nTrial enrollment prediction\nResource allocation\nQuality control",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/r-classification-tidymodels.html#best-practices",
    "href": "workshops/statistical-methods/r-classification-tidymodels.html#best-practices",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "Best Practices",
    "text": "Best Practices\n\nâœ… Doâ€™s\n\nAlways use train/test split\nStratify on outcome variable\nUse cross-validation for tuning\nCheck for data leakage\nDocument preprocessing steps\n\n\n\nâŒ Donâ€™ts\n\nDonâ€™t peek at test set during development\nDonâ€™t overfit with too many features\nDonâ€™t ignore class imbalance\nDonâ€™t forget to normalize/scale\nDonâ€™t skip exploratory analysis",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/r-classification-tidymodels.html#learning-outcomes",
    "href": "workshops/statistical-methods/r-classification-tidymodels.html#learning-outcomes",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nâœ… Preprocess data with {recipes}\nâœ… Split data properly with {rsample}\nâœ… Fit classification models with {parsnip}\nâœ… Evaluate performance with {yardstick}\nâœ… Tune hyperparameters systematically\nâœ… Build complete ML pipelines",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/r-classification-tidymodels.html#beyond-the-workshop",
    "href": "workshops/statistical-methods/r-classification-tidymodels.html#beyond-the-workshop",
    "title": "R-Classification: Predictive Power with tidymodels",
    "section": "Beyond the Workshop",
    "text": "Beyond the Workshop\nNext Steps:\n\nExplore other algorithms (XGBoost, neural nets)\nLearn advanced feature engineering\nStudy model interpretation (SHAP, LIME)\nPractice with real datasets\n\nResources:\n\ntidymodels.org\nTidy Modeling with R (book)\ntidymodels tag on Stack Overflow\n\n\n\nSimilar Workshops\n\nBayesian Survival Models - Advanced modeling\nPython for CSR - Python ML with polars\n\n\n\nRelated Presentations\n\nTabPFN: Deep Learning for Tabular Data - Novel ML approach\n\n\n\nNext Steps\n\nAdvanced ML: Explore TabPFN and other methods in presentations\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "R-Classification: Predictive Power with tidymodels"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/officer-flextable.html",
    "href": "workshops/clinical-reporting/officer-flextable.html",
    "title": "Advanced Clinical Reporting with officer and flextable",
    "section": "",
    "text": "Intermediate Clinical Reporting Word Documents\nMaster the creation of sophisticated clinical reports in Word format using R with a reproducible approach. This workshop covers the complete journey from clinical data to pharmaceutical-grade report generation, focusing on advanced document structure management, complex table creation, and integration of ggplot2 visualizations.\n\n\n\nğŸ“„ Document structure management in Word\nğŸ“Š Complex tables following pharmaceutical standards\nğŸ“ˆ ggplot2 integration for publication-ready figures\nğŸ¨ Styling and formatting for professional outputs\nğŸ”— Cross-references and automated numbering\nâœ… Complete end-to-end CSR generation",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Advanced Clinical Reporting with officer and flextable"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/officer-flextable.html#overview",
    "href": "workshops/clinical-reporting/officer-flextable.html#overview",
    "title": "Advanced Clinical Reporting with officer and flextable",
    "section": "",
    "text": "Intermediate Clinical Reporting Word Documents\nMaster the creation of sophisticated clinical reports in Word format using R with a reproducible approach. This workshop covers the complete journey from clinical data to pharmaceutical-grade report generation, focusing on advanced document structure management, complex table creation, and integration of ggplot2 visualizations.\n\n\n\nğŸ“„ Document structure management in Word\nğŸ“Š Complex tables following pharmaceutical standards\nğŸ“ˆ ggplot2 integration for publication-ready figures\nğŸ¨ Styling and formatting for professional outputs\nğŸ”— Cross-references and automated numbering\nâœ… Complete end-to-end CSR generation",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Advanced Clinical Reporting with officer and flextable"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/officer-flextable.html#prerequisites",
    "href": "workshops/clinical-reporting/officer-flextable.html#prerequisites",
    "title": "Advanced Clinical Reporting with officer and flextable",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRequired Knowledge:\n\nIntermediate R programming\nBasic understanding of clinical trial data\nFamiliarity with data manipulation (dplyr)\n\nRecommended:\n\nExperience with ggplot2\nKnowledge of clinical study report structure",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Advanced Clinical Reporting with officer and flextable"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/officer-flextable.html#key-packages-tools",
    "href": "workshops/clinical-reporting/officer-flextable.html#key-packages-tools",
    "title": "Advanced Clinical Reporting with officer and flextable",
    "section": "Key Packages & Tools",
    "text": "Key Packages & Tools\n\n{officer}\n\n\n{flextable}\n\n\n{ggplot2}\n\n\n{officedown}\n\n\nMicrosoft Word",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Advanced Clinical Reporting with officer and flextable"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/officer-flextable.html#workshop-materials",
    "href": "workshops/clinical-reporting/officer-flextable.html#workshop-materials",
    "title": "Advanced Clinical Reporting with officer and flextable",
    "section": "Workshop Materials",
    "text": "Workshop Materials\n\n\n\n\n\n\nNoteResources\n\n\n\nWorkshop Website: https://ardata.fr/r-in-pharma-2025/\nGitHub Code: https://github.com/ardata-fr/r-in-pharma-2025-codes\nFull Tutorial: https://github.com/ardata-fr/r-in-pharma-reporting-with-officer-flextable",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Advanced Clinical Reporting with officer and flextable"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/officer-flextable.html#part-1-fundamentals",
    "href": "workshops/clinical-reporting/officer-flextable.html#part-1-fundamentals",
    "title": "Advanced Clinical Reporting with officer and flextable",
    "section": "Part 1: Fundamentals",
    "text": "Part 1: Fundamentals\n\nIntroduction to {officer}\n{officer} allows you to create and manipulate Word documents programmatically:\nlibrary(officer)\n\n# Create a new document\ndoc &lt;- read_docx()\n\n# Add content\ndoc &lt;- doc %&gt;%\n  body_add_par(\"Clinical Study Report\", style = \"heading 1\") %&gt;%\n  body_add_par(\"Protocol ABC-123\", style = \"heading 2\") %&gt;%\n  body_add_par(\"This report summarizes the results of...\", \n               style = \"Normal\")\n\n# Save\nprint(doc, target = \"clinical_report.docx\")\n\n\nIntroduction to {flextable}\n{flextable} creates publication-ready tables with fine-grained control:\nlibrary(flextable)\nlibrary(dplyr)\n\n# Create a demographic table\ndemographics %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    age = \"Age (years)\",\n    sex = \"Sex, n (%)\",\n    race = \"Race, n (%)\"\n  ) %&gt;%\n  theme_vanilla() %&gt;%\n  autofit()",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Advanced Clinical Reporting with officer and flextable"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/officer-flextable.html#part-2-advanced-clinical-reporting",
    "href": "workshops/clinical-reporting/officer-flextable.html#part-2-advanced-clinical-reporting",
    "title": "Advanced Clinical Reporting with officer and flextable",
    "section": "Part 2: Advanced Clinical Reporting",
    "text": "Part 2: Advanced Clinical Reporting\n\nDocument Structure Management\nCreating a Structured CSR:\nlibrary(officer)\n\n# Start with a template\ndoc &lt;- read_docx(\"csr_template.docx\")\n\n# Add title page\ndoc &lt;- doc %&gt;%\n  body_add_par(\"CLINICAL STUDY REPORT\", style = \"Title\") %&gt;%\n  body_add_par(sprintf(\"Protocol: %s\", protocol_id), \n               style = \"Subtitle\") %&gt;%\n  body_add_break()\n\n# Table of Contents (auto-updated)\ndoc &lt;- doc %&gt;%\n  body_add_toc(level = 3)\n\n# Section structure\ndoc &lt;- doc %&gt;%\n  body_add_par(\"1. Synopsis\", style = \"heading 1\") %&gt;%\n  body_add_par(\"1.1 Study Objectives\", style = \"heading 2\")\n\n\nComplex Table Creation\nPharmaceutical-Standard Tables:\nlibrary(flextable)\n\n# Adverse events table with nested headers\ncreate_ae_table &lt;- function(data) {\n  data %&gt;%\n    flextable() %&gt;%\n    # Multi-level headers\n    add_header_row(\n      values = c(\"\", \"Treatment A\", \"Treatment B\"),\n      colwidths = c(1, 2, 2)\n    ) %&gt;%\n    add_header_row(\n      values = c(\"System Organ Class\", \"n\", \"%\", \"n\", \"%\"),\n      top = FALSE\n    ) %&gt;%\n    # Merge cells for SOC\n    merge_v(j = 1) %&gt;%\n    # Styling\n    theme_booktabs() %&gt;%\n    bold(part = \"header\") %&gt;%\n    align(align = \"center\", part = \"header\") %&gt;%\n    align(j = 1, align = \"left\", part = \"body\") %&gt;%\n    # Borders\n    hline(i = ~ SOC_change, border = fp_border()) %&gt;%\n    # Conditional formatting\n    bg(i = ~ rate &gt; 10, bg = \"#FFCCCC\") %&gt;%\n    # Footnotes\n    add_footer_lines(\"SOC = System Organ Class; n = number of subjects\") %&gt;%\n    autofit()\n}\n\n\nHeaders and Footers\nCustomized Headers/Footers:\n# Define header\nheader_text &lt;- fpar(\n  ftext(\"Protocol ABC-123\", prop = fp_text(font.size = 8)),\n  ftext(\" | \", prop = fp_text(font.size = 8)),\n  ftext(\"Page \", prop = fp_text(font.size = 8)),\n  run_pagenum(),\n  ftext(\" of \", prop = fp_text(font.size = 8)),\n  run_totalpages()\n)\n\n# Add to document\ndoc &lt;- doc %&gt;%\n  add_header(value = as_paragraph(header_text)) %&gt;%\n  add_footer(value = as_paragraph(\n    ftext(\"Confidential\", prop = fp_text(italic = TRUE))\n  ))\n\n\nCross-References\nAutomated Table/Figure Numbering:\n# Add bookmarked table\ndoc &lt;- doc %&gt;%\n  body_add_par(\"Demographic Characteristics\", \n               style = \"Table Caption\") %&gt;%\n  body_bookmark(\"table_demographics\") %&gt;%\n  body_add_flextable(demographics_table)\n\n# Reference it elsewhere\ndoc &lt;- doc %&gt;%\n  body_add_par(\n    fpar(\n      ftext(\"As shown in Table \"),\n      run_reference(\"table_demographics\"),\n      ftext(\", the treatment groups were well balanced...\")\n    )\n  )\n\n\nIntegrating ggplot2 Visualizations\nPublication-Ready Figures:\nlibrary(ggplot2)\n\n# Create plot\nsurvival_plot &lt;- ggplot(survival_data, \n                        aes(x = time, y = surv, color = arm)) +\n  geom_step(linewidth = 1) +\n  geom_ribbon(aes(ymin = lower, ymax = upper, fill = arm), \n              alpha = 0.2) +\n  scale_color_manual(values = c(\"#0072B2\", \"#D55E00\")) +\n  scale_fill_manual(values = c(\"#0072B2\", \"#D55E00\")) +\n  labs(\n    title = \"Kaplan-Meier Survival Curves\",\n    x = \"Time (months)\",\n    y = \"Survival Probability\",\n    color = \"Treatment Arm\",\n    fill = \"Treatment Arm\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  )\n\n# Add to document\ndoc &lt;- doc %&gt;%\n  body_add_par(\"Figure 1: Overall Survival\", \n               style = \"Figure Caption\") %&gt;%\n  body_add_gg(value = survival_plot, \n              width = 6, height = 4) %&gt;%\n  body_add_par(\"Shaded areas represent 95% confidence intervals.\")",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Advanced Clinical Reporting with officer and flextable"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/officer-flextable.html#workshop-structure",
    "href": "workshops/clinical-reporting/officer-flextable.html#workshop-structure",
    "title": "Advanced Clinical Reporting with officer and flextable",
    "section": "Workshop Structure",
    "text": "Workshop Structure\n\nSession 1: Basics (90 minutes)\n\n{officer} fundamentals\n{flextable} basics\nSimple document creation\nBasic styling\n\n\n\nSession 2: Advanced Techniques (90 minutes)\n\nComplex table structures\nSection management\nHeaders/footers customization\nCross-reference handling\n\n\n\nSession 3: Complete CSR Example (120 minutes)\n\nEnd-to-end workflow\nTemplate-based approach\nAutomated report generation\nQC considerations",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Advanced Clinical Reporting with officer and flextable"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/officer-flextable.html#practical-examples",
    "href": "workshops/clinical-reporting/officer-flextable.html#practical-examples",
    "title": "Advanced Clinical Reporting with officer and flextable",
    "section": "Practical Examples",
    "text": "Practical Examples\n\nExample 1: Demographics Table\ncreate_demographics &lt;- function(data) {\n  summary_stats &lt;- data %&gt;%\n    group_by(ARM) %&gt;%\n    summarise(\n      N = n(),\n      Age_mean = mean(AGE, na.rm = TRUE),\n      Age_sd = sd(AGE, na.rm = TRUE),\n      Sex_M = sum(SEX == \"M\"),\n      Sex_F = sum(SEX == \"F\")\n    ) %&gt;%\n    mutate(\n      Age = sprintf(\"%.1f (%.1f)\", Age_mean, Age_sd),\n      `Male, n (%)` = sprintf(\"%d (%.1f%%)\", \n                              Sex_M, 100 * Sex_M / N),\n      `Female, n (%)` = sprintf(\"%d (%.1f%%)\", \n                                Sex_F, 100 * Sex_F / N)\n    ) %&gt;%\n    select(ARM, N, Age, `Male, n (%)`, `Female, n (%)`)\n  \n  summary_stats %&gt;%\n    flextable() %&gt;%\n    set_header_labels(ARM = \"Treatment Arm\") %&gt;%\n    align(j = 2:5, align = \"center\", part = \"all\") %&gt;%\n    bold(part = \"header\") %&gt;%\n    theme_booktabs() %&gt;%\n    autofit()\n}\n\n\nExample 2: Adverse Events Summary\ncreate_ae_summary &lt;- function(ae_data) {\n  ae_summary &lt;- ae_data %&gt;%\n    group_by(AESOC, AEDECOD, TRTA) %&gt;%\n    summarise(n = n_distinct(USUBJID)) %&gt;%\n    ungroup() %&gt;%\n    pivot_wider(names_from = TRTA, values_from = n, \n                values_fill = 0)\n  \n  ae_summary %&gt;%\n    flextable() %&gt;%\n    set_header_labels(\n      AESOC = \"System Organ Class\",\n      AEDECOD = \"Preferred Term\"\n    ) %&gt;%\n    merge_v(j = \"AESOC\") %&gt;%\n    valign(valign = \"top\") %&gt;%\n    bold(j = \"AESOC\") %&gt;%\n    theme_booktabs() %&gt;%\n    autofit() %&gt;%\n    add_footer_lines(\n      \"Table includes treatment-emergent adverse events\"\n    )\n}\n\n\nExample 3: Complete Report Template\ngenerate_csr &lt;- function(data_list, output_path) {\n  doc &lt;- read_docx(\"templates/csr_template.docx\")\n  \n  # Title page\n  doc &lt;- doc %&gt;%\n    body_add_par(\"CLINICAL STUDY REPORT\", style = \"Title\") %&gt;%\n    body_add_par(data_list$protocol_id, style = \"Subtitle\")\n  \n  # ToC\n  doc &lt;- doc %&gt;%\n    body_add_break() %&gt;%\n    body_add_toc()\n  \n  # Demographics\n  doc &lt;- doc %&gt;%\n    body_add_break() %&gt;%\n    body_add_par(\"Demographics\", style = \"heading 1\") %&gt;%\n    body_add_flextable(create_demographics(data_list$adsl))\n  \n  # Efficacy\n  doc &lt;- doc %&gt;%\n    body_add_break() %&gt;%\n    body_add_par(\"Efficacy Results\", style = \"heading 1\") %&gt;%\n    body_add_gg(create_efficacy_plot(data_list$adtte))\n  \n  # Safety\n  doc &lt;- doc %&gt;%\n    body_add_break() %&gt;%\n    body_add_par(\"Safety Results\", style = \"heading 1\") %&gt;%\n    body_add_flextable(create_ae_summary(data_list$adae))\n  \n  # Save\n  print(doc, target = output_path)\n}",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Advanced Clinical Reporting with officer and flextable"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/officer-flextable.html#best-practices",
    "href": "workshops/clinical-reporting/officer-flextable.html#best-practices",
    "title": "Advanced Clinical Reporting with officer and flextable",
    "section": "Best Practices",
    "text": "Best Practices\n\nâœ… Doâ€™s\n\nUse templates for consistency\nSeparate data processing from formatting\nDocument your styling choices\nTest cross-references thoroughly\nVersion control your templates\n\n\n\nâŒ Donâ€™ts\n\nHard-code numbers in text\nMix styling approaches\nForget page breaks\nIgnore Word template styles\nSkip QC on final document",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Advanced Clinical Reporting with officer and flextable"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/officer-flextable.html#advanced-features",
    "href": "workshops/clinical-reporting/officer-flextable.html#advanced-features",
    "title": "Advanced Clinical Reporting with officer and flextable",
    "section": "Advanced Features",
    "text": "Advanced Features\n\nConditional Formatting\ntable %&gt;%\n  flextable() %&gt;%\n  bg(i = ~ p_value &lt; 0.05, j = \"p_value\", bg = \"yellow\") %&gt;%\n  bold(i = ~ significant == TRUE) %&gt;%\n  color(i = ~ trend == \"increasing\", j = \"value\", color = \"red\")\n\n\nCustom Themes\nmy_pharma_theme &lt;- function(ft) {\n  ft %&gt;%\n    border_remove() %&gt;%\n    hline_top(border = fp_border(width = 2)) %&gt;%\n    hline_bottom(border = fp_border(width = 2)) %&gt;%\n    font(fontname = \"Arial\", part = \"all\") %&gt;%\n    fontsize(size = 10, part = \"body\") %&gt;%\n    fontsize(size = 11, part = \"header\")\n}",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Advanced Clinical Reporting with officer and flextable"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/officer-flextable.html#learning-outcomes",
    "href": "workshops/clinical-reporting/officer-flextable.html#learning-outcomes",
    "title": "Advanced Clinical Reporting with officer and flextable",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of this workshop, you will be able to:\nâœ… Create professional Word documents programmatically\nâœ… Build complex, pharmaceutical-standard tables\nâœ… Integrate ggplot2 visualizations seamlessly\nâœ… Manage document structure with sections and breaks\nâœ… Implement automated cross-referencing\nâœ… Generate complete CSRs from R scripts\nâœ… Apply best practices for reproducible reporting",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Advanced Clinical Reporting with officer and flextable"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/officer-flextable.html#troubleshooting",
    "href": "workshops/clinical-reporting/officer-flextable.html#troubleshooting",
    "title": "Advanced Clinical Reporting with officer and flextable",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nCommon Issues:\n\nTables not fitting page width â†’ Use autofit() or manual width() settings\nPlots appear pixelated â†’ Increase DPI with body_add_gg(width, height, res = 300)\nCross-references not updating â†’ Right-click â†’ â€œUpdate Fieldâ€ in Word\nStyles not applying â†’ Ensure template has required style definitions",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Advanced Clinical Reporting with officer and flextable"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/officer-flextable.html#integration-with-other-tools",
    "href": "workshops/clinical-reporting/officer-flextable.html#integration-with-other-tools",
    "title": "Advanced Clinical Reporting with officer and flextable",
    "section": "Integration with Other Tools",
    "text": "Integration with Other Tools\n\n{officedown}: R Markdown â†’ Word via officer\n{gtsummary}: Statistical tables â†’ flextable\n{officer} + {mschart}: Native Office charts\n{pharmaRTF}: Alternative for RTF output",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Advanced Clinical Reporting with officer and flextable"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/officer-flextable.html#next-steps",
    "href": "workshops/clinical-reporting/officer-flextable.html#next-steps",
    "title": "Advanced Clinical Reporting with officer and flextable",
    "section": "Next Steps",
    "text": "Next Steps\nAfter this workshop:\n\nExplore {officedown} for R Markdown integration\nTry {mschart} for editable Office charts\nInvestigate {pharmaRTF} for RTF submissions\nJoin ardata community for support\n\n\n\nSimilar Workshops\n\nCardinal: Clinical Reporting - Standardized TLG templates\nBranded Documents with Quarto - Alternative approach\n\n\n\nRelated Presentations\n\nautoslideR - Automated slide decks\n\n\n\nNext Steps\n\nFor tables: See Cardinal workshop\nFor multi-format: Try Quarto workshop\nIndustry adoption: Automation trends\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Advanced Clinical Reporting with officer and flextable"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/branded-quarto.html",
    "href": "workshops/clinical-reporting/branded-quarto.html",
    "title": "Creating Polished, Branded Documents with Quarto",
    "section": "",
    "text": "Beginner Friendly Quarto Reporting\nA hands-on workshop exploring the versatility of Quarto output formats. Learn to create dynamic websites, professional PDF documents, engaging presentations, and interactive dashboards - all with consistent branding using the new brand.yml feature.\n\n\n\nğŸŒ Dynamic websites with Quarto\nğŸ“„ Professional PDFs for reports\nğŸ“Š Presentations with reveal.js\nğŸ“ˆ Interactive dashboards\nğŸ¨ Brand.yml for consistent theming\nğŸ”„ Multi-format publishing",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Creating Polished, Branded Documents with Quarto"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/branded-quarto.html#overview",
    "href": "workshops/clinical-reporting/branded-quarto.html#overview",
    "title": "Creating Polished, Branded Documents with Quarto",
    "section": "",
    "text": "Beginner Friendly Quarto Reporting\nA hands-on workshop exploring the versatility of Quarto output formats. Learn to create dynamic websites, professional PDF documents, engaging presentations, and interactive dashboards - all with consistent branding using the new brand.yml feature.\n\n\n\nğŸŒ Dynamic websites with Quarto\nğŸ“„ Professional PDFs for reports\nğŸ“Š Presentations with reveal.js\nğŸ“ˆ Interactive dashboards\nğŸ¨ Brand.yml for consistent theming\nğŸ”„ Multi-format publishing",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Creating Polished, Branded Documents with Quarto"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/branded-quarto.html#prerequisites",
    "href": "workshops/clinical-reporting/branded-quarto.html#prerequisites",
    "title": "Creating Polished, Branded Documents with Quarto",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRequired Knowledge:\n\nBasic R or Python\nFamiliarity with Markdown\nNo prior Quarto experience needed",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Creating Polished, Branded Documents with Quarto"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/branded-quarto.html#key-features",
    "href": "workshops/clinical-reporting/branded-quarto.html#key-features",
    "title": "Creating Polished, Branded Documents with Quarto",
    "section": "Key Features",
    "text": "Key Features\n\nQuarto\n\n\nbrand.yml\n\n\nreveal.js\n\n\nLaTeX/PDF\n\n\nObservable",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Creating Polished, Branded Documents with Quarto"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/branded-quarto.html#workshop-materials",
    "href": "workshops/clinical-reporting/branded-quarto.html#workshop-materials",
    "title": "Creating Polished, Branded Documents with Quarto",
    "section": "Workshop Materials",
    "text": "Workshop Materials\n\n\n\n\n\n\nNoteResources\n\n\n\nPosit Cloud Workspace: 019a4f2d-6b79-72c1-834b-c2a9488f9ec8.share.connect.posit.cloud\nGitHub: github.com/ivelasq/2025-11-04_branded-quarto",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Creating Polished, Branded Documents with Quarto"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/branded-quarto.html#key-topics",
    "href": "workshops/clinical-reporting/branded-quarto.html#key-topics",
    "title": "Creating Polished, Branded Documents with Quarto",
    "section": "Key Topics",
    "text": "Key Topics\n\n1. Brand.yml for Consistent Theming\n# brand.yml\nbrand:\n  color:\n    primary: \"#276DC2\"\n    secondary: \"#4A90E2\"\n  typography:\n    base: \"Arial\"\n    headings: \"Arial\"\n  logo: \"logo.png\"\n\n\n2. Multi-Format Publishing\n\nHTML - Interactive websites and dashboards\nPDF - Publication-ready documents via LaTeX\nPresentations - Reveal.js slideshows\nDashboards - Data visualization apps\n\n\n\n3. Pharmaceutical Applications\n\nClinical study reports with consistent branding\nRegulatory submission documents\nInternal dashboards for data monitoring\nPresentations for study teams",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Creating Polished, Branded Documents with Quarto"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/branded-quarto.html#learning-outcomes",
    "href": "workshops/clinical-reporting/branded-quarto.html#learning-outcomes",
    "title": "Creating Polished, Branded Documents with Quarto",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nâœ… Create multiple output formats from single source\nâœ… Apply consistent branding across all outputs\nâœ… Build interactive dashboards\nâœ… Design professional presentations\nâœ… Generate PDF reports with LaTeX quality\n\n\nSimilar Workshops\n\nAdvanced Clinical Reporting - Word-focused approach\npointblank - Data documentation with Quarto\n\n\n\nTools & Resources\n\nQuarto in Tools Catalog - 4+ mentions across conference\n\n\n\nNext Steps\n\nThis site uses Quarto! Check the GitHub repo for examples\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Creating Polished, Branded Documents with Quarto"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/cardinal-tlgs.html",
    "href": "workshops/clinical-reporting/cardinal-tlgs.html",
    "title": "Hands on with Cardinal: Harmonizing Clinical Reporting",
    "section": "",
    "text": "Beginner Friendly Clinical Reporting TLGs\nCardinal is an open-source collection of standardized table, listing, and graph (TLG) templates designed to streamline the process of clinical output review, comparison, and meta-analysisâ€”promoting efficient communication to stakeholders while aligning with CDISCâ€™s ARD/ARM efforts.\n\n\n\nğŸ“Š {gtsummary} fundamentals for table creation\nğŸ“š Cardinalâ€™s TLG catalog and template library\nğŸ”§ Hands-on practice generating key clinical outputs\nğŸ¤ Contributing to the Cardinal project\nğŸ¯ Integration into your own workflows",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Hands on with Cardinal: Harmonizing Clinical Reporting"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/cardinal-tlgs.html#overview",
    "href": "workshops/clinical-reporting/cardinal-tlgs.html#overview",
    "title": "Hands on with Cardinal: Harmonizing Clinical Reporting",
    "section": "",
    "text": "Beginner Friendly Clinical Reporting TLGs\nCardinal is an open-source collection of standardized table, listing, and graph (TLG) templates designed to streamline the process of clinical output review, comparison, and meta-analysisâ€”promoting efficient communication to stakeholders while aligning with CDISCâ€™s ARD/ARM efforts.\n\n\n\nğŸ“Š {gtsummary} fundamentals for table creation\nğŸ“š Cardinalâ€™s TLG catalog and template library\nğŸ”§ Hands-on practice generating key clinical outputs\nğŸ¤ Contributing to the Cardinal project\nğŸ¯ Integration into your own workflows",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Hands on with Cardinal: Harmonizing Clinical Reporting"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/cardinal-tlgs.html#prerequisites",
    "href": "workshops/clinical-reporting/cardinal-tlgs.html#prerequisites",
    "title": "Hands on with Cardinal: Harmonizing Clinical Reporting",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRequired Knowledge:\n\nBasic R programming\nFamiliarity with clinical trial data (CDISC preferred)\nNo prior experience with {gtsummary} required\n\nRecommended:\n\nUnderstanding of common TLG types\nExperience with clinical study reports",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Hands on with Cardinal: Harmonizing Clinical Reporting"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/cardinal-tlgs.html#key-packages-tools",
    "href": "workshops/clinical-reporting/cardinal-tlgs.html#key-packages-tools",
    "title": "Hands on with Cardinal: Harmonizing Clinical Reporting",
    "section": "Key Packages & Tools",
    "text": "Key Packages & Tools\n\n{gtsummary}\n\n\n{cardinal}\n\n\n{gt}\n\n\n{cards}\n\n\nCDISC ADaM",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Hands on with Cardinal: Harmonizing Clinical Reporting"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/cardinal-tlgs.html#why-cardinal",
    "href": "workshops/clinical-reporting/cardinal-tlgs.html#why-cardinal",
    "title": "Hands on with Cardinal: Harmonizing Clinical Reporting",
    "section": "Why Cardinal?",
    "text": "Why Cardinal?\n\nThe Problem\n\nğŸ”„ Inconsistent formatting across studies and organizations\nâ±ï¸ Time-consuming manual table creation\nğŸ” Difficult to compare outputs across trials\nğŸ“ Repetitive coding for standard analyses\n\n\n\nThe Solution\nCardinal provides:\n\nâœ… Standardized templates for common TLGs\nâœ… Quality-controlled outputs aligned with CDISC\nâœ… Reusable code reducing development time\nâœ… Harmonized formats for meta-analysis",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Hands on with Cardinal: Harmonizing Clinical Reporting"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/cardinal-tlgs.html#introduction-to-gtsummary",
    "href": "workshops/clinical-reporting/cardinal-tlgs.html#introduction-to-gtsummary",
    "title": "Hands on with Cardinal: Harmonizing Clinical Reporting",
    "section": "Introduction to {gtsummary}",
    "text": "Introduction to {gtsummary}\n{gtsummary} is the foundation for creating publication-ready tables:\nlibrary(gtsummary)\nlibrary(dplyr)\n\n# Simple demographic table\ntrial %&gt;%\n  select(age, grade, trt) %&gt;%\n  tbl_summary(\n    by = trt,\n    statistic = list(\n      all_continuous() ~ \"{mean} ({sd})\",\n      all_categorical() ~ \"{n} ({p}%)\"\n    )\n  ) %&gt;%\n  add_p() %&gt;%\n  add_overall()\n\nKey Features\n\nIntuitive syntax for common analyses\nFlexible customization options\nBuilt-in statistical tests\nExport to multiple formats (Word, HTML, LaTeX)",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Hands on with Cardinal: Harmonizing Clinical Reporting"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/cardinal-tlgs.html#cardinal-tlg-catalog",
    "href": "workshops/clinical-reporting/cardinal-tlgs.html#cardinal-tlg-catalog",
    "title": "Hands on with Cardinal: Harmonizing Clinical Reporting",
    "section": "Cardinal TLG Catalog",
    "text": "Cardinal TLG Catalog\n\nTables\n\n1. Demographics and Baseline Characteristics\n\nSubject disposition\nDemographics (age, sex, race, ethnicity)\nBaseline disease characteristics\nMedical history\n\n\n\n2. Efficacy Tables\n\nPrimary/secondary endpoints\nSubgroup analyses\nTime-to-event summaries\nResponse rates\n\n\n\n3. Safety Tables\n\nAdverse events (by SOC, PT, severity)\nLaboratory values\nVital signs\nDeaths and serious AEs\n\n\n\n4. Disposition Tables\n\nStudy completion\nProtocol deviations\nExposure duration\n\n\n\n\nListings\n\nPatient profiles\nAdverse event listings\nLaboratory abnormalities\nConcomitant medications\n\n\n\nGraphs\n\nKaplan-Meier survival curves\nForest plots (subgroup analysis)\nSwimmer plots (treatment duration)\nWaterfall plots (tumor response)",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Hands on with Cardinal: Harmonizing Clinical Reporting"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/cardinal-tlgs.html#workshop-content",
    "href": "workshops/clinical-reporting/cardinal-tlgs.html#workshop-content",
    "title": "Hands on with Cardinal: Harmonizing Clinical Reporting",
    "section": "Workshop Content",
    "text": "Workshop Content\n\nSession 1: Getting Started with {gtsummary}\nBasic Table Creation:\nlibrary(gtsummary)\nlibrary(cardinal)\n\n# Load example CDISC data\ndata(\"adsl\", package = \"cardinal\")\n\n# Demographic table\ndemo_table &lt;- adsl %&gt;%\n  select(AGE, SEX, RACE, ARM) %&gt;%\n  tbl_summary(\n    by = ARM,\n    label = list(\n      AGE ~ \"Age (years)\",\n      SEX ~ \"Sex\",\n      RACE ~ \"Race\"\n    )\n  ) %&gt;%\n  add_p() %&gt;%\n  add_overall() %&gt;%\n  modify_header(label ~ \"**Characteristic**\") %&gt;%\n  bold_labels()\n\n\nSession 2: Cardinal Templates\nUsing Pre-built Templates:\n# Cardinal provides ready-to-use templates\ntemplate_demographics(\n  data = adsl,\n  treatment_var = \"ARM\"\n)\n\ntemplate_ae_summary(\n  data = adae,\n  treatment_var = \"ARM\",\n  ae_vars = c(\"AESOC\", \"AEDECOD\")\n)\n\n\nSession 3: Customization\nAdapting Templates to Your Needs:\n\nModifying statistical summaries\nCustom formatting rules\nAdding footnotes and titles\nStyling for different audiences (clinical vs.Â regulatory)\n\n\n\nSession 4: Advanced Topics\nComplex Tables:\n\nMulti-level headers\nNested statistics\nCross-tabulations\nSurvival analysis tables\n\n\n\nSession 5: Contributing to Cardinal\nHow to Get Involved:\n\nSubmit new template ideas\nContribute code improvements\nReport issues and bugs\nShare use cases and examples",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Hands on with Cardinal: Harmonizing Clinical Reporting"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/cardinal-tlgs.html#hands-on-exercises",
    "href": "workshops/clinical-reporting/cardinal-tlgs.html#hands-on-exercises",
    "title": "Hands on with Cardinal: Harmonizing Clinical Reporting",
    "section": "Hands-On Exercises",
    "text": "Hands-On Exercises\n\nExercise 1: Basic Demographics Table\nCreate a standard demographics table using your own data or provided examples.\n\n\nExercise 2: Adverse Events Summary\nGenerate a treatment-emergent adverse events (TEAE) table by system organ class.\n\n\nExercise 3: Efficacy Analysis\nBuild a primary endpoint analysis table with statistical comparisons.\n\n\nExercise 4: Custom Template\nModify an existing Cardinal template to match your organizationâ€™s standards.",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Hands on with Cardinal: Harmonizing Clinical Reporting"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/cardinal-tlgs.html#practical-applications",
    "href": "workshops/clinical-reporting/cardinal-tlgs.html#practical-applications",
    "title": "Hands on with Cardinal: Harmonizing Clinical Reporting",
    "section": "Practical Applications",
    "text": "Practical Applications\n\nClinical Study Reports (CSRs)\n\nConsistent formatting across studies\nRapid generation of standard tables\nEasy updates during review cycles\n\n\n\nMeta-Analysis\n\nHarmonized outputs from multiple trials\nSimplified data extraction\nComparable statistics across studies\n\n\n\nRegulatory Submissions\n\nCDISC-aligned outputs\nTraceable and reproducible code\nAudit-ready documentation",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Hands on with Cardinal: Harmonizing Clinical Reporting"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/cardinal-tlgs.html#cardinal-vs.-other-approaches",
    "href": "workshops/clinical-reporting/cardinal-tlgs.html#cardinal-vs.-other-approaches",
    "title": "Hands on with Cardinal: Harmonizing Clinical Reporting",
    "section": "Cardinal vs.Â Other Approaches",
    "text": "Cardinal vs.Â Other Approaches\n\n\n\n\n\n\n\n\n\nFeature\nCardinal + {gtsummary}\nTraditional Approach\nOther Tools\n\n\n\n\nStandardization\nâœ… High\nâŒ Low\nâš ï¸ Medium\n\n\nLearning Curve\nâœ… Gentle\nâŒ Steep\nâš ï¸ Moderate\n\n\nFlexibility\nâœ… High\nâœ… High\nâŒ Limited\n\n\nOpen Source\nâœ… Yes\nâš ï¸ Varies\nâŒ Often proprietary\n\n\nCDISC Alignment\nâœ… Built-in\nâŒ Manual\nâš ï¸ Varies\n\n\nCost\nâœ… Free\nâš ï¸ Varies\nâŒ Often expensive",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Hands on with Cardinal: Harmonizing Clinical Reporting"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/cardinal-tlgs.html#ardarm-integration",
    "href": "workshops/clinical-reporting/cardinal-tlgs.html#ardarm-integration",
    "title": "Hands on with Cardinal: Harmonizing Clinical Reporting",
    "section": "ARD/ARM Integration",
    "text": "ARD/ARM Integration\nCardinal aligns with CDISC Analysis Results Data (ARD) and Analysis Results Metadata (ARM) standards:\n\nStructured output metadata\nMachine-readable results\nFacilitates automation\nSupports regulatory review",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Hands on with Cardinal: Harmonizing Clinical Reporting"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/cardinal-tlgs.html#learning-outcomes",
    "href": "workshops/clinical-reporting/cardinal-tlgs.html#learning-outcomes",
    "title": "Hands on with Cardinal: Harmonizing Clinical Reporting",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of this workshop, you will be able to:\nâœ… Create professional clinical tables with {gtsummary}\nâœ… Use Cardinal templates for standard TLGs\nâœ… Customize outputs to match your organizationâ€™s needs\nâœ… Generate CDISC-aligned, reproducible tables\nâœ… Contribute to the Cardinal project\nâœ… Accelerate your clinical reporting workflows",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Hands on with Cardinal: Harmonizing Clinical Reporting"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/cardinal-tlgs.html#workshop-materials",
    "href": "workshops/clinical-reporting/cardinal-tlgs.html#workshop-materials",
    "title": "Hands on with Cardinal: Harmonizing Clinical Reporting",
    "section": "Workshop Materials",
    "text": "Workshop Materials\n\n\n\n\n\n\nNoteResources\n\n\n\nCardinal Website: https://pharmaverse.github.io/cardinal/\nGitHub: https://github.com/pharmaverse/cardinal\n{gtsummary} Documentation: https://www.danieldsjoberg.com/gtsummary/",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Hands on with Cardinal: Harmonizing Clinical Reporting"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/cardinal-tlgs.html#example-complete-workflow",
    "href": "workshops/clinical-reporting/cardinal-tlgs.html#example-complete-workflow",
    "title": "Hands on with Cardinal: Harmonizing Clinical Reporting",
    "section": "Example: Complete Workflow",
    "text": "Example: Complete Workflow\nlibrary(cardinal)\nlibrary(gtsummary)\nlibrary(gt)\n\n# 1. Load data\ndata(\"adsl\")\ndata(\"adae\")\n\n# 2. Create demographics table\ndemo &lt;- template_demographics(adsl, treatment_var = \"ARM\")\n\n# 3. Create AE summary\nae_summary &lt;- template_ae_summary(\n  adae,\n  treatment_var = \"ARM\",\n  soc_var = \"AESOC\",\n  pt_var = \"AEDECOD\"\n)\n\n# 4. Export to Word\ndemo %&gt;%\n  as_gt() %&gt;%\n  gt::gtsave(\"demographics.docx\")\n\nae_summary %&gt;%\n  as_gt() %&gt;%\n  gt::gtsave(\"adverse_events.docx\")",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Hands on with Cardinal: Harmonizing Clinical Reporting"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/cardinal-tlgs.html#future-of-cardinal",
    "href": "workshops/clinical-reporting/cardinal-tlgs.html#future-of-cardinal",
    "title": "Hands on with Cardinal: Harmonizing Clinical Reporting",
    "section": "Future of Cardinal",
    "text": "Future of Cardinal\nRoadmap:\n\nExpanded template library\nMore graph types\nEnhanced CDISC integration\nIntegration with {teal} and other pharmaverse tools\nMachine learning-assisted table suggestions",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Hands on with Cardinal: Harmonizing Clinical Reporting"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/cardinal-tlgs.html#getting-help",
    "href": "workshops/clinical-reporting/cardinal-tlgs.html#getting-help",
    "title": "Hands on with Cardinal: Harmonizing Clinical Reporting",
    "section": "Getting Help",
    "text": "Getting Help\n\nDocumentation: Comprehensive guides on the website\nGitHub Issues: Report bugs and request features\nSlack: pharmaverse Slack channel\nEmail: Contact maintainers directly",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Hands on with Cardinal: Harmonizing Clinical Reporting"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/cardinal-tlgs.html#next-steps",
    "href": "workshops/clinical-reporting/cardinal-tlgs.html#next-steps",
    "title": "Hands on with Cardinal: Harmonizing Clinical Reporting",
    "section": "Next Steps",
    "text": "Next Steps\nAfter this workshop:\n\nExplore the full Cardinal template catalog\nTry Cardinal in your next project\nJoin the pharmaverse community\nConsider contributing a template\n\n\n\n\n\n\n\nTipPro Tips\n\n\n\n\nStart with templates - Donâ€™t reinvent the wheel\nCustomize gradually - Master basics before advanced features\nDocument your modifications - Make it reproducible\nShare back - Your improvements can help others\nStay updated - Cardinal is actively developed\n\n\n\n\n\nSimilar Workshops\n\nAdvanced Clinical Reporting - Word documents with officer/flextable\nPolished Documents with Quarto - Multi-format publishing\n\n\n\nRelated Presentations\n\nBeyond {gtsummary}: {crane} - Extension for pharma\nLLM-Powered {gtsummary} - AI-enhanced tables\n\n\n\nTools & Resources\n\n{gtsummary} - Most mentioned package\n{cardinal} - Full catalog entry\n\n\n\nNext Steps\n\nFor Word output: Try officer/flextable workshop\nCareer skills: Clinical Reporting Automation\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Hands on with Cardinal: Harmonizing Clinical Reporting"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R/Pharma 2025 Conference Materials",
    "section": "",
    "text": "Comprehensive Conference Materials Repository\nYour guide to the cutting-edge of R in pharmaceutical research and development"
  },
  {
    "objectID": "index.html#conference-at-a-glance",
    "href": "index.html#conference-at-a-glance",
    "title": "R/Pharma 2025 Conference Materials",
    "section": "ğŸ“Š Conference at a Glance",
    "text": "ğŸ“Š Conference at a Glance\n\n\n\n19\nWorkshops\n\n\n8\nEU/US Sessions\n\n\n30+\nPresentations\n\n\nNov 2025\nEvent Date"
  },
  {
    "objectID": "index.html#top-highlights",
    "href": "index.html#top-highlights",
    "title": "R/Pharma 2025 Conference Materials",
    "section": "ğŸ”¥ Top Highlights",
    "text": "ğŸ”¥ Top Highlights\n\n\n\nğŸ¤– AI Revolution\n8+ sessions focused on LLM integration with R, including practical implementations with {ellmer}, {shinychat}, and enterprise AI solutions.\n\n\n\nğŸ“Š Open Source Momentum\nGSKâ€™s journey to 50%+ R code, industry-wide shift from SAS to open-source tools, and collaborative development through pharmaverse.\n\n\n\nâœ… Regulatory Acceptance\nMultiple talks on validation in GxP contexts, FDA guidance implementation, and production-ready AI applications in clinical trials."
  },
  {
    "objectID": "index.html#quick-navigation",
    "href": "index.html#quick-navigation",
    "title": "R/Pharma 2025 Conference Materials",
    "section": "ğŸ¯ Quick Navigation",
    "text": "ğŸ¯ Quick Navigation\n\n\n\nğŸ“š Workshops\n19 hands-on sessions covering:\n\nğŸ¤– AI & LLM Integration (3 workshops)\nğŸ“Š Clinical Reporting (5 workshops)\nğŸ”§ Package Development (3 workshops)\nğŸ“ˆ Statistical Methods (4 workshops)\nğŸ§¬ Specialized Applications (4 workshops)\n\nBrowse all workshops â†’\n\n\n\nğŸ¤ Presentations\n30+ talks organized in:\n\n8 Europe/US thematic sessions\n13 Asia/Pacific presentations\nTopics from AI agents to clinical study reporting\n\nExplore presentations â†’"
  },
  {
    "objectID": "index.html#key-insights-analysis",
    "href": "index.html#key-insights-analysis",
    "title": "R/Pharma 2025 Conference Materials",
    "section": "ğŸ’¡ Key Insights & Analysis",
    "text": "ğŸ’¡ Key Insights & Analysis\n\nğŸ“ˆ Trends & Analysis\nComprehensive analysis of emerging trends, including:\n\nThe AI/LLM revolution in pharma R programming\nOpen-source adoption patterns and success stories\nRegulatory landscape evolution\nFuture directions for R in clinical development\n\nRead full analysis â†’\n\n\nğŸ› ï¸ Tools Catalog\nComplete A-Z reference of packages and tools mentioned across workshops and presentations:\n\nMost mentioned: {gtsummary}, {ellmer}, {teal}, {officer}, {flextable}\nEmerging stars: {cards}, {crane}, {admiral}, {polars}\nAI ecosystem: {ellmer}, {shinychat}, {ragnar}, {mcpr}\n\nBrowse complete catalog â†’\n\n\nğŸ’¼ Career Insights\nPractical guidance for R professionals in pharma:\n\nSkills in high demand\nTransitioning from SAS to R\nBuilding validation expertise\nContributing to open source\n\nView career guidance â†’"
  },
  {
    "objectID": "index.html#featured-workshops",
    "href": "index.html#featured-workshops",
    "title": "R/Pharma 2025 Conference Materials",
    "section": "ğŸŒŸ Featured Workshops",
    "text": "ğŸŒŸ Featured Workshops\n\n\n\nGetting Started with LLM APIs in R\nSara Altman (Posit PBC)\nLearn to integrate LLMs into R workflows using {ellmer}. Build chatbots, implement tool calling, and design effective system prompts.\nBeginner Friendly AI/LLM\n\n\n\nAdvanced Clinical Reporting with officer and flextable\nDavid Gohel (Ardata)\nMaster sophisticated clinical reports in Word format using R. From complex tables to complete pharmaceutical report generation.\nIntermediate Clinical Reporting\n\n\n\n\n\n\nIntroduction to Building (Better) R Packages\nNicola Rennie (Data Viz Specialist)\nTransform scripts into packages. Learn documentation, testing, and best practices. By the end, youâ€™ll have created your own R package!\nBeginner Friendly Development\n\n\n\nFrom Data to Insights: Hands-On with {teal}\nNina Qi & Dony Unardi (Genentech)\nBuild interactive clinical trial data exploration apps with the {teal} framework. Based on the latest 1.0 release.\nIntermediate Shiny/Apps"
  },
  {
    "objectID": "index.html#about-this-repository",
    "href": "index.html#about-this-repository",
    "title": "R/Pharma 2025 Conference Materials",
    "section": "ğŸ“Œ About This Repository",
    "text": "ğŸ“Œ About This Repository\nThis repository provides comprehensive documentation of the R/Pharma 2025 conference, including:\n\nDetailed workshop descriptions with prerequisites and learning objectives\nPresentation summaries and key takeaways\nTrend analysis and industry insights\nComplete catalog of tools and packages\nCareer guidance for R professionals in pharma\n\nAll materials are organized for easy navigation and reference, suitable for:\n\nğŸ“ Attendees reviewing conference materials\nğŸ” Researchers exploring R solutions for pharma workflows\nğŸ’¼ Hiring managers assessing industry trends and skills\nğŸš€ Developers discovering new tools and best practices\n\n\n\n\n\n\n\n\nTipContributing & Feedback\n\n\n\nThis is a living repository. If you notice errors or have suggestions for improvements, please feel free to open an issue or submit a pull request on GitHub.\n\n\n\nCompiled and curated by Szymon Myrta | Last updated: November 2025"
  },
  {
    "objectID": "summary/trends-insights.html",
    "href": "summary/trends-insights.html",
    "title": "Trends & Insights",
    "section": "",
    "text": "R/Pharma 2025 marked a pivotal moment in pharmaceutical R adoption, with three dominant themes emerging: AI/LLM integration, open-source transformation, and regulatory acceptance. The conference showcased 19 workshops and 30+ presentations revealing an industry in rapid transition from proprietary tools to collaborative, standards-based approaches.\nKey Statistics:\n\nğŸ¤– 8+ sessions on AI/LLM (50% increase from previous years)\nğŸ“Š GSK achieved 50%+ R code across Biostatistics\nâœ… FDA neutrality on statistical software increasingly normalized\nğŸŒ CDISC ARS/ARM driving automation and standardization"
  },
  {
    "objectID": "summary/trends-insights.html#executive-summary",
    "href": "summary/trends-insights.html#executive-summary",
    "title": "Trends & Insights",
    "section": "",
    "text": "R/Pharma 2025 marked a pivotal moment in pharmaceutical R adoption, with three dominant themes emerging: AI/LLM integration, open-source transformation, and regulatory acceptance. The conference showcased 19 workshops and 30+ presentations revealing an industry in rapid transition from proprietary tools to collaborative, standards-based approaches.\nKey Statistics:\n\nğŸ¤– 8+ sessions on AI/LLM (50% increase from previous years)\nğŸ“Š GSK achieved 50%+ R code across Biostatistics\nâœ… FDA neutrality on statistical software increasingly normalized\nğŸŒ CDISC ARS/ARM driving automation and standardization"
  },
  {
    "objectID": "summary/trends-insights.html#the-aillm-revolution",
    "href": "summary/trends-insights.html#the-aillm-revolution",
    "title": "Trends & Insights",
    "section": "1. The AI/LLM Revolution ğŸ¤–",
    "text": "1. The AI/LLM Revolution ğŸ¤–\n\nEmergence as Dominant Theme\nAI and Large Language Models were the most discussed topic, appearing in:\n\n3 dedicated workshops (ellmer basics, enterprise tooling, clinical data privacy)\n8+ presentations across all sessions\nMultiple vendor tools ({ellmer}, {llumen}, {mcpr}, Databot, {meRlin})\n\n\n\nFrom Prototype to Production\nThe conversation has matured from â€œCan we use AI?â€ to â€œHow do we deploy it safely?â€\nKey Developments:\n\n1. Enterprise-Ready Frameworks\nOrganizations are building production AI systems:\n\nMerckâ€™s {llumen} - Comprehensive agentic framework used internally\n\nSupports multiple LLM sources (Azure, OpenAI, local)\nRAG with vector databases for documents\nDatabase querying (SQL, Cypher, GraphQL)\nFoundation model integration (TxGemma)\n\nRocheâ€™s Multi-Agent Copilot - Package-specific AI agents\n\nEach internal package gets its own expert agent\nLangGraph orchestration\nIntegration with Cursor via MCP\n\nA2-AIâ€™s GxP Solutions - Validated AI workflows\n\nAWS Bedrock integration\nMCP server implementations\nCompliance documentation\n\n\n\n\n2. Privacy-Preserving Approaches\nCritical for pharma with sensitive clinical data:\n\nOn-premise LLM deployments (llama.cpp)\nQuery sanitization removing PII\nAudit logging for compliance\nRole-based access controls\n{DataChat} example - Chat interface with data never leaving secure environment\n\n\n\n3. Practical Applications\nMoving beyond novelty to genuine productivity:\nData Exploration:\n\nNatural language queries on CDISC data\nConversational interfaces for non-programmers\nRAG-powered document search\n\nCode Assistance:\n\nContext-aware programming help\nDebugging complex statistical code\nDocumentation generation\n\nReport Automation:\n\nLLM-powered table summarization\nNarrative generation from results\nQC log generation\n\n\n\n\nStandards Emerging\nModel Context Protocol (MCP):\n\nStandardized interface for AI tools\nR implementation via {mcpr}\nCross-language interoperability\nGrowing ecosystem (Claude Code, Cursor, VS Code)\n\n\n\nChallenges Identified\nDespite enthusiasm, several barriers remain:\n\nâš ï¸ Validation complexity - How to validate non-deterministic outputs\nâš ï¸ Cost management - API costs at scale\nâš ï¸ Regulatory uncertainty - Limited guidance on AI in submissions\nâš ï¸ Skills gap - Prompt engineering and AI orchestration expertise"
  },
  {
    "objectID": "summary/trends-insights.html#open-source-transformation",
    "href": "summary/trends-insights.html#open-source-transformation",
    "title": "Trends & Insights",
    "section": "2. Open-Source Transformation ğŸŒ",
    "text": "2. Open-Source Transformation ğŸŒ\n\nGSKâ€™s Landmark Achievement\nSam Wardenâ€™s presentation on GSKâ€™s journey was a conference highlight:\nTimeline:\n\n2018-2020: Pilot programs and early adopters\n2020-2022: COVID acceleration and platform adoption\n2023-2024: 50%+ R code target achieved\n2025: Rburst initiative for full integration\n\nSuccess Factors:\n\nExecutive commitment - Clear target setting (50%+ R)\nMulti-wave approach - Gradual transformation\nTraining investment - Bookdown courses, Resource Hub, AccelerateR\nPlatform deployment - Posit Workbench for infrastructure\nCultural change - Growth mindset and adaptability\n\n\n\nIndustry-Wide Movement\nGSK is not alone - the tipping point has been reached:\nNovartis:\n\nMosaic platform for ARS-driven TFL automation\nStandards-based, language-agnostic approach\nReact UI with R backend\n\nRoche/Genentech:\n\nLeading {admiral}, {teal}, {gtsummary} development\nautoslideR for slide automation\n{crane} for pharma-specific reporting\n\nPfizer:\n\nCustom R packages for RWD programming\nDual R/SAS syntax support\nQuarto documentation sites\n\nModerna:\n\nAI-enhanced Shiny apps for trial data\nellmer/GPT integration\nAccelerating exploratory analysis\n\n\n\nPharmaverse Ecosystem\nThe collaborative open-source movement is thriving:\nKey Packages:\n\n{admiral} - Transitioning to stable maintenance (feature complete)\n{teal} - Version 1.0 released (interactive apps)\n{gtsummary}/{crane} - Clinical table generation\n{cardinal} - Standardized TLG templates\n{sdtm.oak} - SDTM programming framework\n\nBenefits Realized:\n\nâœ… Reduced duplication across companies\nâœ… Shared validation burden\nâœ… Faster innovation cycles\nâœ… Transparent, auditable code"
  },
  {
    "objectID": "summary/trends-insights.html#regulatory-evolution",
    "href": "summary/trends-insights.html#regulatory-evolution",
    "title": "Trends & Insights",
    "section": "3. Regulatory Evolution âœ…",
    "text": "3. Regulatory Evolution âœ…\n\nFDA Software Neutrality\nThe 2015 FDA clarification on software neutrality is now fully operationalized:\n\nNo preference for SAS vs R vs Python\nFocus on methodology and validation, not tools\nR-based submissions increasingly routine\n\n\n\nNew FDA Guidance Impact\n2023 Covariate Adjustment Guidance driving change:\n\nNovartis developed {beeca} in response\nASA-BIOP collaboration on {RobinCar2}\nAcademic methods reaching practice faster\nR enabling rapid response to guidance\n\n\n\nValidation Maturity\nKey Developments:\n\n1. Risk-Based Approaches\n\n{riskmetric} for package assessment\nShared metric repositories (R Validation Hub)\nAutomated quality scoring\nFocus on critical packages\n\n\n\n2. Acceptance-Test Driven Development (ATDD)\n\nPlain-language tests in Quarto\nShareable with regulators\nâ€œGiven-when-thenâ€ format\nBrian Repko bringing JBehave concepts to R\n\n\n\n3. Shiny App Validation\n\nLitmusverse suite (Jumping Rivers)\nRisk-based validation frameworks\nCode quality assessment\nTraceability and documentation tools\n\n\n\n\nGxP-Ready AI\nPioneering work on validating AI applications:\n\nDevin Pastoor (A2-AI): Production AI in GxP contexts\nTesting strategies for non-deterministic outputs\nAudit trails and logging\nVersion control and rollback procedures"
  },
  {
    "objectID": "summary/trends-insights.html#automation-efficiency",
    "href": "summary/trends-insights.html#automation-efficiency",
    "title": "Trends & Insights",
    "section": "4. Automation & Efficiency ğŸ“Š",
    "text": "4. Automation & Efficiency ğŸ“Š\n\nARS/ARM as Game Changers\nCDISC Analysis Results Standard driving automation:\nMosaic (Novartis):\n\nYAML captures ARD requirements\nLanguage-agnostic rules (metadata â†’ R or any language)\nReact UI for customization\nPush-button TFL generation\n\n{cards} Ecosystem:\n\nAnalysis Results Data objects\nQC becomes straightforward (compare ARDs)\nLLMs can summarize language-agnostic results\n{crane} and {gtsummary} building on this foundation\n\n\n\nReport Automation\nTime Savings Achieved:\n\nautoslideR (Roche): 0.5-4 days per slide deck\nDMC materials automation: Week â†’ Day for exposure reports (AstraZeneca)\nLLM-powered {gtsummary}: Submission-ready tables in minutes vs hours\n\nTechnologies Enabling This:\n\nofficer/flextable - Programmatic Word reports\nQuarto - Multi-format publishing (PDF, HTML, presentations)\nTemplate systems - Reusable structures (Cardinal)\nCI/CD pipelines - Automated reanalysis on code changes"
  },
  {
    "objectID": "summary/trends-insights.html#advanced-analytics-methods",
    "href": "summary/trends-insights.html#advanced-analytics-methods",
    "title": "Trends & Insights",
    "section": "5. Advanced Analytics & Methods ğŸ“ˆ",
    "text": "5. Advanced Analytics & Methods ğŸ“ˆ\n\nBayesian Methods Maturing\nTools Reaching Production:\n\nBayesERtools (Genentech) - Exposure-response with Stan\nbmstate (Generable) - Multistate survival models\nStan debugging workshop - Making Bayesian accessible\n\nAdvantages in Pharma:\n\nPrior information integration\nUncertainty quantification\nSmall sample performance\nComplex model flexibility\n\n\n\nHigh-Performance Computing\nPolars for Clinical Data:\n\n10-100x faster than pandas\nApache Arrow native\nLazy evaluation\nParallel processing out-of-the-box\n\nHPC Integration:\n\nOffloading Shiny computations to clusters\nMaintaining interactive UX\nResource optimization\n\n\n\nMachine Learning\nTabPFN - Novel deep learning for tabular data:\n\nNo training required (pre-trained on priors)\nFast inference\nBayesian-like uncertainty\nPromising for exploratory analysis\n\nSynthetic Data:\n\n{synthpop} for privacy-preserving data sharing\nSoftware testing without PHI\nModel training on synthetic patients"
  },
  {
    "objectID": "summary/trends-insights.html#data-quality-validation",
    "href": "summary/trends-insights.html#data-quality-validation",
    "title": "Trends & Insights",
    "section": "6. Data Quality & Validation ğŸ”",
    "text": "6. Data Quality & Validation ğŸ”\n\n{pointblank} Adoption\nComprehensive data validation framework:\nUse Cases:\n\nQuick dataset understanding (scan_data())\nValidation at scale (35+ tables daily)\nBeautiful automated documentation\nIntegration with databases, Arrow, Shiny\n\nPharmaceutical Applications:\n\nSDTM compliance checks\nADaM dataset verification\nCross-domain consistency\nLongitudinal data integrity\n\n\n\nAutomated Traceability\nDevOps Principles in Pharma:\nGraticuleâ€™s approach:\n\nDocker containers for reproducibility\nCI/CD for automatic reanalysis\nVersion control with validated outputs\nCloud storage (AWS S3) for results\n\nBenefits:\n\nClear audit trail\nReproducible analyses\nAutomated documentation\nIntegrated QC"
  },
  {
    "objectID": "summary/trends-insights.html#specialized-applications",
    "href": "summary/trends-insights.html#specialized-applications",
    "title": "Trends & Insights",
    "section": "7. Specialized Applications ğŸ§¬",
    "text": "7. Specialized Applications ğŸ§¬\n\nReal-World Evidence\nPfizerâ€™s RWD Programming:\n\nCustom R package for database queries\nDual R/SAS syntax support\nShiny apps for workflow support\nQuarto documentation\n\n\n\nNiche Medical Devices\nAbbottâ€™s Synthetic Data:\n\n{synthpop} for diagnostics\nPrivacy-preserving test data\nValidation datasets\n\n\n\nOncology Innovations\nPMDA Inquiries:\n\n{cards} for rapid response (Japan regulatory)\nStructured ARD approach\nAccelerated turnaround\n\n\n\nPharmacokinetics\naNCA (Pharmaverse):\n\nOpen-source NCA software\nPKNCA backend (200+ parameters)\n100% test coverage\nValidated against commercial tools (Â±0.1%)"
  },
  {
    "objectID": "summary/trends-insights.html#regional-insights",
    "href": "summary/trends-insights.html#regional-insights",
    "title": "Trends & Insights",
    "section": "8. Regional Insights ğŸŒ",
    "text": "8. Regional Insights ğŸŒ\n\nAsia/Pacific Innovations\nStrengths:\n\nStrong PMDA regulatory focus\nPractical automation tools\nUnder-represented population research\nGenetic diversity considerations\n\nNotable Presentations:\n\nSHIONOGIâ€™s open-source culture\n{meRlin} AI assistant\nPMDA e-CRT automation\nVibe Coding approaches\n\n\n\nGlobal Collaboration\nConference demonstrated truly global R pharma community:\n\nWorkshops from US, Europe, Asia\nShared challenges and solutions\nPharmaverse as unifying force"
  },
  {
    "objectID": "summary/trends-insights.html#future-directions",
    "href": "summary/trends-insights.html#future-directions",
    "title": "Trends & Insights",
    "section": "9. Future Directions ğŸ”®",
    "text": "9. Future Directions ğŸ”®\n\nShort-Term (2025-2026)\nAI Integration:\n\nâœ… More validated AI applications in production\nâœ… MCP ecosystem expansion\nâœ… Regulatory guidance on AI in submissions\nâœ… Cost-effective on-premise LLM solutions\n\nAutomation:\n\nâœ… ARS/ARM adoption across industry\nâœ… Push-button TFL generation becomes standard\nâœ… Automated slide decks and CSRs routine\n\nValidation:\n\nâœ… Shared metric repositories operational\nâœ… Risk-based validation industry standard\nâœ… Shiny app validation frameworks mature\n\n\n\nMedium-Term (2027-2028)\nTool Consolidation:\n\nLanguage-agnostic analysis platforms\nR/Python/Julia interoperability\nCloud-native pharma analytics\n\nRegulatory:\n\nAI-specific guidance documents\nElectronic submissions with code\nReal-time regulatory review platforms\n\nSkills:\n\nAI/LLM literacy standard for programmers\nBayesian methods more accessible\nDevOps practices ubiquitous\n\n\n\nLong-Term (2029+)\nTransformative Possibilities:\n\nAI-Designed Clinical Trials\n\nLLMs suggesting optimal designs\nAutomated SAP generation\nReal-time adaptation\n\nFully Automated TFLs\n\nFrom data lock to submission package\nHuman review only, no coding\nMulti-language outputs for global submissions\n\nReal-Time Safety Monitoring\n\nContinuous analysis during trials\nAI-detected signals\nPredictive adverse event modeling\n\nPersonalized Trial Design\n\nBayesian adaptive with real-time learning\nBiomarker-driven enrollment\nPrecision medicine integration"
  },
  {
    "objectID": "summary/trends-insights.html#challenges-barriers",
    "href": "summary/trends-insights.html#challenges-barriers",
    "title": "Trends & Insights",
    "section": "10. Challenges & Barriers âš ï¸",
    "text": "10. Challenges & Barriers âš ï¸\n\nTechnical Challenges\nValidation Complexity:\n\nNon-deterministic AI outputs\nRapidly evolving tools\nKeeping pace with innovation\n\nIntegration Issues:\n\nLegacy system compatibility\nData silos\nIT security constraints\n\n\n\nOrganizational Challenges\nCultural Resistance:\n\nâ€œSAS is what weâ€™ve always usedâ€\nFear of change\nRisk aversion in regulated environment\n\nResource Constraints:\n\nTraining investment required\nDual maintenance (SAS + R) during transition\nValidation documentation burden\n\n\n\nRegulatory Challenges\nUncertainty:\n\nLimited AI/LLM guidance\nEvolving expectations\nRegional variations (FDA vs EMA vs PMDA)\n\nDocumentation:\n\nWhat level of detail for AI systems?\nHow to handle model updates?\nThird-party API dependencies"
  },
  {
    "objectID": "summary/trends-insights.html#success-patterns",
    "href": "summary/trends-insights.html#success-patterns",
    "title": "Trends & Insights",
    "section": "11. Success Patterns ğŸ¯",
    "text": "11. Success Patterns ğŸ¯\n\nWhat Works\nBased on successful implementations presented:\n1. Executive Sponsorship\n\nClear targets (GSKâ€™s 50%)\nResource commitment\nLong-term vision\n\n2. Gradual Transformation\n\nMulti-wave approach\nPilot programs\nLearn and adapt\n\n3. Support Infrastructure\n\nTraining programs (not just one-time)\nHelp desks and office hours\nDocumentation and resources\n\n4. Community Building\n\nInternal R user groups\nExternal pharmaverse participation\nShared learning culture\n\n5. Platform Investment\n\nPosit Workbench\nVersion control (Git/GitHub)\nCI/CD pipelines\nCloud infrastructure\n\n6. Standards Adoption\n\nCDISC ARS/ARM\nPharmaverse conventions\nStyle guides and linters"
  },
  {
    "objectID": "summary/trends-insights.html#actionable-insights",
    "href": "summary/trends-insights.html#actionable-insights",
    "title": "Trends & Insights",
    "section": "12. Actionable Insights ğŸ’¡",
    "text": "12. Actionable Insights ğŸ’¡\n\nFor Organizations Starting R Adoption\n\nStart with low-risk projects (exploratory analysis, visualizations)\nInvest in training early and continuously\nBuild internal community (lunch & learns, Slack channels)\nAdopt pharmaverse packages (donâ€™t reinvent)\nEstablish validation framework from day one\nUse Posit products (Workbench, Connect, Package Manager)\n\n\n\nFor Organizations Scaling R\n\nEmbrace automation (ARS/ARM, template systems)\nImplement CI/CD for reproducibility\nExplore AI carefully (privacy first, validate thoroughly)\nContribute to pharmaverse (share internal packages)\nFormalize support model (beyond training)\nPlan SAS sunset (donâ€™t maintain dual indefinitely)\n\n\n\nFor R Professionals\n\nLearn AI/LLM basics (will be essential skill)\nMaster one reporting framework (officer/flextable or Quarto)\nUnderstand validation (not just coding)\nContribute to open source (career differentiator)\nStay current (R/Pharma, webinars, blogs)\nNetwork (pharmaverse Slack, conferences)"
  },
  {
    "objectID": "summary/trends-insights.html#the-bigger-picture",
    "href": "summary/trends-insights.html#the-bigger-picture",
    "title": "Trends & Insights",
    "section": "13. The Bigger Picture ğŸŒ",
    "text": "13. The Bigger Picture ğŸŒ\n\nR/Pharma 2025 in Context\nThis conference revealed an industry at an inflection point:\nFrom:\n\nProprietary, siloed tools\nManual, repetitive coding\nConservative, risk-averse culture\nIsolated company solutions\n\nTo:\n\nOpen-source, collaborative development\nAutomated, standards-driven workflows\nInnovative, evidence-based adoption\nShared industry platforms\n\n\n\nWhy This Matters\nFor Patients:\n\nFaster drug development\nMore rigorous analysis\nTransparent, reproducible science\nPrecision medicine enablement\n\nFor Industry:\n\nReduced costs\nFaster time to market\nBetter talent recruitment\nCompetitive advantage\n\nFor Science:\n\nReproducibility crisis addressed\nMethod innovation accelerated\nGlobal collaboration enabled\nDemocratized advanced analytics"
  },
  {
    "objectID": "summary/trends-insights.html#how-rpharma-has-evolved",
    "href": "summary/trends-insights.html#how-rpharma-has-evolved",
    "title": "Trends & Insights",
    "section": "14. How R/Pharma Has Evolved ğŸ“ˆ",
    "text": "14. How R/Pharma Has Evolved ğŸ“ˆ\n\nConference Evolution (2018-2025)\nBased on publicly available information and this yearâ€™s content, hereâ€™s how the conference themes have shifted:\n\n2018-2020: Foundation Years\nKey Themes:\n\nğŸ“¦ Package development basics - Building pharma packages\nğŸ“Š Basic Shiny apps - Interactive visualizations\nğŸ”„ SAS vs R debates - â€œShould we switch?â€\nğŸ“ Simple reporting - Basic TFLs\n\nSentiment: Cautious optimism, early adopters sharing success stories\nNotable:\n\nPharmaverse not yet established\nValidation was major concern\nFDA software neutrality clarification (2015) still fresh\n\n\n\n\n2021-2022: Acceleration Phase\nKey Themes:\n\nğŸš€ COVID acceleration - Remote work drove R adoption\nğŸ¤ Pharmaverse launch - Collaborative ecosystem born\nâœ… Validation frameworks - {riskmetric}, R Validation Hub\nğŸ“¦ Production packages - {admiral}, {teal} gaining traction\n\nSentiment: Growing confidence, momentum building\nMilestones:\n\nMajor pharma (Roche, Novartis) sharing production solutions\nRegulatory submissions with R increasing\nTraining programs formalized (GSK example)\n\n\n\n\n2023-2024: Mainstream Adoption\nKey Themes:\n\nğŸ“Š CDISC integration - Analysis Results Standard (ARS/ARM)\nğŸ”§ Automation focus - Template-based TFL generation\nğŸ“ˆ Advanced analytics - Bayesian methods, ML\nğŸŒ Multi-language - R + Python workflows\n\nSentiment: R is mainstream, focus shifts to optimization\nEvidence:\n\nMultiple talks on automation (not just feasibility)\nValidation becoming standardized, less controversial\nAdvanced topics (multistate models, Stan) gaining space\n\n\n\n\n2025: AI Revolution Year\nDominant Themes:\n\nğŸ¤– AI/LLM explosion - 8+ sessions (50% increase)\nğŸ¢ Enterprise transformation - GSKâ€™s 50%+ achievement\nâœ… Regulatory maturity - GxP-ready AI, validated Shiny\nğŸš€ Production at scale - From â€œcan we?â€ to â€œhow fast?â€\n\nShift in Discourse:\n\n2018: â€œIs R viable for pharma?â€\n2022: â€œHow do we migrate from SAS?â€\n2025: â€œHow do we integrate AI with R?â€\n\nWhatâ€™s New in 2025:\n\nAI Dominance\n\n2018-2022: Barely mentioned\n2023: Experimental projects\n2024: Pilot implementations\n2025: Production systems ({llumen}, multi-agent copilots, enterprise frameworks)\n\nRegulatory Confidence\n\n2018-2020: â€œWill FDA accept this?â€\n2021-2022: â€œHereâ€™s how we validatedâ€\n2023-2024: â€œValidation frameworks establishedâ€\n2025: â€œValidated AI in GxPâ€ - previously unthinkable\n\nSpeed of Innovation\n\n2018: Yearly package updates\n2022: Quarterly releases (pharmaverse)\n2025: Real-time AI integration - tools released months ago\n\nCollaboration Level\n\n2018: Individual company solutions\n2020: Pharmaverse begins\n2023: Cross-company working groups\n2025: Shared AI infrastructure (MCP, metric repos)\n\n\n\n\n\n\nTopic Frequency Evolution\n\n\n\nTopic\n2020\n2022\n2024\n2025\n\n\n\n\nPackage Development\nğŸ”¥ğŸ”¥ğŸ”¥\nğŸ”¥ğŸ”¥\nğŸ”¥\nğŸ”¥\n\n\nValidation\nğŸ”¥ğŸ”¥ğŸ”¥\nğŸ”¥ğŸ”¥ğŸ”¥\nğŸ”¥ğŸ”¥\nğŸ”¥\n\n\nShiny Apps\nğŸ”¥ğŸ”¥\nğŸ”¥ğŸ”¥ğŸ”¥\nğŸ”¥ğŸ”¥\nğŸ”¥ğŸ”¥\n\n\nClinical Reporting\nğŸ”¥ğŸ”¥\nğŸ”¥ğŸ”¥\nğŸ”¥ğŸ”¥ğŸ”¥\nğŸ”¥ğŸ”¥ğŸ”¥\n\n\nBayesian Methods\nğŸ”¥\nğŸ”¥\nğŸ”¥ğŸ”¥\nğŸ”¥ğŸ”¥\n\n\nAI/LLM\n-\n-\nğŸ”¥\nğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥\n\n\nAutomation\nğŸ”¥\nğŸ”¥ğŸ”¥\nğŸ”¥ğŸ”¥\nğŸ”¥ğŸ”¥ğŸ”¥\n\n\nPython Integration\nğŸ”¥\nğŸ”¥\nğŸ”¥ğŸ”¥\nğŸ”¥ğŸ”¥\n\n\n\nInterpretation:\n\nTraditional topics (package dev, validation) declining as they become solved problems\nAI/LLM emerged from nowhere to dominate the conversation\nAutomation shifted from â€œnice to haveâ€ to strategic imperative\nPython integration normalized (not R vs Python, but R and Python)\n\n\n\n\nWhat Changed Between 2024 and 2025?\nMajor Shifts:\n\n{ellmer} Package Launch (2024)\n\nMade LLM integration trivial in R\nMultiple workshops/presentations built on it\nEnterprise adoption accelerated\n\nClaude/ChatGPT API Maturity\n\nFunction calling standardized\nMCP protocol emerged\nPrivacy-preserving deployments viable\n\nGSK Milestone\n\n50%+ R code achieved\nProof that full transformation possible\nOther pharma following suit\n\nCDISC ARS Adoption\n\nAnalysis Results Standard gaining traction\nMosaic (Novartis) and others automating TFLs\nMachine-readable results enabling AI\n\nPosit Product Evolution\n\nPositron IDE launched (VS Code + R)\nBetter Python support\nAI assistants integrated\n\n\nWhat Stayed Constant:\n\nâœ… Pharmaverse continues thriving\nâœ… Validation remains priority (but less controversial)\nâœ… Collaboration over competition\nâœ… Open-source commitment\n\n\n\n\nPredictions for R/Pharma 2026\nBased on the trajectory, expect:\nLikely Topics:\n\nâ€œAI in Submissionsâ€ - First AI-powered analysis in regulatory filing\nâ€œReal-time Clinical Trial Analysisâ€ - Adaptive designs with AI\nâ€œSynthetic Patient Generationâ€ - Privacy-preserving AI for trials\nâ€œQuantum ML for Drug Discoveryâ€ - Cutting edge\nâ€œFully Automated Study Reportsâ€ - From data lock to CSR in hours\n\nMaturing Topics:\n\nLLM integration will be assumed (not taught from scratch)\nValidation frameworks standardized across industry\nPython-R seamless interop (not separate tracks)\n\nDeclining Topics:\n\nâ€œIntro to R packagesâ€ (basics widely known)\nâ€œWhy leave SAS?â€ (decision already made)\nâ€œCan we validate R?â€ (settled question)\n\nBold Prediction:\nR/Pharma 2026 will feature the first fully AI-generated clinical study report accepted by a regulatory agency. The discussion will shift from â€œIs this possible?â€ to â€œWhat should humans still review?â€"
  },
  {
    "objectID": "summary/trends-insights.html#conclusion",
    "href": "summary/trends-insights.html#conclusion",
    "title": "Trends & Insights",
    "section": "15. Conclusion",
    "text": "15. Conclusion\nR/Pharma 2025 demonstrated that open-source pharmaceutical analytics is not the future itâ€™s the present. With AI integration, regulatory acceptance, and enterprise adoption converging, the question is no longer â€œShould we adopt R?â€ but â€œHow fast can we transform?â€\nThe conference showcased:\n\nâœ… Production-ready AI systems\nâœ… Validated, GxP-compliant workflows\nâœ… Major pharma success stories (GSK 50%+)\nâœ… Thriving ecosystem (pharmaverse)\nâœ… Regulatory confidence (FDA neutrality)\n\nThe momentum is undeniable. The future is open source. The time is now.\n\n\n\n\n\n\n\nTipStay Connected\n\n\n\n\nR/Pharma website: rinpharma.com\nPharmaverse: pharmaverse.org\nPosit: posit.co\nR Validation Hub: pharmar.org\n\n\n\n\nAnalysis compiled from R/Pharma 2025 Conference materials | Last updated: November 2025"
  },
  {
    "objectID": "presentations/europe-us-sessions.html",
    "href": "presentations/europe-us-sessions.html",
    "title": "Europe/US Sessions",
    "section": "",
    "text": "Simon Couch (Posit)\nFocus on unglamorous but practical AI uses in pharma: structured data extraction, tool calling, and coding assistance. Using {ellmer} for secure, on-premise deployments that protect proprietary information.\nKey Points:\n\nMost AI discourse focuses on flashy applications, but practical uses dominate daily work\nSecure deployments enable AI even with confidential data\nThree core use cases: extraction, tool calling, coding\n\nResources: github.com/simonpcouch",
    "crumbs": [
      "Presentations",
      "Europe/US Sessions"
    ]
  },
  {
    "objectID": "presentations/europe-us-sessions.html#session-1-practical-ai-industry-adoption",
    "href": "presentations/europe-us-sessions.html#session-1-practical-ai-industry-adoption",
    "title": "Europe/US Sessions",
    "section": "",
    "text": "Simon Couch (Posit)\nFocus on unglamorous but practical AI uses in pharma: structured data extraction, tool calling, and coding assistance. Using {ellmer} for secure, on-premise deployments that protect proprietary information.\nKey Points:\n\nMost AI discourse focuses on flashy applications, but practical uses dominate daily work\nSecure deployments enable AI even with confidential data\nThree core use cases: extraction, tool calling, coding\n\nResources: github.com/simonpcouch",
    "crumbs": [
      "Presentations",
      "Europe/US Sessions"
    ]
  },
  {
    "objectID": "presentations/europe-us-sessions.html#session-2-data-engineering-training",
    "href": "presentations/europe-us-sessions.html#session-2-data-engineering-training",
    "title": "Europe/US Sessions",
    "section": "Session 2: Data Engineering & Training",
    "text": "Session 2: Data Engineering & Training\n\nduckplyr: Analyze Large Data with DuckDB\nKirill Muller (Cynkra)\nStable release (v1.1.2) of {duckplyr} brings DuckDBâ€™s performance to dplyr syntax. Handle larger-than-memory data from disk or cloud with familiar tidyverse semantics.\nKey Features:\n\nSpeed up existing dplyr code\nAnalyze Parquet/CSV directly\nAccess DuckDB functionality\nMaintain R/tidyverse compatibility\n\n\n\nBeyond Training: Teaching R Adoption at GSK\nAlanah Jonas (GSK)\nGSKâ€™s evolution from basic R courses to Rburst - a comprehensive support model embedding R into daily workflows across Biostatistics.\nEvolution:\n\nTwo core courses in bookdown\nSelf-certification and Resource Hub\nAccelerateR for early adopters\nRburst for cross-functional integration\n\n\n\nValidating Shiny Apps in Regulated Environments\nPedro Silva (Jumping Rivers)\nPractical validation approaches for Shiny in clinical/healthcare settings using Litmusverse suite.\nKey Topics:\n\nTraceability and documentation\nRisk-based validation\nVersioning strategies\nCode quality assessment tools\n\n\n\nBeyond {gtsummary}: The {crane} Package\nDaniel Sjoberg, Davide Garolini (Genentech)\n{crane} extends {gtsummary} for pharmaceutical reporting with ARD-based QC and LLM summarization.\nAdvantages:\n\nInstant upgrades when {crane} is loaded\nARD-based QC is straightforward\nLLMs summarize results for medical writers\nAdapt for your needs or build your own\n\nResources: danieldsjoberg.com/RinPharma-crane-2025",
    "crumbs": [
      "Presentations",
      "Europe/US Sessions"
    ]
  },
  {
    "objectID": "presentations/europe-us-sessions.html#session-3-advanced-ai-systems",
    "href": "presentations/europe-us-sessions.html#session-3-advanced-ai-systems",
    "title": "Europe/US Sessions",
    "section": "Session 3: Advanced AI Systems",
    "text": "Session 3: Advanced AI Systems\n\n{llumen}: Agentic LLM Framework for Biomedical Documents\nSven-Eric Schelhorn (Merck KGaA)\nInternal package at Merck enabling pharmaceutical researchers to work with biomedical documents, databases, and foundation models using LLMs as orchestrators.\nCapabilities:\n\nMulti-source LLMs (Azure, OpenAI, local)\nVector DB with office document support (Word, Excel, PDF)\nRAG with PaperQA2 approach\nDatabase querying (SQL, Cypher, GraphQL)\nFoundation models (e.g., TxGemma)\nHistopathology image analysis\n\nUse Cases:\n\nExtract results from clinical trial documents\nQuery tabular databases and knowledge graphs\nBiological foundation models integration\nAutomated literature review\nHistopathology analysis\nDrug discovery support\n\nResources: Drive presentation\n\n\nBuild Model Context Protocol Servers in R\nJohn Coene (Opifex)\n{mcpr} provides R implementation of Model Context Protocol (MCP) - standardized JSON-RPC interface for AI models.\nFeatures:\n\nSchema-based tool definitions\nMulti-modal responses\nIntegration with Claude Code, Cursor, VS Code\nServer and client functionality\n\nImpact: Democratizes AI-R integration with standards-based approach.",
    "crumbs": [
      "Presentations",
      "Europe/US Sessions"
    ]
  },
  {
    "objectID": "presentations/europe-us-sessions.html#session-4-automation-innovation",
    "href": "presentations/europe-us-sessions.html#session-4-automation-innovation",
    "title": "Europe/US Sessions",
    "section": "Session 4: Automation & Innovation",
    "text": "Session 4: Automation & Innovation\n\nMosaic: ARS-Driven Automation of Standard TFLs\nConor Moloney (Novartis)\nCDISC Analysis Results Standard (ARS) coupled with open-source stack for automated TFL generation.\nArchitecture:\n\nYAML captures ARD requirements (ARS-aligned)\nLinkML validation\nPython storage via SQL Alchemy\nR derives ARD (language-agnostic rules)\nReact UI for customization and export\n\nBenefits:\n\nReplaces ad-hoc programming\nStandards-based pipeline\nAccelerates delivery\nSafeguards traceability\n\n\n\nIntegrating Collaborative Programming with Traceability\nJennifer Dusendang, Sundeep Bath (Graticule Inc)\nAdapting DevOps best practices for epidemiological studies and RWD projects.\nImplementation:\n\nParameterized pipelines in Docker containers\nCI/CD for automatic reanalysis\nCode review with validated outputs\nAutomated traceability and audit trails\n\nTools: Git, GitHub Actions, SQL, Python, R, Docker, AWS S3\nResources: PharmaSUG 2025 Paper",
    "crumbs": [
      "Presentations",
      "Europe/US Sessions"
    ]
  },
  {
    "objectID": "presentations/europe-us-sessions.html#session-5-machine-learning-ai-tools",
    "href": "presentations/europe-us-sessions.html#session-5-machine-learning-ai-tools",
    "title": "Europe/US Sessions",
    "section": "Session 5: Machine Learning & AI Tools",
    "text": "Session 5: Machine Learning & AI Tools\n\nTabPFN: Deep Learning for Tabular Data\nMax Kuhn (Posit)\nVersion 2 of TabPFN offers Bayesian-like approach for tabular data with significant advantages.\nKey Points:\n\nTrained on simulated tabular datasets\nFast inference (no training needed)\nEmulates Bayesian posterior\nNotable trade-offs to consider\n\nResources: topepo.github.io/2025-r-pharma\n\n\nThe LLM Lounge: Live Coding with Databot\nJoe Cheng, Eric Nantz\nInteractive demonstration of Databot for exploratory data analysis.\nTopics:\n\nDatabot origin story\nAI trends in life sciences\nGuiding principles for AI tools\nAudience Q&A format\n\nResources: github.com/posit-dev/querychat\n\n\nLLM-Powered {gtsummary}: QC-Ready Tables\nDavide Garolini (Roche/NEST)\nWorkflow fusing {gtsummary}, {cards} validation, and offline LLM helper.\nProcess:\n\nCreate table with {gtsummary}\nValidate with {cards}\nLLM explains steps and results\nGenerate descriptive log\nRerun with real data unchanged\n\nBenefits: Submission-ready tables in minutes with enhanced clarity.\n\n\nPost-Approval Drug Exposure Estimation\nFeifei Yang, Yu Zhang (AstraZeneca)\nR Shiny app automating post-marketing exposure calculations.\nResults:\n\nReport generation: 1 week â†’ 1 day\nBuilt-in QC functions\nTrend visualization\nTeam accessibility without training\n\n\n\nImplementing End-to-End NCA Software\nGerardo Rodriguez, Jana Spinner (Lucid Analytics)\naNCA - open-source Non-Compartmental Analysis app within Pharmaverse.\nFeatures:\n\nInteractive plots for exploration\nHalf-life customization\nTLGs and report generation\n100% testing coverage\nUses PKNCA (200+ PK parameters)\nIndustry-standard validation (Â±0.1%)\n\nResources: github.com/pharmaverse/aNCA\n\n\nIntegrating LLM for Clinical Data Review\nZhen Wu, Peng Zhang (CIMS Global)\n{DataChat} - conversational interface for clinical data with privacy and validity emphasis.\nTechnologies: {ellmer}, {shinychat}, {ragnar}, RAG capabilities\n\n\nR We There Yet? {admiral}â€™s Journey to Stability\nEdoardo Mancini (Roche)\nDiscussion on transitioning mature packages from active development to maintenance.\nQuestions Addressed:\n\nSustaining team momentum at stability\nNew priorities post-feature completeness\nLessons from {admiral} evolution",
    "crumbs": [
      "Presentations",
      "Europe/US Sessions"
    ]
  },
  {
    "objectID": "presentations/europe-us-sessions.html#session-6-enterprise-transformation",
    "href": "presentations/europe-us-sessions.html#session-6-enterprise-transformation",
    "title": "Europe/US Sessions",
    "section": "Session 6: Enterprise Transformation",
    "text": "Session 6: Enterprise Transformation\n\nGSKâ€™s Journey to Clinical Study Reporting Using Open Source\nSam Warden, Tim Colman (GSK)\nChronicles GSKâ€™s transformation from SAS-dominated era to open-source innovation.\nKey Milestones:\n\nFDA clarification on software neutrality\nRise of R and open-source platforms\nGSKâ€™s 50%+ open-source code commitment\nCOVID-19 acceleration\n\nChallenges:\n\nTechnical validation\nRegulatory uncertainty\nCultural resistance\nGovernance and training needs\n\nSuccess Factors: Growth mindset, adaptability, collaborative vision",
    "crumbs": [
      "Presentations",
      "Europe/US Sessions"
    ]
  },
  {
    "objectID": "presentations/europe-us-sessions.html#session-7-ai-integration-case-studies",
    "href": "presentations/europe-us-sessions.html#session-7-ai-integration-case-studies",
    "title": "Europe/US Sessions",
    "section": "Session 7: AI Integration Case Studies",
    "text": "Session 7: AI Integration Case Studies\n\nLeveraging ellmer and GPT in Shiny for Trials\nXing Chen, Xiaolin Chang (Moderna)\nAI-enhanced Shiny app for CMI data across mRNA infectious disease programs.\nFeatures:\n\nNatural language queries â†’ R operations\nInteractive data exploration\nCustomizable visualizations\nNo manual coding required\n\nResults:\n\nFaster insight extraction\nReduced ad-hoc programming\nEnhanced cross-functional collaboration\n\n\n\nautoslideR: Streamlining Slide Deck Generation\nYolanda Zhou, Joe Zhu (Roche)\nR package automating slide decks for clinical reporting events.\nBenefits:\n\n0.5-4 days saved per deck\nCustomized layout from templates\nPlaceholder slides for rapid prep\nMultiple event types supported\n\nResources: pharmaverse.github.io/examples/digit_files/autoslider.html\n\n\nPutting the â€˜Râ€™ in RWD\nSachin Heerah, Darren Jeng (Pfizer)\nPfizer RWD teamâ€™s R package and tools for programmers with varying backgrounds.\nTools:\n\nCustom R package for database queries\nDual R/SAS syntax support\nShiny apps for workflow support\nCode snippets in RStudio\nQuarto website for documentation\n\n\n\nGenerating Synthetic Data with synthpop\nSophie Furlow (Abbott Diagnostics)\nIntroduction to synthetic data generation for pharma and diagnostics.\nTopics:\n\nSynthetic vs simulated vs resampled data\nHealthcare applications - {synthpop} machine learning algorithms\nQuality evaluation features\nGeneration caveats",
    "crumbs": [
      "Presentations",
      "Europe/US Sessions"
    ]
  },
  {
    "objectID": "presentations/europe-us-sessions.html#session-8-innovation-advanced-methods",
    "href": "presentations/europe-us-sessions.html#session-8-innovation-advanced-methods",
    "title": "Europe/US Sessions",
    "section": "Session 8: Innovation & Advanced Methods",
    "text": "Session 8: Innovation & Advanced Methods\n\nGenAI in Production\nDevin Pastoor (A2-AI)\nMoving beyond prototypes to production AI applications in GxP contexts.\nTopics:\n\nTesting approaches for GenAI\nValidation strategies\nHandling user interaction flexibility\nRelease and maintenance\n\n\n\nBuilding the Ultimate R AI Assistant\nPawel Rucki (Roche)\nMulti-agent R co-pilot with LangGraph framework.\nArchitecture:\n\nEach R package gets its own AI agent\nNetwork of specialized agents\nContext-aware code generation\nIntegration with cursor via MCP\n\nFeatures: Write, debug, and explain complex R code for clinical trials\n\n\nThe Dependency Whisperer: AI for Impact Analysis\nMing Yan, Vina Ro (Eli Lilly)\nAI-powered tool for identifying dependencies in clinical programming.\nProblem: Traditional tools canâ€™t distinguish active code from comments or identify indirect dependencies.\nSolution:\n\nParses SDTM, ADaM, TFL specs/programs\nLearns dependency structure\nGenerates graphical impact reports\nEnsures accurate refreshes\n\n\n\nBayesERtools: Bayesian Exposure-Response Analysis\nKenta Yoshida (Genentech)\nNew R package for Bayesian ER analysis with user-friendly interface.\nFeatures:\n\nLinear and Emax models\nContinuous and binary endpoints\nBased on Stan ecosystem\nComprehensive online book (BayesERbook)\n\nResources: genentech.github.io/BayesERtools\n\n\nAdapting to Regulatory Guidance\nAlex Przybylski (Novartis)\nFDA 2023 guidance on covariate adjustment and R-enabled submissions.\nCase Study:\n\nStrategic response to FDA feedback\nLightweight R solutions ({beeca})\nASA-BIOP collaboration ({RobinCar2})\nBenefits of open-source community\n\n\n\nR Library Validation using ATDD\nBrian Repko (ex-Novartis)\nAcceptance-Test Driven Development for R package libraries.\nApproach:\n\nTests written in Quarto markdown\nâ€œGiven-when-thenâ€ format\nRegex-matched against annotated functions\nCan drive Shiny apps with {chromote}\n\nBenefits: Plain-language tests shareable with regulators",
    "crumbs": [
      "Presentations",
      "Europe/US Sessions"
    ]
  },
  {
    "objectID": "presentations/europe-us-sessions.html#key-themes-across-sessions",
    "href": "presentations/europe-us-sessions.html#key-themes-across-sessions",
    "title": "Europe/US Sessions",
    "section": "ğŸ”‘ Key Themes Across Sessions",
    "text": "ğŸ”‘ Key Themes Across Sessions\n\n1. AI Integration\n\nPractical, secure implementations\nAgentic systems and MCP\nPrivacy-preserving approaches\nProduction-ready validation\n\n\n\n2. Open Source Adoption\n\nGSK leading with 50%+ R code\nIndustry-wide shift from SAS\nCollaborative development (pharmaverse)\nFDA acceptance growing\n\n\n\n3. Automation & Efficiency\n\nARS-driven TFL generation\nAutomated slide decks\nOne-day turnaround for reports\nCI/CD for clinical workflows\n\n\n\n4. Validation & Compliance\n\nGxP-ready AI applications\nRisk-based validation approaches\nShared validation repositories\nATDD for package libraries\n\n\n\n5. Advanced Analytics\n\nBayesian methods with Stan\nMachine learning (TabPFN)\nSynthetic data generation\nHigh-performance computing\n\n\nEurope/US Sessions from R/Pharma 2025 Conference | Last updated: November 2025",
    "crumbs": [
      "Presentations",
      "Europe/US Sessions"
    ]
  },
  {
    "objectID": "presentations/index.html",
    "href": "presentations/index.html",
    "title": "Presentations",
    "section": "",
    "text": "The R/Pharma 2025 conference featured over 30 presentations organized into 8 Europe/US thematic sessions and an Asia/Pacific session. These talks covered the latest innovations, case studies, and strategic insights in using R for pharmaceutical research and development.",
    "crumbs": [
      "Presentations"
    ]
  },
  {
    "objectID": "presentations/index.html#overview",
    "href": "presentations/index.html#overview",
    "title": "Presentations",
    "section": "",
    "text": "The R/Pharma 2025 conference featured over 30 presentations organized into 8 Europe/US thematic sessions and an Asia/Pacific session. These talks covered the latest innovations, case studies, and strategic insights in using R for pharmaceutical research and development.",
    "crumbs": [
      "Presentations"
    ]
  },
  {
    "objectID": "presentations/index.html#browse-by-region",
    "href": "presentations/index.html#browse-by-region",
    "title": "Presentations",
    "section": "ğŸ“ Browse by Region",
    "text": "ğŸ“ Browse by Region\n\n\n\nğŸŒ Europe/US Sessions\n8 thematic sessions covering:\n\nğŸ¤– Practical AI for data science\nğŸ“Š Advanced analytics and tools\nğŸ¢ Enterprise transformations\nâœ… Validation approaches\nğŸš€ Innovation case studies\n\nFeatured presentations:\n\nGSKâ€™s Journey to Open Source\nMosaic: ARS-Driven TFL Automation\n{llumen}: Agentic LLM Framework\nGenAI in Production\n\nExplore Europe/US sessions â†’\n\n\n\nğŸŒ Asia/Pacific Presentations\n13 presentations featuring:\n\nğŸ›ï¸ Open-source culture and strategy\nâš™ï¸ Automation and efficiency tools\nğŸ¤– AI and LLM implementations\nğŸ“‹ Regulatory compliance (PMDA)\nğŸ”¬ Specialized applications\n\nHighlights:\n\nSHIONOGIâ€™s Open-Source Strategy\n{meRlin}: AI Assistant for Clinical Programming\nDynamic K-M Plots with Shiny\nPMDA e-CRT Creation\n\nExplore Asia/Pacific presentations â†’",
    "crumbs": [
      "Presentations"
    ]
  },
  {
    "objectID": "presentations/index.html#top-themes",
    "href": "presentations/index.html#top-themes",
    "title": "Presentations",
    "section": "ğŸ”¥ Top Themes",
    "text": "ğŸ”¥ Top Themes\n\nğŸ¤– AI & LLM Integration\n8+ presentations on artificial intelligence:\n\nPractical AI for secure deployments\nAgentic frameworks ({llumen}, {mcpr})\nLLM-powered clinical data review\nMulti-agent R copilots\nAI assistants for programming\n\n\n\nğŸ“Š Clinical Reporting Automation\n10+ presentations on streamlining TFLs and reports:\n\nARS-driven automation (Mosaic)\n{crane} extending {gtsummary}\nautoslideR for slide generation\nTemplate-based DMC materials\nInteractive Shiny applications\n\n\n\nâœ… Validation & Compliance\n6+ presentations on GxP and regulatory topics:\n\nShiny apps in regulated environments\nR library validation with ATDD\nRisk assessment frameworks\nCode review best practices\nMetric repositories for quality assessment\n\n\n\nğŸš€ Enterprise Adoption\n5+ presentations on organizational transformation:\n\nGSKâ€™s 50%+ open-source journey\nBeyond training: R support models\nSAS to R migration strategies\nPfizerâ€™s RWD programming infrastructure\nCollaborative programming with traceability\n\n\n\nğŸ“ˆ Advanced Methods\n8+ presentations on statistical and ML approaches:\n\nBayesian exposure-response analysis\nTabPFN for tabular data\nSynthetic data generation\nNCA software implementation\nSurvival analysis automation",
    "crumbs": [
      "Presentations"
    ]
  },
  {
    "objectID": "presentations/index.html#session-breakdown",
    "href": "presentations/index.html#session-breakdown",
    "title": "Presentations",
    "section": "ğŸ“Š Session Breakdown",
    "text": "ğŸ“Š Session Breakdown\n\nEurope/US Sessions\n\nSession 1: Practical AI & Industry Adoption\n\nPractical AI for Data Science (Posit)\nduckplyr: DuckDB with dplyr (Cynkra)\nBeyond Training: R Adoption (GSK)\nValidating Shiny Apps (Jumping Rivers)\nBeyond {gtsummary}: {crane} (Genentech)\n\n\n\nSession 2: Advanced AI Systems\n\n{llumen}: Agentic Framework (Merck KGaA)\nMCP Servers in R (Opifex)\n\n\n\nSession 3: Automation & Innovation\n\nMosaic: ARS-Driven TFLs (Novartis)\nCollaborative Programming (Graticule)\n\n\n\nSession 4: Machine Learning & Tools\n\nTabPFN for Tabular Data (Posit)\nLLM Lounge with Databot (Posit)\nLLM-Powered {gtsummary} (Roche)\nPost-Approval Exposure (AstraZeneca)\nEnd-to-End NCA (Lucid Analytics)\n{admiral}â€™s Journey (Roche)\n\n\n\nSession 5: Enterprise Transformation\n\nGSKâ€™s Open Source Journey (GSK)\n\n\n\nSession 6: AI Integration Case Studies\n\nellmer + GPT in Shiny (Moderna)\nautoslideR (Roche)\nR in RWD (Pfizer)\nSynthetic Data (Abbott)\n\n\n\nSession 7: Advanced Innovation\n\nGenAI in Production (A2-AI)\nUltimate R AI Assistant (Roche)\nDependency Whisperer (Eli Lilly)\nBayesERtools (Genentech)\n\n\n\nSession 8: Regulatory & Methods\n\nCovariate Adjustment (Novartis)\nR Library Validation (ex-Novartis)\n\n\n\n\nAsia/Pacific Session\n13 presentations covering strategic initiatives, automation tools, AI implementations, and regional regulatory requirements (especially PMDA).",
    "crumbs": [
      "Presentations"
    ]
  },
  {
    "objectID": "presentations/index.html#organizations-represented",
    "href": "presentations/index.html#organizations-represented",
    "title": "Presentations",
    "section": "ğŸ¢ Organizations Represented",
    "text": "ğŸ¢ Organizations Represented\nMajor pharmaceutical companies and organizations presenting:\n\n\n\n\nPosit (6+ presentations)\nRoche/Genentech (5+ presentations)\nGSK (2 presentations)\nNovartis (2 presentations)\nEli Lilly (2 presentations)\nPfizer (1 presentation)\nModerna (1 presentation)\nAstraZeneca (1 presentation)\nMerck KGaA (1 presentation)\nAbbott (1 presentation)\nSHIONOGI (1 presentation)\nA2-AI (1 presentation)\nJumping Rivers (1 presentation)\nCynkra (1 presentation)\nOpifex (1 presentation)\nLucid Analytics (1 presentation)\nGraticule (1 presentation)\nCIMS Global (2 presentations)\nAmaris Consulting (1 presentation)",
    "crumbs": [
      "Presentations"
    ]
  },
  {
    "objectID": "presentations/index.html#quick-links-by-topic",
    "href": "presentations/index.html#quick-links-by-topic",
    "title": "Presentations",
    "section": "ğŸ¯ Quick Links by Topic",
    "text": "ğŸ¯ Quick Links by Topic\nLooking for specific topics?\n\nAI & LLM\n\nPractical AI for Data Science\n{llumen}: Agentic Framework\nLLM-Powered {gtsummary}\nGenAI in Production\nUltimate R AI Assistant\n\n\n\nClinical Reporting\n\nMosaic: ARS-Driven Automation\nBeyond {gtsummary}: {crane}\nautoslideR\nofficer/flextable workshop details\n\n\n\nValidation\n\nValidating Shiny Apps\nR Library Validation ATDD\nRisk Assessment Deep Dive\n\n\n\nEnterprise Adoption\n\nGSKâ€™s Open Source Journey\nBeyond Training at GSK\nPfizer R in RWD",
    "crumbs": [
      "Presentations"
    ]
  },
  {
    "objectID": "presentations/index.html#presentation-statistics",
    "href": "presentations/index.html#presentation-statistics",
    "title": "Presentations",
    "section": "ğŸ“ˆ Presentation Statistics",
    "text": "ğŸ“ˆ Presentation Statistics\n\n\n\n30+\nTotal Presentations\n\n\n8\nEU/US Sessions\n\n\n13\nAsia/Pacific\n\n\n20+\nOrganizations",
    "crumbs": [
      "Presentations"
    ]
  },
  {
    "objectID": "presentations/index.html#key-takeaways",
    "href": "presentations/index.html#key-takeaways",
    "title": "Presentations",
    "section": "ğŸ’¡ Key Takeaways",
    "text": "ğŸ’¡ Key Takeaways\n\nIndustry Transformation\n\nGSK leading with 50%+ R code target achieved\nMajor pharma embracing open-source workflows\nSAS to R migration gaining momentum\n\n\n\nAI Revolution\n\nPractical, production-ready implementations\nPrivacy-preserving approaches for clinical data\nAgentic systems and tool orchestration\n\n\n\nStandards Adoption\n\nCDISC ARS/ARM driving automation\nHarmonized outputs across organizations\nMachine-readable analysis results\n\n\n\nValidation Maturity\n\nRisk-based approaches becoming standard\nShared metric repositories emerging\nGxP-ready Shiny applications feasible\n\n\n\n\n\n\n\n\nTipWant More Details?\n\n\n\nMost presentations include links to GitHub repositories, slides, or additional resources. Check the individual session pages for:\n\nğŸ“ Links to materials\nğŸ”— GitHub repositories\nğŸ“Š Presentation slides\nğŸ“§ Speaker contact information\n\n\n\n\nAll presentations from R/Pharma 2025 Conference | Last updated: November 2025",
    "crumbs": [
      "Presentations"
    ]
  },
  {
    "objectID": "presentations/asia-pacific-sessions.html",
    "href": "presentations/asia-pacific-sessions.html",
    "title": "Asia/Pacific Presentations",
    "section": "",
    "text": "The Asia/Pacific session featured 13 presentations covering a wide range of topics from open-source culture and data strategy to AI implementation and clinical reporting automation. These presentations showcase the growing adoption of R and innovative applications across pharmaceutical companies in the region.",
    "crumbs": [
      "Presentations",
      "Asia/Pacific Presentations"
    ]
  },
  {
    "objectID": "presentations/asia-pacific-sessions.html#overview",
    "href": "presentations/asia-pacific-sessions.html#overview",
    "title": "Asia/Pacific Presentations",
    "section": "",
    "text": "The Asia/Pacific session featured 13 presentations covering a wide range of topics from open-source culture and data strategy to AI implementation and clinical reporting automation. These presentations showcase the growing adoption of R and innovative applications across pharmaceutical companies in the region.",
    "crumbs": [
      "Presentations",
      "Asia/Pacific Presentations"
    ]
  },
  {
    "objectID": "presentations/asia-pacific-sessions.html#strategic-cultural-topics",
    "href": "presentations/asia-pacific-sessions.html#strategic-cultural-topics",
    "title": "Asia/Pacific Presentations",
    "section": "Strategic & Cultural Topics",
    "text": "Strategic & Cultural Topics\n\nOpen-Source Culture and Data Strategy in SHIONOGI\nYoshitake Kitanishi (SHIONOGI)\nPresentation on creating value through open-source adoption and data strategy implementation at SHIONOGI.\nTopics likely covered:\n\nOpen-source adoption strategy\nData-driven decision making\nCreating organizational value\nCultural transformation\n\n\n\n\nStrategically Assisting Statistical Programmers (SAS2R)\nKuenHung Lin\nFramework for supporting statistical programmers transitioning from SAS to R.\nFocus areas:\n\nMigration strategies\nTraining approaches\nSupport systems\nSuccess metrics",
    "crumbs": [
      "Presentations",
      "Asia/Pacific Presentations"
    ]
  },
  {
    "objectID": "presentations/asia-pacific-sessions.html#technical-implementations",
    "href": "presentations/asia-pacific-sessions.html#technical-implementations",
    "title": "Asia/Pacific Presentations",
    "section": "Technical Implementations",
    "text": "Technical Implementations\n\nSide-by-Side by Design: Pharma Data Handling\nYutaka Morioka, Yuki Nakagawa\nComprehensive comparison of data handling approaches in pharma using merge, join, match, and hash operations in R.\nKey concepts:\n\nData merging strategies\nJoin operations optimization\nHash table implementations\nPerformance comparisons\n\n\n\n\nRisk Assessment Deep Dive\nRyo Nakaya\nIn-depth exploration of risk assessment methodologies and tools for pharmaceutical R packages.\nTopics:\n\nPackage risk evaluation\nValidation frameworks\nQuality metrics\nDecision criteria",
    "crumbs": [
      "Presentations",
      "Asia/Pacific Presentations"
    ]
  },
  {
    "objectID": "presentations/asia-pacific-sessions.html#automation-efficiency",
    "href": "presentations/asia-pacific-sessions.html#automation-efficiency",
    "title": "Asia/Pacific Presentations",
    "section": "Automation & Efficiency",
    "text": "Automation & Efficiency\n\nLeverage Template-Based Automated Reporting on DMC Materials\nNina Han, Peng Zhang\nAutomation of Data Monitoring Committee (DMC) materials preparation using template-based approaches.\nBenefits:\n\nReduced preparation time\nConsistency across reports\nError reduction\nReproducibility\n\n\n\n\nDynamic Creation of Kaplan-Meier Plots and Summary Tables\nTakumi Imamura, Takahiro Hasegawa\nR Shiny application for interactive survival data visualization and table generation.\nFeatures:\n\nInteractive K-M plots\nDynamic summary measures\nUser-friendly interface\nExport capabilities\n\n\n\n\nautoslideR: Streamlining Slide Deck Generation\nYolanda Zhou, Joe Zhu (Roche)\nR package for automating clinical reporting slide decks (also presented in EU/US sessions).\nKey advantages:\n\n0.5-4 days time savings\nMultiple event types supported\nCustomizable layouts\nPlaceholder functionality\n\nResources: pharmaverse.github.io/examples/digit_files/autoslider.html",
    "crumbs": [
      "Presentations",
      "Asia/Pacific Presentations"
    ]
  },
  {
    "objectID": "presentations/asia-pacific-sessions.html#r-adoption-strategy",
    "href": "presentations/asia-pacific-sessions.html#r-adoption-strategy",
    "title": "Asia/Pacific Presentations",
    "section": "R Adoption & Strategy",
    "text": "R Adoption & Strategy\n\nFrom Harmony to Hybrid: Charting a Course for R Adoption\nJoe Zhu\nPractical strategies for R adoption in pharmaceutical organizations.\nTopics:\n\nAdoption frameworks\nHybrid approaches (SAS + R)\nChange management\nSuccess factors\n\n\n\n\nEmerging Trend of LLM Development in R\nMia Chen, Peng Zhang\nOverview of Large Language Model implementation trends in R ecosystem.\nCoverage:\n\nCurrent LLM packages in R\nIntegration patterns\nUse cases in pharma\nFuture directions",
    "crumbs": [
      "Presentations",
      "Asia/Pacific Presentations"
    ]
  },
  {
    "objectID": "presentations/asia-pacific-sessions.html#specialized-applications",
    "href": "presentations/asia-pacific-sessions.html#specialized-applications",
    "title": "Asia/Pacific Presentations",
    "section": "Specialized Applications",
    "text": "Specialized Applications\n\nAn R Package for Pretest Probability Models for CAD\nJeremy Selva\nPackage consolidating pretest probability models and guidelines for Coronary Artery Disease.\nFeatures:\n\nMultiple model implementations\nGuideline integration\nClinical utility\nValidation support\n\n\n\n\nEnhancing e-CRT Creation for PMDA with Vibe Coding\nNaoki Yoshida\nR Shiny app development for PMDA (Japanâ€™s regulatory agency) electronic Case Report Tabulation using innovative â€œVibe Codingâ€ approach.\nInnovation:\n\nRapid development methodology\nRegulatory-ready outputs\nUser-friendly interface\nEfficiency gains",
    "crumbs": [
      "Presentations",
      "Asia/Pacific Presentations"
    ]
  },
  {
    "objectID": "presentations/asia-pacific-sessions.html#ai-advanced-methods",
    "href": "presentations/asia-pacific-sessions.html#ai-advanced-methods",
    "title": "Asia/Pacific Presentations",
    "section": "AI & Advanced Methods",
    "text": "AI & Advanced Methods\n\n{meRlin}: Context-Aware AI Assistant for Clinical Programming\nSteven Brooks, Pietro Mascheroni, Xiecheng Gu\nAI assistant specifically designed for clinical programming workflows.\nCapabilities:\n\nContext-aware code suggestions\nClinical programming patterns\nIntegration with development environment\nDomain-specific knowledge\n\n\n\n\nImproving Precision Healthcare for Under-Represented Populations\nJimmy Breen\nApplication of R in advancing precision healthcare for genetically diverse and under-represented global populations.\nFocus:\n\nGenetic diversity considerations\nPopulation-specific analyses\nHealthcare equity\nPrecision medicine approaches\n\n\n\n\nExploring AI Tools in Clinical Trial Data Analysis\nTerry Zhang\nSurvey and exploration of AI tools applicable to clinical trial data analysis.\nCoverage:\n\nAvailable AI tools\nUse case demonstrations\nIntegration strategies\nPractical applications\n\n\n\n\ncgmguru: Automated Glycemic Event Detection\nSang Ho Park\nR package for automated detection of glycemic events from continuous glucose monitoring data.\nFeatures:\n\nEvent detection algorithms\nAutomated analysis\nClinical relevance\nValidation metrics",
    "crumbs": [
      "Presentations",
      "Asia/Pacific Presentations"
    ]
  },
  {
    "objectID": "presentations/asia-pacific-sessions.html#reporting-documentation",
    "href": "presentations/asia-pacific-sessions.html#reporting-documentation",
    "title": "Asia/Pacific Presentations",
    "section": "Reporting & Documentation",
    "text": "Reporting & Documentation\n\nInteractive and Reproducible Reports with Quarto\nJaspreet Pabla\nCreating interactive, reproducible clinical reports using Quarto.\nAdvantages:\n\nMulti-format output\nInteractivity options\nReproducibility\nModern publishing\n\n\n\n\nUsing ARD with {cards} for PMDA Oncology Inquiries\nShunsuke Goto\nApplication of Analysis Results Data using {cards} package for responding to PMDA oncology regulatory inquiries.\nBenefits:\n\nStructured data approach\nRapid response capability\nConsistency\nTraceability",
    "crumbs": [
      "Presentations",
      "Asia/Pacific Presentations"
    ]
  },
  {
    "objectID": "presentations/asia-pacific-sessions.html#practical-tips-tools",
    "href": "presentations/asia-pacific-sessions.html#practical-tips-tools",
    "title": "Asia/Pacific Presentations",
    "section": "Practical Tips & Tools",
    "text": "Practical Tips & Tools\n\nReach for R Low Hanging Fruit for Faster Results\nSunil Gupta\nIdentifying and implementing quick wins with R for immediate productivity gains.\nTopics:\n\nQuick wins identification\nPractical implementations\nEfficiency improvements\nGetting started tips\n\n\n\n\nMedxR Package: Bridging FDA and Health Canada Drug Data\nRenzo CÃ¡ceres Rossi\nR package providing access to regulatory drug data from FDA and Health Canada.\nFeatures:\n\nUnified data access\nAPI integration\nData standardization\nResearch facilitation\n\n\n\n\nrisk.assessr: Extending Package Validation\nHugo Bottois\nExtension of risk assessment tools for package validation processes.\nEnhancements:\n\nValidation workflow support\nExtended metrics\nIntegration capabilities\nReporting features\n\n\n\n\nCode Review for Compliance: Validated R Workflows\nAlexandros Kouretsis\nBest practices for code review in validated R workflows for pharmaceutical compliance.\nTopics:\n\nReview strategies\nCompliance requirements\nQuality assurance\nDocumentation\n\n\n\n\nWeb-Based R Application for Patient Enrollment Forecasting\nAkifumi Okayama, Motoki Oe, Nobushige Matsuoka\nShiny application for forecasting patient enrollment in clinical trials.\nFeatures:\n\nForecasting models\nInteractive interface\nScenario planning\nDecision support",
    "crumbs": [
      "Presentations",
      "Asia/Pacific Presentations"
    ]
  },
  {
    "objectID": "presentations/asia-pacific-sessions.html#regional-themes",
    "href": "presentations/asia-pacific-sessions.html#regional-themes",
    "title": "Asia/Pacific Presentations",
    "section": "ğŸ”‘ Regional Themes",
    "text": "ğŸ”‘ Regional Themes\n\n1. Regulatory Focus\nStrong emphasis on PMDA (Japan) regulatory requirements with specialized tools and approaches.\n\n\n2. Practical Implementation\nFocus on real-world applications and efficiency improvements in daily workflows.\n\n\n3. AI Adoption\nGrowing interest in AI/LLM tools tailored for clinical programming and analysis.\n\n\n4. Open Source Movement\nIncreasing adoption of open-source culture and collaborative development.\n\n\n5. Cultural Considerations\nPresentations addressing unique challenges and opportunities in Asia/Pacific markets including:\n\nUnder-represented populations\nRegional regulatory requirements\nLocal healthcare systems\nGenetic diversity",
    "crumbs": [
      "Presentations",
      "Asia/Pacific Presentations"
    ]
  },
  {
    "objectID": "presentations/asia-pacific-sessions.html#topic-distribution",
    "href": "presentations/asia-pacific-sessions.html#topic-distribution",
    "title": "Asia/Pacific Presentations",
    "section": "ğŸ“Š Topic Distribution",
    "text": "ğŸ“Š Topic Distribution\n\n\n\n5\nAutomation & Tools\n\n\n4\nAI & Advanced Methods\n\n\n4\nStrategy & Culture",
    "crumbs": [
      "Presentations",
      "Asia/Pacific Presentations"
    ]
  },
  {
    "objectID": "presentations/asia-pacific-sessions.html#geographic-insights",
    "href": "presentations/asia-pacific-sessions.html#geographic-insights",
    "title": "Asia/Pacific Presentations",
    "section": "ğŸŒ Geographic Insights",
    "text": "ğŸŒ Geographic Insights\nThe Asia/Pacific presentations demonstrate:\n\nStrong R adoption across major pharma companies in the region\nInnovation in automation for regulatory compliance (especially PMDA)\nFocus on practical tools addressing regional-specific needs\nGrowing AI integration in clinical workflows\nEmphasis on efficiency and time-saving solutions\n\n\nAsia/Pacific Presentations from R/Pharma 2025 Conference | Last updated: November 2025",
    "crumbs": [
      "Presentations",
      "Asia/Pacific Presentations"
    ]
  },
  {
    "objectID": "summary/tools-catalog.html",
    "href": "summary/tools-catalog.html",
    "title": "Tools & Packages Catalog",
    "section": "",
    "text": "This catalog documents all R packages, Python libraries, and tools mentioned across the 19 workshops and 30+ presentations at R/Pharma 2025. Entries include descriptions, primary use cases, and links to where they were discussed.\n\n\n\n\n\n\nNotePopularity Indicators\n\n\n\nPackages are marked with mentions:\n\nğŸ”¥ğŸ”¥ğŸ”¥ 5+ mentions\nHot topic, widespread interest\nğŸ”¥ğŸ”¥ 3-4 mentions\nMultiple sessions\nğŸ”¥ 1-2 mentions\nEmerging or specialized"
  },
  {
    "objectID": "summary/tools-catalog.html#overview",
    "href": "summary/tools-catalog.html#overview",
    "title": "Tools & Packages Catalog",
    "section": "",
    "text": "This catalog documents all R packages, Python libraries, and tools mentioned across the 19 workshops and 30+ presentations at R/Pharma 2025. Entries include descriptions, primary use cases, and links to where they were discussed.\n\n\n\n\n\n\nNotePopularity Indicators\n\n\n\nPackages are marked with mentions:\n\nğŸ”¥ğŸ”¥ğŸ”¥ 5+ mentions\nHot topic, widespread interest\nğŸ”¥ğŸ”¥ 3-4 mentions\nMultiple sessions\nğŸ”¥ 1-2 mentions\nEmerging or specialized"
  },
  {
    "objectID": "summary/tools-catalog.html#r-packages",
    "href": "summary/tools-catalog.html#r-packages",
    "title": "Tools & Packages Catalog",
    "section": "R Packages",
    "text": "R Packages\n\nA\n\n{admiral} ğŸ”¥ğŸ”¥\nADaM in R Asset Library\nModular R package for creating ADaM datasets. Part of pharmaverse ecosystem, transitioning from active development to stable maintenance.\n\nUse Cases: ADaM dataset creation, CDISC compliance\nMentioned in: Presentations - admiralâ€™s Journey\nLinks: pharmaverse.github.io/admiral\n\n\n\n\n\nB\n\n{beeca} ğŸ”¥\nBayesian Covariate Adjustment\nLightweight R package implementing FDA 2023 covariate adjustment guidance.\n\nUse Cases: Covariate adjustment, regulatory compliance\nMentioned in: Presentations - Covariate Adjustment\nDeveloper: Novartis\n\n\n\n{bmstate} ğŸ”¥\nBayesian Multistate Models\nStan-based package for Bayesian survival and multistate models in clinical trials.\n\nUse Cases: Time-to-event analysis, competing risks\nMentioned in: Workshop - Bayesian Survival\nLinks: github.com/generable/bmstate\n\n\n\n\n\nC\n\n{cardinal} ğŸ”¥ğŸ”¥\nStandardized Clinical TLG Templates\nOpen-source collection of table, listing, and graph templates aligned with CDISC standards.\n\nUse Cases: TFLs, CSRs, meta-analysis, harmonized outputs\nMentioned in: Workshop - Cardinal\nLinks: pharmaverse.github.io/cardinal\n\n\n\n{cards} ğŸ”¥ğŸ”¥ğŸ”¥\nAnalysis Results Data Objects\nCreates language-agnostic Analysis Results Data (ARD) objects for clinical reporting.\n\nUse Cases: ARD generation, QC, multi-language results\nMentioned in: Cardinal workshop, {crane}, PMDA inquiries, LLM-powered tables\nLinks: Part of {gtsummary} ecosystem\n\n\n\n{cgmguru} ğŸ”¥\nContinuous Glucose Monitoring Analysis\nAutomated glycemic event detection from CGM data.\n\nUse Cases: Diabetes trials, CGM analysis\nMentioned in: Asia/Pacific Presentations\n\n\n\n{chromote} ğŸ”¥\nChrome Remote Interface\nHeadless Chrome control for testing Shiny apps.\n\nUse Cases: Shiny app testing, validation, ATDD\nMentioned in: Presentations - ATDD\n\n\n\n{crane} ğŸ”¥ğŸ”¥\nExtension to {gtsummary}\nPharmaceutical-specific extensions to {gtsummary} with ARD-based QC.\n\nUse Cases: Clinical tables, regulatory submissions, QC workflows\nMentioned in: Presentations - Beyond gtsummary\nDeveloper: Genentech\nLinks: danieldsjoberg.com/RinPharma-crane-2025\n\n\n\n\n\nD\n\n{datasetjson} ğŸ”¥\nCDISC Dataset-JSON Support\nRead and write CDISC Dataset-JSON format in R and Python.\n\nUse Cases: Modern data exchange, CDISC compliance\nMentioned in: Workshop - datasetjson\nLinks: atorus-research.github.io/datasetjson\n\n\n\n{devtools} ğŸ”¥ğŸ”¥\nPackage Development Tools\nEssential package for R package development.\n\nUse Cases: Package creation, testing, documentation\nMentioned in: Workshop - Building R Packages\nLinks: devtools.r-lib.org\n\n\n\n{dplyr} ğŸ”¥ğŸ”¥\nData Manipulation\nCore tidyverse package for data transformation.\n\nUse Cases: Data wrangling, pipelines\nMentioned in: Multiple workshops (implicit in many examples)\n\n\n\n{duckplyr} ğŸ”¥\nDuckDB with dplyr Syntax\ndplyr backend using DuckDB for performance.\n\nUse Cases: Large data, Parquet files, performance\nMentioned in: Presentations - duckplyr\nLinks: Stable v1.1.2 on CRAN\n\n\n\n\n\nE\n\n{ellmer} ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥\nLLM Integration for R\nUnified interface for working with Large Language Models from R.\n\nUse Cases: AI integration, chatbots, tool calling\nMentioned in: Workshop - LLM APIs, Workshop - LLM Tooling, multiple presentations\nDeveloper: Posit\nLinks: posit-dev.github.io/ellmer\n\n\n\n\n\nF\n\n{flextable} ğŸ”¥ğŸ”¥ğŸ”¥\nFlexible Tables\nCreate publication-quality tables with fine-grained control.\n\nUse Cases: Clinical reports, Word documents, pharmaceutical tables\nMentioned in: Workshop - officer/flextable, multiple presentations\nDeveloper: David Gohel (Ardata)\nLinks: ardata.fr/flextable-book\n\n\n\n\n\nG\n\n{ggplot2} ğŸ”¥ğŸ”¥\nGrammar of Graphics\nThe standard for data visualization in R.\n\nUse Cases: Publication-ready plots, clinical graphics\nMentioned in: officer/flextable workshop, multiple examples\n\n\n\n{gt} ğŸ”¥\nGrammar of Tables\nModern table creation with flexible styling.\n\nUse Cases: HTML tables, dashboards\nMentioned in: Cardinal workshop (gt backend for gtsummary)\nLinks: gt.rstudio.com\n\n\n\n{gtsummary} ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥\nSummary Tables\nCreate publication-ready summary tables with minimal code.\n\nUse Cases: Demographics, safety, efficacy tables\nMentioned in: Workshop - Cardinal, {crane}, LLM-powered tables, multiple presentations\nLinks: danieldsjoberg.com/gtsummary\n\n\n\n\n\nL\n\n{llumen} ğŸ”¥ğŸ”¥\nAgentic LLM Framework (Internal)\nMerck KGaAâ€™s internal package for biomedical documents, databases, and foundation models.\n\nUse Cases: Document extraction, database querying, RAG, histopathology\nMentioned in: Presentations - llumen\nStatus: Internal Merck package\n\n\n\n\n\nM\n\n{mcpr} ğŸ”¥ğŸ”¥\nModel Context Protocol in R\nR implementation of Model Context Protocol for AI integration.\n\nUse Cases: AI tool servers, MCP clients, standardized AI interfaces\nMentioned in: Workshop - LLM Tooling, Presentations - MCP\nLinks: Standards-based AI integration\n\n\n\n{meRlin} ğŸ”¥\nAI Assistant for Clinical Programming\nContext-aware AI assistant specifically for clinical programming.\n\nUse Cases: Code suggestions, clinical programming patterns\nMentioned in: Asia/Pacific Presentations\n\n\n\n\n\nO\n\n{officer} ğŸ”¥ğŸ”¥ğŸ”¥\nManipulate Microsoft Word Documents\nCreate and modify Word documents programmatically.\n\nUse Cases: CSRs, clinical reports, automated documentation\nMentioned in: Workshop - officer/flextable\nDeveloper: David Gohel (Ardata)\nLinks: ardata.fr/officer-book\n\n\n\n\n\nP\n\n{parsnip} ğŸ”¥\nUnified ML Interface\nPart of tidymodels, provides consistent interface for models.\n\nUse Cases: Machine learning, classification, regression\nMentioned in: Workshop - tidymodels\n\n\n\n{pkgdown} ğŸ”¥\nPackage Websites\nGenerate websites from R package documentation.\n\nUse Cases: Package documentation, user guides\nMentioned in: Workshop - Building Packages\n\n\n\n{PKNCA} ğŸ”¥\nPharmacokinetic Non-Compartmental Analysis\nCalculate 200+ PK parameters for NCA.\n\nUse Cases: PK analysis, NCA studies\nMentioned in: Presentations - aNCA\nLinks: Powers aNCA app\n\n\n\n{pointblank} ğŸ”¥ğŸ”¥\nData Validation and Documentation\nComprehensive data quality and documentation workflows.\n\nUse Cases: Data validation, quality checks, documentation, SDTM compliance\nMentioned in: Workshop - pointblank\nDeveloper: Rich Iannone (Posit)\nLinks: github.com/rich-iannone/pointblank-workshop\n\n\n\n\n\nR\n\n{ragnar} ğŸ”¥ğŸ”¥\nRetrieval-Augmented Generation\nVector database and RAG functionality for R.\n\nUse Cases: Document search, context retrieval, AI applications\nMentioned in: Workshop - LLM Clinical Data, {DataChat}\n\n\n\n{recipes} ğŸ”¥\nPreprocessing for Modeling\nPart of tidymodels, feature engineering and preprocessing.\n\nUse Cases: Data preprocessing, ML pipelines\nMentioned in: Workshop - tidymodels\n\n\n\n{riskassessment} ğŸ”¥ğŸ”¥\nPackage Risk Assessment\nShiny app for package validation and risk assessment.\n\nUse Cases: Validation workflows, package quality assessment\nMentioned in: Workshop - R Validation\nLinks: R Validation Hub\n\n\n\n{riskmetric} ğŸ”¥ğŸ”¥\nRisk Metrics for Packages\nCalculate risk metrics for R package validation.\n\nUse Cases: Package assessment, validation\nMentioned in: Workshop - R Validation\nLinks: R Validation Hub\nLinks: pharmar.github.io/riskmetric\n\n\n\n{RobinCar2} ğŸ”¥\nCovariate Adjustment Methods\nASA-BIOP working group package for covariate adjustment.\n\nUse Cases: FDA guidance compliance, covariate adjustment\nMentioned in: Presentations - Covariate Adjustment\n\n\n\n{roxygen2} ğŸ”¥ğŸ”¥\nDocumentation Generation\nGenerate R documentation from code comments.\n\nUse Cases: Package documentation, help pages\nMentioned in: Workshop - Building Packages\n\n\n\n{rpact} ğŸ”¥\nClinical Trial Design and Simulation\nComprehensive package for adaptive and group sequential designs.\n\nUse Cases: Trial design, simulation, sample size\nMentioned in: Workshop - rpact\nLinks: rpact.org\n\n\n\n{rsample} ğŸ”¥\nData Splitting and Resampling\nPart of tidymodels, train/test splitting and cross-validation.\n\nUse Cases: ML workflows, resampling\nMentioned in: Workshop - tidymodels\n\n\n\n\n\nS\n\n{sdtm.oak} ğŸ”¥\nSDTM Programming Framework\nEDC-agnostic SDTM dataset creation with reusable algorithms.\n\nUse Cases: SDTM programming, CDISC compliance\nMentioned in: Workshop - SDTM oak\nLinks: Pharmaverse\n\n\n\n{shiny} ğŸ”¥ğŸ”¥ğŸ”¥\nInteractive Web Applications\nFramework for building interactive web apps in R.\n\nUse Cases: Dashboards, clinical data exploration, interactive tools\nMentioned in: Multiple workshops (teal, HPC, DataChat, validating Shiny)\n\n\n\n{shinychat} ğŸ”¥ğŸ”¥ğŸ”¥\nChatbot Interface for Shiny\nAdd conversational interfaces to Shiny apps.\n\nUse Cases: LLM interfaces, chatbots, conversational UIs\nMentioned in: Workshop - LLM APIs, DataChat\nDeveloper: Posit\n\n\n\n{synthpop} ğŸ”¥\nSynthetic Data Generation\nGenerate synthetic datasets using machine learning.\n\nUse Cases: Privacy-preserving data, testing, sharing\nMentioned in: Presentations - Synthetic Data\n\n\n\n\n\nT\n\n{teal} ğŸ”¥ğŸ”¥\nInteractive Clinical Trial Apps\nFramework for building interactive data exploration apps.\n\nUse Cases: Clinical data exploration, interactive analysis\nMentioned in: Workshop - teal\nVersion: 1.0 released\nDeveloper: Roche/Genentech\nLinks: Part of pharmaverse\n\n\n\n{testthat} ğŸ”¥ğŸ”¥\nUnit Testing\nStandard testing framework for R packages.\n\nUse Cases: Package testing, validation, QC\nMentioned in: Workshop - Building Packages\n\n\n\n{tidymodels} ğŸ”¥ğŸ”¥\nMachine Learning Framework\nComprehensive ecosystem for modeling and ML in R.\n\nUse Cases: Classification, regression, ML workflows\nMentioned in: Workshop - tidymodels Classification\nLinks: tidymodels.org\n\n\n\n\n\nU\n\n{usethis} ğŸ”¥ğŸ”¥\nPackage Development Workflow\nAutomate package setup and development tasks.\n\nUse Cases: Package creation, git setup, GitHub integration\nMentioned in: Workshop - Building Packages\nLinks: usethis.r-lib.org\n\n\n\n\n\nV\n\n{val.criterion} ğŸ”¥\nValidation Criteria\nDefine validation criteria for package assessment.\n\nUse Cases: Validation standards, quality thresholds\nMentioned in: Workshop - R Validation\nLinks: R Validation Hub\n\n\n\n{val.meter} ğŸ”¥\nValidation Metrics\nStore and retrieve validation metrics.\n\nUse Cases: Metric repository, shared validation database\nMentioned in: Workshop - R Validation\nLinks: R Validation Hub\n\n\n\n\n\nY\n\n{yardstick} ğŸ”¥\nModel Performance Metrics\nPart of tidymodels, calculate model performance metrics.\n\nUse Cases: Model evaluation, ML metrics\nMentioned in: Workshop - tidymodels"
  },
  {
    "objectID": "summary/tools-catalog.html#python-libraries",
    "href": "summary/tools-catalog.html#python-libraries",
    "title": "Tools & Packages Catalog",
    "section": "Python Libraries",
    "text": "Python Libraries\n\npolars ğŸ”¥ğŸ”¥\nHigh-Performance DataFrames\nBlazingly fast DataFrame library with Apache Arrow backend.\n\nUse Cases: Large data processing, clinical trial data exploration\nMentioned in: Workshop - Polars Python, Workshop - Python CSR\nLinks: pola.rs\n\n\n\nplotnine ğŸ”¥\nGrammar of Graphics for Python\nggplot2 equivalent for Python.\n\nUse Cases: Data visualization, TFLs\nMentioned in: Workshop - Python CSR\n\n\n\nGreat Tables ğŸ”¥ğŸ”¥\nModern Table Creation\nPython package for creating publication-ready tables.\n\nUse Cases: TFLs, clinical reporting\nMentioned in: Workshop - Polars\nDeveloper: Posit\nLinks: Launched at conference\n\n\n\nrtflite ğŸ”¥\nRTF Generation\nPython library for RTF document generation.\n\nUse Cases: Regulatory submissions, CSRs\nMentioned in: Workshop - Python CSR\n\n\n\nLangChain ğŸ”¥\nLLM Application Framework\nPopular Python framework for building LLM applications.\n\nUse Cases: AI agents, tool orchestration\nMentioned in: Workshop - LLM Tooling\n\n\n\nLangGraph ğŸ”¥\nMulti-Agent Orchestration\nFramework for building multi-agent LLM systems.\n\nUse Cases: Complex AI workflows, agent networks\nMentioned in: Presentations - R AI Assistant\nDeveloper: LangChain team\n\n\n\nuv ğŸ”¥\nPython Package Manager\nFast Python package and project manager.\n\nUse Cases: Python environment management, reproducibility\nMentioned in: Workshop - Python CSR"
  },
  {
    "objectID": "summary/tools-catalog.html#external-tools-platforms",
    "href": "summary/tools-catalog.html#external-tools-platforms",
    "title": "Tools & Packages Catalog",
    "section": "External Tools & Platforms",
    "text": "External Tools & Platforms\n\nQuarto ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥\nPublishing System\nModern scientific publishing system supporting multiple formats.\n\nUse Cases: Reports, presentations, websites, dashboards, brand.yml\nMentioned in: Workshop - Branded Quarto, multiple examples\nLinks: quarto.org\n\n\n\nStan ğŸ”¥ğŸ”¥\nBayesian Inference\nPlatform for statistical modeling and Bayesian inference.\n\nUse Cases: Bayesian models, MCMC, complex hierarchical models\nMentioned in: Workshop - Debugging Stan, Workshop - Bayesian Survival, BayesERtools\nLinks: mc-stan.org\n\n\n\nAWS Bedrock ğŸ”¥ğŸ”¥\nManaged LLM Service\nAmazonâ€™s managed service for foundational models.\n\nUse Cases: Enterprise LLM deployment, GxP compliance\nMentioned in: Workshop - LLM Tooling\n\n\n\nDocker ğŸ”¥ğŸ”¥\nContainerization\nPlatform for deploying applications in containers.\n\nUse Cases: Reproducibility, deployment, validation\nMentioned in: Presentations - Traceability\n\n\n\nGitHub Actions ğŸ”¥ğŸ”¥\nCI/CD\nAutomation platform for building, testing, and deploying.\n\nUse Cases: Automated testing, validation, deployment\nMentioned in: Multiple workshops (package dev, validation)\n\n\n\nPosit Workbench ğŸ”¥ğŸ”¥\nDevelopment Platform\nCommercial RStudio IDE for teams (formerly RStudio Workbench).\n\nUse Cases: Team collaboration, enterprise R/Python development\nMentioned in: GSK journey, multiple enterprise examples\nLinks: posit.co\n\n\n\nPosit Connect ğŸ”¥\nPublishing Platform\nShare R/Python content (Shiny, reports, APIs).\n\nUse Cases: Shiny deployment, report distribution\nMentioned in: HPC workshop, enterprise deployments\n\n\n\nRPACT Cloud ğŸ”¥\nTrial Design Platform\nUser-friendly platform for {rpact} trial design.\n\nUse Cases: Clinical trial design, simulations\nMentioned in: Workshop - rpact\nLinks: cloud.rpact.com\n\n\n\nMCP (Model Context Protocol) ğŸ”¥ğŸ”¥\nAI Integration Standard\nStandardized protocol for AI tool integration.\n\nUse Cases: Cross-platform AI tools, standardized interfaces\nMentioned in: Multiple AI/LLM workshops and presentations\nLinks: modelcontextprotocol.io\n\n\n\nClaude Code ğŸ”¥\nAI Coding Assistant\nAnthropicâ€™s coding assistant using Claude.\n\nUse Cases: AI-assisted coding, MCP integration\nMentioned in: MCP presentations\n\n\n\nCursor ğŸ”¥\nAI Code Editor\nAI-powered code editor with MCP support.\n\nUse Cases: AI-assisted development\nMentioned in: MCP presentations, Roche AI assistant"
  },
  {
    "objectID": "summary/tools-catalog.html#specialized-applications",
    "href": "summary/tools-catalog.html#specialized-applications",
    "title": "Tools & Packages Catalog",
    "section": "Specialized Applications",
    "text": "Specialized Applications\n\naNCA ğŸ”¥\nNon-Compartmental Analysis App\nOpen-source Shiny app for PK analysis.\n\nStatus: Upcoming CRAN release\nBackend: {PKNCA}\nMentioned in: Presentations - NCA\nLinks: github.com/pharmaverse/aNCA\n\n\n\nautoslideR ğŸ”¥ğŸ”¥\nSlide Deck Automation\nR package for automating clinical reporting slide decks.\n\nDeveloper: Roche\nMentioned in: Presentations - autoslideR\nLinks: pharmaverse.github.io/examples/digit_files/autoslider.html\n\n\n\nDataChat ğŸ”¥\nConversational Clinical Data Interface\nShiny app for natural language queries on clinical data.\n\nDeveloper: CIMS Global\nMentioned in: Workshop - LLM Clinical Data\n\n\n\nDatabot ğŸ”¥\nData Analysis Assistant\nPositâ€™s LLM-powered data analysis tool.\n\nMentioned in: Presentations - LLM Lounge\nLinks: github.com/posit-dev/querychat\n\n\n\nMosaic ğŸ”¥\nARS-Driven TFL Platform\nNovartis platform for automated TFL generation.\n\nArchitecture: YAML â†’ Python â†’ R â†’ React UI\nMentioned in: Presentations - Mosaic\n\n\n\nLitmusverse ğŸ”¥\nShiny Validation Suite\nJumping Riversâ€™ suite for Shiny app validation.\n\nMentioned in: Presentations - Validating Shiny"
  },
  {
    "objectID": "summary/tools-catalog.html#standards-specifications",
    "href": "summary/tools-catalog.html#standards-specifications",
    "title": "Tools & Packages Catalog",
    "section": "Standards & Specifications",
    "text": "Standards & Specifications\n\nCDISC ARS/ARM ğŸ”¥ğŸ”¥ğŸ”¥\nAnalysis Results Standard\nEmerging standard for machine-readable analysis results.\n\nImpact: Driving automation (Mosaic, {cards}, {crane})\nMentioned in: Multiple workshops and presentations\n\n\n\nCDISC SDTM ğŸ”¥ğŸ”¥\nStudy Data Tabulation Model\nStandard for clinical trial data submission.\n\nMentioned in: Multiple workshops ({sdtm.oak}, validation, data handling)\n\n\n\nCDISC ADaM ğŸ”¥ğŸ”¥\nAnalysis Data Model\nStandard for analysis datasets.\n\nMentioned in: {admiral}, multiple analysis workshops"
  },
  {
    "objectID": "summary/tools-catalog.html#most-mentioned-tools",
    "href": "summary/tools-catalog.html#most-mentioned-tools",
    "title": "Tools & Packages Catalog",
    "section": "ğŸ“Š Most Mentioned Tools",
    "text": "ğŸ“Š Most Mentioned Tools\n\nTop 10 by Mentions\n\n{ellmer} - 5+ mentions (AI/LLM integration)\nQuarto - 4+ mentions (Publishing)\n{gtsummary} - 4+ mentions (Clinical tables)\n{cards} - 3+ mentions (ARD)\n{officer}/{flextable} - 3+ mentions (Word reports)\n{shinychat} - 3+ mentions (Chatbots)\n{teal} - 2+ mentions (Interactive apps)\nStan - 2+ mentions (Bayesian)\n{tidymodels} - 2+ mentions (ML)\n{pointblank} - 2+ mentions (Validation)"
  },
  {
    "objectID": "summary/tools-catalog.html#by-use-case",
    "href": "summary/tools-catalog.html#by-use-case",
    "title": "Tools & Packages Catalog",
    "section": "ğŸ¯ By Use Case",
    "text": "ğŸ¯ By Use Case\n\nAI/LLM Integration\n{ellmer}, {mcpr}, {shinychat}, {ragnar}, {llumen}, LangChain, LangGraph, AWS Bedrock, MCP\n\n\nClinical Reporting\n{gtsummary}, {crane}, {cardinal}, {cards}, {officer}, {flextable}, Quarto, Great Tables, autoslideR\n\n\nData Validation\n{pointblank}, {riskmetric}, {riskassessment}, Litmusverse\n\n\nPackage Development\n{devtools}, {usethis}, {testthat}, {roxygen2}, {pkgdown}\n\n\nStatistical Analysis\nStan, {rpact}, {tidymodels}, {bmstate}, BayesERtools\n\n\nCDISC Compliance\n{admiral}, {sdtm.oak}, {datasetjson}, ARS/ARM\n\n\nHigh Performance\npolars, {duckplyr}, Apache Arrow, HPC integration\n\n\n\n\n\n\n\nTipFinding the Right Tool\n\n\n\nUse this catalog to:\n\nDiscover tools for your specific use case\nCompare alternatives (e.g., {officer} vs Quarto for reports)\nIdentify dependencies (e.g., {gtsummary} uses {cards})\nPlan your learning path (prerequisites and related tools)\n\n\n\n\nCompiled from R/Pharma 2025 Conference materials | Last updated: November 2025"
  },
  {
    "objectID": "summary/career-insights.html",
    "href": "summary/career-insights.html",
    "title": "Career Insights",
    "section": "",
    "text": "Based on the workshops, presentations, and industry trends from R/Pharma 2025, this guide provides actionable career advice for R professionals working in (or aspiring to work in) the pharmaceutical industry."
  },
  {
    "objectID": "summary/career-insights.html#overview",
    "href": "summary/career-insights.html#overview",
    "title": "Career Insights",
    "section": "",
    "text": "Based on the workshops, presentations, and industry trends from R/Pharma 2025, this guide provides actionable career advice for R professionals working in (or aspiring to work in) the pharmaceutical industry."
  },
  {
    "objectID": "summary/career-insights.html#skills-in-high-demand",
    "href": "summary/career-insights.html#skills-in-high-demand",
    "title": "Career Insights",
    "section": "ğŸ”¥ Skills in High Demand",
    "text": "ğŸ”¥ Skills in High Demand\n\n1. AI/LLM Integration (ğŸ”¥ğŸ”¥ğŸ”¥ Hot!)\nWhy it matters: 8+ sessions focused on AI, representing the biggest shift in pharma programming since R adoption began.\nWhat to learn:\n\n{ellmer} package for LLM integration\nPrompt engineering basics\nRAG (Retrieval-Augmented Generation) concepts\nModel Context Protocol (MCP)\nPrivacy-preserving AI approaches\n\nGetting started:\n\nWorkshop: Getting Started with LLM APIs\nExperiment with ChatGPT/Claude API for data analysis tasks\nBuild a simple Shiny chatbot with {shinychat}\n\nCareer impact: Early adopters will be positioned as AI specialists in their organizations.\n\n\n\n2. Clinical Reporting Automation\nWhy it matters: Industry moving from manual coding to template-based, automated TFL generation.\nWhat to learn:\n\n{gtsummary} and {crane} for tables\n{officer} and {flextable} for Word reports\nQuarto for multi-format publishing\nCDISC ARS/ARM standards\n{cards} for Analysis Results Data\n\nGetting started:\n\nWorkshop: Advanced Clinical Reporting\nWorkshop: Cardinal TLGs\nCreate a personal TFL template library\n\nCareer impact: Ability to reduce TFL turnaround time from weeks to days makes you invaluable.\n\n\n\n3. Package Development & Validation\nWhy it matters: Internal package development is standard practice. Validation expertise is critical for GxP environments.\nWhat to learn:\n\n{devtools}, {usethis}, {testthat}\nPackage structure and best practices\nDocumentation with {roxygen2}\nRisk-based validation approaches\n{riskmetric} for package assessment\n\nGetting started:\n\nWorkshop: Building R Packages\nPackage one of your current projects\nContribute to a pharmaverse package\n\nCareer impact: Package developers and validation experts command premium salaries and have job security.\n\n\n\n4. Shiny Application Development\nWhy it matters: Interactive apps for clinical data exploration, AI interfaces, and decision support are proliferating.\nWhat to learn:\n\nShiny fundamentals (reactive programming)\n{teal} framework for clinical apps\nShiny app validation strategies\nPerformance optimization\nDeployment (Posit Connect)\n\nGetting started:\n\nWorkshop: teal Framework\nBuild a simple adverse event explorer\nLearn about Shiny app testing ({shinytest2})\n\nCareer impact: Shiny developers with validation experience are in short supply.\n\n\n\n5. Bayesian Methods\nWhy it matters: FDA increasingly accepting Bayesian approaches. Powerful for small samples and adaptive designs.\nWhat to learn:\n\nStan basics\n{brms} for applied modeling\nBayesian workflow principles\nPrior specification\nMCMC diagnostics\n\nGetting started:\n\nWorkshop: Bayesian Survival Models\nWorkshop: Debugging Stan\nStatistical Rethinking book/course\n\nCareer impact: Bayesian expertise opens doors to advanced analytics roles and consulting.\n\n\n\n6. Data Validation & Quality\nWhy it matters: Data quality is paramount in regulated environments. Validation is non-negotiable.\nWhat to learn:\n\n{pointblank} for data validation\nGxP requirements (21 CFR Part 11, EU Annex 11)\nIQ/OQ/PQ documentation\nAutomated testing strategies\n\nGetting started:\n\nWorkshop: pointblank\nImplement validation checks on your datasets\nLearn about validation documentation\n\nCareer impact: Quality/validation specialists are always in demand."
  },
  {
    "objectID": "summary/career-insights.html#career-paths",
    "href": "summary/career-insights.html#career-paths",
    "title": "Career Insights",
    "section": "ğŸš€ Career Paths",
    "text": "ğŸš€ Career Paths\n\nEntry Level â†’ Mid Career\nTypical progression:\n\nJunior Statistical Programmer (0-2 years)\n\nFocus: Learn CDISC standards, basic R programming\nSkills to develop: dplyr, ggplot2, SDTM/ADaM basics\nGoal: Independently create simple TFLs\n\nStatistical Programmer (2-5 years)\n\nFocus: Complex TFLs, package development, validation\nSkills to develop: {gtsummary}, {admiral}, git/GitHub\nGoal: Lead TFL development for studies\n\nSenior Statistical Programmer (5-8 years)\n\nFocus: Automation, AI integration, team leadership\nSkills to develop: Shiny, AI/LLM, validation strategies\nGoal: Architect solutions, mentor juniors\n\n\n\n\nSpecialization Paths\n\nPath 1: AI/Data Science Specialist\n\nFocus on LLM integration, advanced analytics, ML\nLearn Python, Stan, cloud platforms (AWS)\nBecome the â€œAI expertâ€ in your organization\n\n\n\nPath 2: Clinical Reporting Architect\n\nMaster reporting automation (officer, Quarto, ARS)\nDesign template systems and workflows\nLead efficiency initiatives\n\n\n\nPath 3: Validation & Compliance Lead\n\nDeep dive into GxP, validation frameworks\nBecome risk assessment expert\nInterface with regulators and auditors\n\n\n\nPath 4: Open-Source Contributor\n\nContribute to pharmaverse packages\nBuild reputation in community\nOpportunities at Posit, pharma, consulting"
  },
  {
    "objectID": "summary/career-insights.html#for-experienced-professionals",
    "href": "summary/career-insights.html#for-experienced-professionals",
    "title": "Career Insights",
    "section": "ğŸ’¼ For Experienced Professionals",
    "text": "ğŸ’¼ For Experienced Professionals\n\nTransitioning from SAS to R\nReality: SAS experience is still valuable! Many organizations need people who know both.\nStrategy:\n\nLeverage your domain knowledge\n\nYou understand clinical trials, regulations, CDISC\nR syntax is learnable in weeks; expertise takes years\n\nStart with parallel development\n\nCreate R versions of your SAS programs\nCompare outputs (builds confidence)\nDocument differences and learnings\n\nFocus on R strengths\n\nInteractive visualizations (ggplot2, plotly)\nShiny apps (impossible in SAS)\nModern reporting (Quarto)\nAI integration (trivial in R, hard in SAS)\n\nGet formal training\n\nGSKâ€™s success built on structured training\nInvest in courses (R/Pharma workshops, Posit)\nJoin internal R user groups\n\n\nTimeline: 6-12 months to become productive; 18-24 months to become expert.\n\n\n\nMoving into Leadership\nRequirements for technical leadership:\n\nTechnical depth: Can solve complex problems\nStrategic vision: Understand industry trends\nCommunication: Explain technical concepts to non-technical stakeholders\nMentorship: Develop junior team members\nBusiness acumen: Connect technology to outcomes (cost savings, time reduction)\n\nPositioning yourself:\n\nLead high-visibility projects\n\nAI pilot programs\nAutomation initiatives\nValidation frameworks\n\nPresent at conferences\n\nR/Pharma\nPharmaSUG\nInternal symposiums\n\nPublish and share\n\nBlog posts\nGitHub repositories\nWhite papers\n\nBuild your network\n\nLinkedIn presence\nConference attendance\nIndustry working groups"
  },
  {
    "objectID": "summary/career-insights.html#learning-resources",
    "href": "summary/career-insights.html#learning-resources",
    "title": "Career Insights",
    "section": "ğŸ“ Learning Resources",
    "text": "ğŸ“ Learning Resources\n\nFree Resources\nOnline Courses:\n\nR for Data Science (book + online)\nStatistical Rethinking (lectures on YouTube)\nPosit Recipes & How-Tos\n\nCommunities:\n\nPharmaverse Slack\nR/Pharma conference\nPosit Community\nStack Overflow\n\nPractice:\n\nCDISC Pilot datasets\nKaggle pharma competitions\nPersonal projects on GitHub\n\n\n\nPaid Resources\nCourses:\n\nPosit Academy\nStatistical Rethinking course\nDataCamp / Coursera specializations\n\nBooks:\n\nR Packages (2nd edition)\nAdvanced R\nClinical Trial Data Analysis using R\nBiostatistics textbooks\n\nConferences:\n\nPharmaSUG\nUseR!\nrstudio::conf (now posit::conf)"
  },
  {
    "objectID": "summary/career-insights.html#networking-strategies",
    "href": "summary/career-insights.html#networking-strategies",
    "title": "Career Insights",
    "section": "ğŸ¤ Networking Strategies",
    "text": "ğŸ¤ Networking Strategies\n\nBuild Your Reputation\n\nContribute to open source\n\nFix bugs in pharmaverse packages\nAdd features\nImprove documentation\nYour GitHub is your portfolio\n\nPresent your work\n\nInternal brown bag sessions\nR/Pharma lightning talks\nRegional R user groups\n\nWrite about your experiences\n\nCompany blog (if allowed)\nLinkedIn posts\nMedium articles\nPersonal website\n\n\n\n\nEngage with Community\nOnline:\n\nJoin pharmaverse Slack\nAnswer questions on Stack Overflow (r+pharma tags)\nComment on relevant LinkedIn posts\nParticipate in working groups (R Validation Hub, ASA-BIOP)\n\nIn-Person:\n\nAttend R/Pharma (even virtually)\nLocal R user groups\nCompany data science meetups"
  },
  {
    "objectID": "summary/career-insights.html#job-search-tips",
    "href": "summary/career-insights.html#job-search-tips",
    "title": "Career Insights",
    "section": "ğŸ¯ Job Search Tips",
    "text": "ğŸ¯ Job Search Tips\n\nFor Your Resume\nHighlight:\n\nR packages youâ€™ve developed (link to GitHub)\nAutomation youâ€™ve implemented (quantify time savings)\nValidation experience (GxP compliance)\nAI/Shiny projects\nContributions to open-source projects\n\nInclude:\n\nLink to GitHub profile\nLink to personal website/portfolio\nSpecific technologies (not just â€œRâ€)\n\n\n\nFor Interviews\nPrepare to discuss:complex technical problem you solved - How you approach validation - Experience with clinical data - Contributions to efficiency - Learning new technologies quickly\nAsk about: - Open-source contribution policies - Training budget and opportunities - Technology stack and roadmap - Team culture around R adoption - Career development paths\n\n\nCompanies to Watch\nActively hiring R professionals:\n\nBig Pharma: GSK, Roche/Genentech, Novartis, Pfizer, Moderna\nTechnology: Posit PBC\nCROs: Many transitioning to R\nConsulting: A2-AI, Jumping Rivers, Ardata\nStartups: Biotech companies using modern stacks"
  },
  {
    "objectID": "summary/career-insights.html#industry-outlook",
    "href": "summary/career-insights.html#industry-outlook",
    "title": "Career Insights",
    "section": "ğŸ“Š Industry Outlook",
    "text": "ğŸ“Š Industry Outlook\n\nPositive Trends\nâœ… R adoption accelerating (GSK 50%+ is proof)\nâœ… AI creating new roles (LLM specialists)\nâœ… Regulatory acceptance (FDA neutrality)\nâœ… Open-source momentum (pharmaverse thriving)\nâœ… Remote work normalized (global opportunities)\n\n\nChallenges\nâš ï¸ Validation complexity (but solvable)\nâš ï¸ Skills gap (more demand than supply - good for you!)\nâš ï¸ Rapid change (must commit to continuous learning)\n\n\nBottom Line\nThe next 5 years will see explosive growth in demand for R professionals in pharma. Early adopters with AI/automation/validation skills will have their pick of opportunities."
  },
  {
    "objectID": "summary/career-insights.html#final-thoughts",
    "href": "summary/career-insights.html#final-thoughts",
    "title": "Career Insights",
    "section": "ğŸ’¡ Final Thoughts",
    "text": "ğŸ’¡ Final Thoughts\nThe R/Pharma 2025 conference made clear: this is the golden age for R professionals in pharma. With AI integration, open-source adoption, and regulatory acceptance converging, opportunities are abundant for those willing to:\n\nLearn continuously (technology changes fast)\nContribute generously (open source is key)\nCommunicate effectively (technical + business)\nValidate rigorously (quality is non-negotiable)\nInnovate boldly (early adopters win)\n\nYour R skills are valuable. Make them invaluable.\n\n\n\n\n\n\n\nTipStay Informed\n\n\n\n\nSubscribe to pharmaverse newsletter\nFollow key people on LinkedIn (Joe Zhu, Daniel Sjoberg, others from conference)\nBookmark this site and check back for updates\nJoin r-pharma mailing list\n\n\n\n\nCareer guidance compiled from R/Pharma 2025 trends and industry insights | Last updated: November 2025"
  },
  {
    "objectID": "Descriptions.html",
    "href": "Descriptions.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nR-Pharma-2025-Workshops\n\nWORKSHOP: Flexible Clinical Trial Design, Simulation, and Analysis with the R Package rpact\nDaniel Sabanes Bove Entrepreneur in Biostatistics Consulting and Engineering, RCONIS\nFriedrich Pahlke Tech Entrepreneur | Driving Innovation in R/Shiny\nIn this workshop we will explore the capabilities of the validated open-source R package â€˜rpactâ€™, which is available on CRAN and GitHub. rpact is a comprehensive validated, open source, free-of-charge R software package for clinical trial planning, design simulation, and data analysis. rpact is under continuous full-time development since 2017, and comprehensive documentation is available atÂ www.rpact.org. We will provide an introduction to the package and illustrate the usage by several examples. The focus of the package is on group sequential and adaptive designs with p-value combination tests, but also fixed sample designs can be considered. The software can specifically be used to assess design characteristics of popular group sequential designs. However, going beyond those, adaptive designs are a strength of rpact: Adaptive multi-armed and population enrichment designs that are based on closed combination tests can be assessed by simulation. The application of the designs for simulation, real data, and estimation is possible for continuous, binary, survival, and count data. Furthermore, we introduce â€˜RPACT Cloudâ€™, a user-friendly platform designed to simplify and enhance the process of clinical trial design and simulations for researchers and practitioners. A free version of RPACT Cloud is available atÂ https://cloud.rpact.com/Â and workshop participants will be able to try out the software themselves following the workshop.\nLink to the presentation: https://rpharma.presentation.2025.rpact.com/\n\n\nWORKSHOP: Debugging Stan Programs\nDaniel Lee\nData Scientist, Bayesian statistician, Stan developer\nHaving trouble getting your Stan models to behave? This hands-on workshop will explore practical strategies for debugging Stan code, from identifying non-identifiable parameters to improving runtime performance. Participants will gain tools and intuition for building more robust modelsâ€”skills that extend to a wide range of statistical and stochastic programming tasks.Prerequisite: Familiarity with Stan is recommended.\nWorkshop related link: https://github.com/bayesianops/stan-tutorials/tree/main/debugging-2025-11-03\n\n\nWORKSHOP: Introduction to building (better) R packages\nNicola Rennie\nData Visualisation Specialist\nThere are many benefits to turning your R scripts or functions into a package, like making your code easier to re-use, easier to share with others, easier to document, and easier to test. But the process of writing a package can feel intimidating, especially if you havenâ€™t done it before. But it doesnâ€™t need to! In this interactive workshop, weâ€™ll discuss:* What things you need to make a package and how to create them* How to write functions (in a package-friendly way) and add them to a package* How to write documentation and examples for functions* Best practices for package development* How to share your package with other people* Useful resources. By the end of this workshop, you will have made an R package! The session aims to be introductory, so you donâ€™t need any previous experience of building R packages (or even writing functions!) but some basic knowledge of R will be useful. If youâ€™re already a seasoned R package developer, youâ€™re also welcome to attend and hopefully youâ€™ll still pick up a few package development tips!\nWorkshop related link: https://nrennie.rbind.io/r-pharma-2025-r-packages/#/title-slide\n\n\nWORKSHOP: Getting Started with LLM APIs in R\nSara Altman\nData Science Educator\nPosit PBC\nLLMs are transforming how we write code, build tools, and analyze data, but getting started with directly working with LLM APIs can feel daunting. This workshop will introduce participants to programming with LLM APIs in R using ellmer, an open-source package that makes it easy to work with LLMs from R. Weâ€™ll cover the basics of calling LLMs from R, as well as system prompt design, tool calling, and building basic chatbots.\nNo AI or machine learning background is requiredâ€”just basic R familiarity. Participants will leave with example scripts they can adapt to their own projects.\nWorkshop related link: https://skaltman.github.io/r-pharma-llm/\n\n\nWORKSHOP: Hands on with Cardinal: Harmonizing Clinical Reporting with Openâ€‘Source TLGs\nAbinaya Yogasekaram\nBioinformatics | Software Development | Data Science\nAmaris Consulting\nEmily De La Rua\nData Scientist at Roche\nCardinal is an open-source collection of standardized table, listing, and graph (TLG) templates designed to streamline the process of clinical output review, comparison, and meta-analysisâ€”promoting efficient communication to stakeholders while aligning with CDISCâ€™s ARD/ARM efforts.\nIn this workshop, weâ€™ll introduce the {gtsummary} package for table creation and walk through Cardinalâ€™s growing catalog of TLGs. Participants will get hands-on experience generating key outputsâ€”ideally working on examples that are directly relevant to your own work.\nWhether youâ€™re new to Cardinal, looking to integrate it into your own workflow, or interested in contributing to the project, this workshop will provide practical insights and real-world applications for modern clinical reporting.\nWorkshop related link: https://pharmaverse.github.io/cardinal/\n\n\nWORKSHOP: Advanced Clinical Reporting with officer and flextable\nDavid Gohel\nAdvanced Clinical Reporting with officer and flextable\nIn this workshop you will be working with the officer and flextable packages to create sophisticated clinical reports in Word format using R with a reproducible approach. Going from clinical data all the way through to complete pharmaceutical report generation. Specifically, we will walk through an end-to-end example focusing on advanced document structure management, complex table creation following pharmaceutical standards, and integration of ggplot2 visualizations. The workshop is divided into two parts: first, discovering the fundamentals of officer and flextable packages, then moving to advanced clinical reporting techniques including section management, headers and footers customization, and cross-reference handling for complete pharmaceutical report composition.\nWorkshop related links:\nhttps://ardata.fr/r-in-pharma-2025/\nhttps://github.com/ardata-fr/r-in-pharma-2025-codes\nhttps://github.com/ardata-fr/r-in-pharma-reporting-with-officer-flextable\n\n\nWORKSHOP: Creating Polished, Branded Documents with Quarto\nIsabella VelÃ¡squez\nSr.Â Product Marketing Manager\nPosit PBC\nJoin us for a hands-on, one-day workshop at R/Phama, where weâ€™ll explore the versatility of Quarto output formats. You will learn how to create dynamic websites, professional PDF documents, engaging presentations, and interactive dashboards using Quarto. This workshop highlights Quartoâ€™s powerful theming capabilities, including the new support for brand.yml, which ensures that your work maintains a professional and cohesive style across all formats.\nWorkshop related links:\nhttps://019a4f2d-6b79-72c1-834b-c2a9488f9ec8.share.connect.posit.cloud/\nhttps://github.com/ivelasq/2025-11-04_branded-quarto\n\n\nWORKSHOP: Polars: The Blazing Fast Python Framework for Modern Clinical Trial Data Exploration\nMichael Chow\nPrincipal Software Engineer at Posit (Open Source)\nPosit\nJeroen Janssens\nHead of Developer Relations\nPosit PBC\nClinical trials generate complex and standards driven datasets that can slow down traditional data processing tools. This workshop introduces Polars, a cutting-edge Python DataFrame library engineered with a high-performance backend and the Apache Arrow columnar format for blazingly fast data manipulation. Attendees will learn how Polars lays the foundation for the pharmaverse-py, streamlining the data clinical workflow from database querying and complex data wrangling to the potential task of prepping data for regulatory Tables, Figures, and Listings (TFLs). Discover the â€˜delightfulâ€™ Polars API and how its speed dramatically accelerates both exploratory and regid data tasks in pharmaceutical drug development. The workshop is led by Michael Chow, a Python developer at Posit who is a key contributor to open-source data tools, notably helping to launch the data presentation library Great Tables, and focusing on bringing efficient data analysis patterns to Python.\nWorkshop related link:\nhttps://github.com/machow/examples-great-tables-pharma\n\n\nWORKSHOP: How to use pointblank to understand, validate, and document your data\nRich Iannone\nSoftware Engineer at Posit PBC\nThis workshop will focus on the data quality and data documentation workflows that the pointblank package makes possible. We will use functions that allow us to:\n(1) quickly understand a new dataset\n(2) validate tabular data using rules that are based on our understanding of the data\n(3) fully document a table by describing its variables and other important details. The pointblank package was created to scale from small validation problems (â€œLetâ€™s make certain this table fits my expectations before moving onâ€) to very large (â€œLetâ€™s validate these 35 database tables every day and ensure data quality is maintainedâ€) and weâ€™ll delve into all sorts of data quality scenarios so youâ€™ll be comfortable using this package in your organization. Data documentation is seemingly and unfortunately less common in organizations (maybe even less than the practice of data validation). Weâ€™ll learn all about how this doesnâ€™t have to be a tedious chore. The pointblank package allows you to create informative and beautiful data documentation that will help others understand whatâ€™s in all those tables that are so vital to an organization.\nWorkshop related link:\nhttps://github.com/rich-iannone/pointblank-workshop\n\nEurope/US Session #1\n\n\n\nPractical AI for data science\nSimon Couch (Posit). Practical AI for data science\nWhile most discourse about AI focuses on glamorous, ungrounded applications, data scientists spend most of their days tackling unglamorous problems in sensitive data. Integrated thoughtfully, LLMs are quite useful in practice for all sorts of everyday data science tasks, even when restricted to secure deployments that protect proprietary information. At Posit, our work on ellmer and related R packages has focused on enabling these practical uses. This talk will outline three practical AI use-casesâ€”structured data extraction, tool calling, and codingâ€”and offer guidance on getting started with LLMs when your data and code is confidential.\nhttps://github.com/simonpcouch\n\nEurope/US Session #2\n\n\n\nduckplyr: Analyze large data with DuckDB and dplyr compatibility\nKirill Muller (Cynkra).\nThe duckplyr package is now stable; version 1.1.2 is on CRAN. It builds on top of DuckDB, a fast and flexible analytical engine that can work with larger-than-memory data from disk or cloud storage. All these features are available to duckplyr, with syntax and semantics much closer to R and the tidyverse. Use it to speed up existing code, to analyze Parquet or CSV files directly, or to access the plethora of exciting functionality provided by DuckDB.\n\n\nBeyond Training: Evolving Strategies to Teach and Support R Adoption in Pharma\nAlanah Jonas (GSK).\nTeaching R effectively is just the starting point for lasting adoption in pharma. At GSK, we began by developing two core R courses written in bookdown and delivered interactively, an approach that enabled high completion rates.\nRecognizing that training alone isnâ€™t enough, we expanded support through options like self-certification, intermediate courses, and a Resource Hub. We also launched AccelerateR to provide tailored support to early adopters during their transition.\nToday, those efforts have scaled and evolved into Rburst, a cross-functional initiative designed not just to teach R, but to embed it into our ways of working across all of Biostatistics.\nThis talk will explore how our training strategy has matured into a broader support model, enabling wide-scale, integrated adoption of R in everyday work.\n\n\nValidating Shiny Apps in Regulated Environments\nPedro Silva (Jumping Rivers).\nShiny apps are increasingly used to deliver interactive tools in clinical and healthcare settings. But when these tools are used in regulated environments, validation becomes essential. How can we ensure that our Shiny apps are trustworthy, without stifling innovation?\nIn this talk weâ€™ll explore practical approaches to validating Shiny applications in regulated contexts, drawing on principles from software engineering, quality assurance, and risk based validation. Weâ€™ll discuss key challenges like traceability, documentation, and versioning, as well as share techniques for building apps that are easier to validate from the start.\nIâ€™ll highlight some of the tools and packages used in Jumping Rivers that can support validation workflows that satisfies both internal reviewers and external regulators, including the Litmusverse, a suite of R packages designed to assess code quality and generate validation evidence.\nBy the end of the session, youâ€™ll understand: - Why validation is critical for Shiny apps in regulated contexts; - What elements make a Shiny app more or less validatable; - How to incorporate validation strategies into your development process.\nThis session is ideal for R users in pharma, clinical research, and healthcare who want to build confidence in their dashboards, while maintaining flexibility in how they work.\n\n\nBeyond {gtsummary}: How the {crane} Package Extends the Framework for Pharmaceutical Reporting\nDaniel Sjoberg (Genentech) and Davide Garolini (Genentech).\n{gtsummary} has become the most-used R package for clinical tables in the R ecosystem, winning awards from the American Statistical Association and Posit. Building on this foundation, we created {crane}, an open-source extension that facilitates reporting requirements in the pharmaceutical space. Because {crane} and {gtsummary} are built upon Analysis Results Datasets (ARDs), study teams can take advantage of this foundation that streamlines the QC process and allows for simple re-use of calculated results in subsequent reporting.\nIn this session we show how: (1) a vanilla {gtsummary} script instantly upgrades when {crane} is loaded; (2) ARD-based outputs make QC a straightforward operation; (3) LLMs can summarise the language-agnostic ARD results for medical writers in seconds. Youâ€™ll leave with a blueprint of how to adapt {crane} for your reporting needs or (or roll your own) and shorten table turnaround on day one.\nhttps://www.danieldsjoberg.com/RinPharma-crane-2025/\nhttps://github.com/ddsjoberg/RinPharma-crane-2025/tree/main\nhttps://github.com/cynkra/r-in-pharma-2025/\nhttps://docs.google.com/presentation/d/1atV5rBLQF2vcCg2lraiutg_Xx2BOEklZzf9LJt842ig/edit?usp=sharing\n\nEurope/US Session #3\n\n\n\n{llumen}: An agentic LLM framework for biomedical documents, databases & foundation models\nSven-Eric Schelhorn (lumen).\nWe introduce {llumen}, an R package used at Merck KGaA, Darmstadt, Germany that implements an agentic framework bases on {ellmer} and enables pharmaceutical researchers to work with biomedical documents, large real-world evidence and biomarker databases, as well as with biological foundation models using large language models as orchestrators.\n{llumen} currently supports the following functionalities:\n\nLarge language models*\nRuns on standard company laptops (Windows, Linux, or macOS).\nAllows using a wide range of private (Azure), public (OpenAI, Anthropic, Bedrock), and local (llama.cpp) LLMs.\nProvides both programmatic and fully interactive (chatbot) ways to interact with the LLM.\nSupports chain-of-thoughts, tree-of-thoughts, and ReAct reasoning strategies.\nSupports context management to maintain conversation history and state.\nText analysis*\nSupports embeddings (Azure, public, or local) using a local, high-performance vector DB (based on {ragnar}).\nImplements chunking office documents (Word, Excel, Powerpoint, RTF, PDF) into the vector DB.\nWhile reading office documents, supports extracting tables with summary data.\nEnables retrieval-augmented generation (RAG) using the vector DB, either explicitly or by using a RAG tool that the LLM calls.\nEmulates the Future Houseâ€™s PaperQA2 approach for summarizing and re-ranking parts of semantically related documents so that answer quality and comprehensiveness is significantly boosted.\nExtracts text from documents to fill structured templates, such as the JSON-based {llumen} Analysis Results Schema (LARS). Extracted JSONs are directly validated by the LLM using an R tool.\nSupports automatic diagram generation from texts, for instance to capture the main concepts of a paper in two or three flow charts.\nProvides advanced text preprocessing and cleaning capabilities for various document types.\nOffers customizable tokenization and text segmentation methods.\nAgentic tool use and codegen*\nAllows the LLM to search the internet via Google and access the textual content of websites in a controlled manner. Also includes specialized query tools for Pubmed, Pubchem, and Wikipedia.\nEnables the LLM agent to query Parquet files, Neo4j knowledge graphs, and GraphQL APIs using SQL, Cypher and GraphQL codegen in a secure context. The database agents are very capable and can answer complex, open-ended scientific questions using these data sources.\nEnables the LLM agent to use local foundation models, such as Googleâ€™s TxGemma model for pharmaceutical applications, for instance for predicting solubility of small molecules.\nUses multimodal LLMs to accurately diagnose cancer histopathology images with very comprehensive pathology reports that investigate architectural and cytological Features of H&E slides.\nMakes it easy to give the LLM new R tool capabilities (basically all R functions), share tools via MCP (built-in pure R MCP server), or utilize tools of external MCP server as internal tools (via a MCP tool wrapper).\nSupports dynamic tool creation and registration during runtime.\nProvides a framework for defining custom tools and integrating them seamlessly with the LLM.\n\nWe are internally using the package for the following usecases:\n\nSupported usecase*\n\n\nExtracting results of statistical analyses from office documents: The vignettes show how to configure an agentic LLM to extract chunks oftext from documents and tables, semantically query a vector store (agent-based retrieval-augmented generation, RAG), load JSON definitions, extract JSON-structured data from documents, and validate these definitions against a user-defined JSON template. {llumen} can be utilized to extract results data from a large corpus of clinical trial results files. Templates for both CDISC Analysis Result Standard (ARS) and the {llumen} Analysis Result Schema (LARS) are provided.\nQuerying tabular databases and knowledge graphs: {llumen} excels at SQL codegen, allowing users to query very large Parquet files located on the userâ€™s device and perform complex data analyses. It also supports querying Neo4j knowledge graphs and GraphQL APIs, enabling comprehensive data exploration and analysis.\nWorking with biological foundation models: {llumen}â€™s ability to use local foundation models specialized for particular biological or pharmaceutical applications, such as Googleâ€™s GemmaTX, enhances agentic LLMs with specialized capabilities for tasks like predicting molecular properties or analyzing biological pathways.\nAutomated literature review and summarization: {llumen} can be used to perform comprehensive literature reviews by searching multiple databases (e.g., PubMed, OpenAlex), extracting relevant information, and generating structured summaries of findings.\nHistopathology image analysis: The package supports the use of multimodal LLMs for analyzing histopathology images, providing detailed reports on architectural and cytological features of H&E slides, which can aid in cancer diagnosis and research.\nDrug discovery and development support: By integrating various tools and databases (e.g., PubChem, TxGemma), {llumen} can assist in various stages of drug discovery, from initial compound screening to predicting drug properties and potential interactions.\nClinical trial data analysis and reporting: {llumen}â€™s capabilities in extracting and structuring data from various document types make it well-suited for analyzing and reporting on clinical trial data, potentially accelerating the process of deriving insights from trial results.\nBiomedical knowledge graph exploration: The packageâ€™s ability to query knowledge graphs allows researchers to explore complex relationships in biomedical data, potentially uncovering new insights or research directions.\nAutomated scientific writing assistance: {llumen} can be used to assist in scientific writing tasks, such as generating literature summaries, creating structured abstracts, or even drafting sections of scientific papers based on analyzed data and literature.\nRegulatory document preparation and analysis: The packageâ€™s capabilities in extracting and structuring information from various document types can be applied to preparing and analyzing regulatory documents in the pharmaceutical industry\n\nPresentation related link:\nhttps://drive.google.com/file/d/1g0_O_wSoeBHzeXl6DF3-NIY7gT2Bk62_/view\n\n\nBuild Model Context Protocol servers and clients in R\nJohn Coene (Opifex).\nThe mcpr package provides a comprehensive R implementation of the Model Context Protocol (MCP), a standardized JSON-RPC 2.0 interface that enables R applications to expose computational capabilities to AI models. This package bridges the gap between Râ€™s statistical computing environment and modern AI assistants by allowing R developers to create MCP servers that expose tools, resources, and prompts as well as client functionality to interact with existing MCP servers.\nKey features include schema-based tool definitions, multi-modal response support, and seamless integration with popular AI development environments including Claude Code, Cursor, and VS Code. This implementation democratizes AI-R integration by providing a standards-based approach to exposing Râ€™s extensive ecosystem to conversational AI interfaces, opening new possibilities for interactive data analysis and statistical computing workflows.\n\nEurope/US Session #4\n\n\n\nMosaic: Open-Source, ARS-Driven Automation of Standard TFLs\nConor Moloney (Novartis).\nCDISC standards help streamline datasets (e.g.Â SDTM and ADaM), yet analysis results lack a common model. The emerging Analysis Results Standard (ARS) will help to fill this gap with machine-readable specifications. Mosaic couples ARS with an open-source stack to generate fully validated standard TFLs, fast and reproducibly.\nHow it works\nMosaic captures the Analysis Results Dataset (ARD) requirement in ARS-aligned YAML; updates and changes are made by editing this metadata, not code. After LinkML validation, a Python layer stores the YAML in a database via a SQL Alchemy ORM engine. The results-generation layer is intentionally language-agnostic . The rules that transform ADaM datasets into the ARD are expressed as metadata that can be rendered in any statistical language. Mosaicâ€™s implementation uses R to derive the ARD.\nA React UI reads the validated ARD. Here, users have controlled customisation options. Users can preview each TFL shell in real time and export regulator-ready RTF files suitable for submission packages.\nMosaic replaces ad-hoc programming with a transparent, standards-based pipeline, accelerating TFL delivery while safeguarding traceability and regulatory compliance. This reallocates programmer effort from repetitive coding to high-value scientific review\n\n\nIntegrating Collaborative Programming with Automated Traceability and Reproducibility in Pharma\nJennifer Dusendang (Graticule Inc) and Sundeep Bath (Graticle Inc).\nIntegrating Collaborative Programming with Automated Traceability and Reproducibility in Pharma Studies and Real-World Data Projects by Adapting DevOps Best-Practices\nTo enhance integrity of research and study findings, data scientists should ensure that studies are traceable and reproducible, which involves meticulous management of datasets, tracking code changes, and robust storage of results.\nWithout infrastructure to support reproducibility efforts, documentation, dependency management, and version control processes can be manual, unreliable, and unclear. This creates problems with determining when analysis changes occurred, which version of study results were produced by which version of code, and whether all study steps are processed in proper order and appropriately documented.\nImplementing procedures and technical infrastructure helps to maintain and automate reproducibility and traceability. To ensure that code can be executed consistently across multiple compute environments, we structure analysis scripts into parameterized pipelines within an isolated Docker container environment which specifies all versions and dependencies. We integrate Continuous Integration (CI) and Continuous Delivery (CD) into analysis pipelines to enable automatic rerunning of analyses following code modifications and storage of results in the cloud. Our process integrates and improves collaborative programming by providing code reviewers with the validated outputs that are produced by the code. By design, study close-out and compliance activities are incorporated within our infrastructure.\nIn this paper we will discuss how we implemented and adapted DevOps best-practices like CI/CD in a collaborative coding environment to work for epidemiological studies and real-world data projects. Although the concepts discussed are applicable to many tools, our implementation uses Git, GitHub Actions, SQL, Python, R, Docker, and AWS S3. This content is applicable for all skill levels.\nhttps://www.lexjansen.com/pharmasug/2025/OS/PharmaSUG-2025-OS-111.pdf\n\n\nTabPFN: A Deep-Learning Solution for Tabular Data\nMax Kuhn (Posit).\nThere have been numerous proposals for deep neural networks for tabular data, such as rectangular data sets (e.g., data frames). To date, none have really worked well and take far too long to train. TabPFN is a model that emulates a Bayesian approach and trains a deep learning model on a prior of simulated tabular datasets. Version 2 was released this year and offers several significant advantages, but also has one notable disadvantage. Iâ€™ll introduce this model and show an example.\nhttps://topepo.github.io/2025-r-pharma/#/title-slide\n\n\nThe LLM Lounge: Live-coding and conversation on using AI for data science\nJoe Cheng\nPosit\nEric Nantz\nEli Lilly\nThe number of workflows and tools leveraging large-language-models (LLMs) is growing exponentially across the world, and the life sciences industry is no exception. In this session, Eric Nantz takes the role of the curious and savvy data scientist as he is joined by Posit CTO Joe Cheng in a special live demonstration on using Databot to power an exploratory data science analysis. Throughout the demonstration, Joe will share the fascinating origin story of how Databot came to be, along with a deeper conversation on the recent trends and guiding principles of harnessing AI tools for data analyses and software engineering. This interactive, spontaneous format will allow the audience to submit questions and steer the direction of the session, making it a truly engaging, shared experience.\nhttps://github.com/posit-dev/querychat\n\n\nLLM-Powered {gtsummary}: QC-Ready Clinical Tables in Minutes\nDavide Garolini\nData Scientist, NEST, Roche\nHe significantly contributes to the NEST project by improving key tools like {gtsummary} and {rtables} for regulatory compliance.\nOpen-source R tooling has made table generation easy, yet interpreting the code and guaranteeing bullet-proof QC can still delay submissions. We present a workflow that fuses {gtsummary} for rapid table building with cards for â€œautomaticâ€ validation, topped by a fully offline, model-agnostic LLM helper. Starting on synthetic CDISC-like data, we create a standard summary table, validate it with {cards}, and ask the LLM-helper to explain each step and result and propose, if necessary, next stepsâ€”all without revealing trial data. A descriptive log is generated on the fly, combining code, QC results, and LLM explanations in one auditor-friendly document. When real data arrive, the same script reruns unchanged, delivering submission-ready tables in minutes while boosting clarity and compliance.\n\n\nPost-Approval Drug Exposure Estimation Using an R Shiny App\nFeifei Yang\nAstraZeneca\nYu Zhang\nAstraZeneca\nBackground\nRegulatory authorities require product post-marketing exposure estimates in aggregate safety reports for purposes of signal detection. The calculation of exposure estimates requires multiple data sources between sales data and aggregated de-identified patient data. Traditionally, the calculations were manually handled due to limited countries, products, and indications. With the growth of both markets and products, manual calculations are inefficient to handle the increasing volume of data and regional requests. An approach to automating the entire process has become essential.\nMethod\nR Shiny App is a web application framework that allows us to create interactive web applications for data analysis, data visualization, and interactive reports, making it easier to share data insights with others. We converted monthly patient and sales report into an app-readable format and applied an exposure estimate algorithm to perform calculations by region, product, and indication.\nResults\nThe time to generate a standard report of post-marketing exposure data for regulatory purposes has been reduced from a week to one day. The QC function of application helps check inconsistent numbers from month to month in sales and patient data. In addition to the exposure report, the visualization provided by the application shows trends in sales, patient utilization of products, and exposure by year and geographic regions. The sharing ability of the application enables other team members to generate customized report directly without any prerequisite training.\nConclusion\nThe application significantly improves the efficiency of delivering post-marketing exposure data and reduces time and resources needed for data management and QC. Accessibility to the application helps disseminate the information to a broader range of teams and functions.\n\n\nImplementing an end-to-end NCA software using Shiny\nGerardo Jose Rodriguez\nLucid Analytics\nJana Spinner\nLucid Analytics\nPharmacokinetic studies are essential yet demanding, typically relying on expensive proprietary software for Non-Compartmental Analysis (NCA). aNCA is an alternative developed collaboratively by Roche, Lucid Analytics, Appsilon, and Human Predictions. It is being developed within Pharmaverse, aiming to become a worldwide standard by utilizing open source development to make the process more efficient, economical, and transparent. The app performs an end-to-end approach, using interactive plots for data exploration, half life customization, TLGs, and report generation.aNCA is planned to be released on CRAN soon. It currently reaches 100% of testing coverage on its functions and has successfully passed internal package validations.\naNCA uses PKNCA, a widely used package that calculates over 200 PK parameters, has been tested against other industry-standard NCA software, showing differences within Â±0.1% for most tested parameters.\nWe aim to present aNCA, highlight its innovative software concepts, discuss how we tackled key challenges, and showcase how open source and Shiny achieve industry-standard PK software. However, we welcome suggestions on focus areas for the R/Pharma audience.\nIf you want to know more about aNCA, feel free to follow the link to our GitHub: https://github.com/pharmaverse/aNCA\n\n\nIntegrating LLM using R Shiny for Clinical Data Review by Ensuring Data Privacy and Validity\nZhen Wu\nCIMS Global\nPeng Zhang\nCIMS Global\nThe pharmaceutical industry is shifting from traditional SAS-based workflows toward the open-source R ecosystem. R Shiny applications have become a popular solution for visualizing tables, figures, and listings. However, these applications often require a strong understanding of data structures and familiarity with interface components such as dropdowns, which may not be intuitive for clinical reviewers. Recent advancements in artificial intelligence (AI), particularly large language models (LLMs), have opened new possibilities for how users interact with clinical data. In this session, we present an innovative R Shiny application, {DataChat}, that enables users to â€œchat with dataâ€ through a conversational interface. Powered by the {ellmer}, {shinychat}, and {ragnar} packages, along with internal statistical tools and utilities, the app integrates retrieval-augmented generation (RAG) capabilities tailored to the pharmaceutical domain. The solution emphasizes user-friendliness, enabling non-programmers and clinicians to explore datasets and derive insights while maintaining compliance with data privacy requirements and addressing concerns around statistical validity. This approach exemplifies the potential of AI-augmented tools to enhance clinical data review and exploration in a practical and accessible way\n\n\nR we there yet? {admiral}â€™s journey transitioning from active development to stability\nEdoardo Mancini\nRoche\nIf you are attending R/Pharma 2025, you probably enjoy actively contributing to open source R packages for the pharma industry. Fixing bugs, implementing new features and expanding the use-case is work that feels motivating and meaningful. However, what happens if/when your package reaches a point of stability and maturity, where the main use-case is answered and feature completeness is in sight? How can you sustain your development teamâ€™s momentum without chasing perfection, in a time where the active workload is naturally diminished? What should your new priorities be?\nWhile there is no definitive answer to any of these questions, this talk will discuss the evolution of packages like {admiral} as they transition from active development to mature maintenance, drawing insights from the {admiral} teamâ€™s experiences in navigating this complex shift.\n\n\nWorkshop: R-Classification: Unleashing Predictive Power with tidymodels\n\nHarshavardhan Bajoria\n\nDive into the exciting world of classification with R and the elegant tidymodels framework! This hands-on workshop provides a comprehensive introduction to building, evaluating, and refining machine learning models that predict categorical outcomes. Youâ€™ll learn to preprocess your data effectively using recipes, split datasets for robust model training and testing with rsample, define and fit various classification algorithms using parsnip, and assess model performance using a suite of metrics from yardstick. Through practical exercises, including a wine classification challenge, youâ€™ll gain the skills to tackle real-world predictive problems and make data-driven decisions using the consistent and intuitive tidymodels ecosystem.\n\n\nWorkshop: A Guided Tour to Building and Integrating LLM Based Tooling with R\nDevin Pastoor\nChief Technology and Product Officer, A2-AI\nXu Fei\nXu Fei is a Senior Solutions Engineer at A2-Ai, where he builds AI-powered tools and infrastructure for pharmaceutical research workflows. Working across R and Python stacks, he has developed LLM-enabled applications ranging from interactive chatbots to MCP server implementations, with a focus on making GenAI accessible and practical for scientific computing teams. His work bridges enterprise DevOps, cloud APIs (AWS Bedrock), and domain-specific R and Python packages to help scientists integrate AI capabilities into their existing workflows\nAathira Anil Kumar\nA2-Ai\nJoin A2-AI engineers, Aathira Anil Kumar, Devin, and Xu Fei, for a practical, 2-hour workshop demonstrating how to integrate Generative AI (GenAI) into pharmaceutical workflows. This session focuses on bridging the R and Python ecosystems to deliver scalable, GxP-compliant solutions.\nYou will learn methodology for developing LLM-enabled applicationsâ€”from interactive chatbots for clinical study reporting and SOP management, to more complex MCP server implementations for reproducible analytics. Drawing on their extensive experience with life-science organizations and tools like AWS Bedrock, the instructors will showcase how to navigate real-world IT constraints while making GenAI accessible and practical for scientific computing teams. This workshop is essential for analysts, developers, and project leaders aiming to stand up GxP-compliant statistical computing environments with integrated AI capabilities.\n\nEurope/US Session #5\n\n\n\nGSKâ€™s journey to Clinical Study Reporting Using Open Source\nSam Warden\nVP & Global Head, Clinical Programming & Business Excellence\nGSK\nTim Colman\nGSK\nGSKâ€™s Journey to Clinical Study Reporting Using Open Source chronicles the transformation of clinical reporting at GSK, the presentation marks pivotal moments in the pharmaceutical industryâ€™s evolutionâ€”from the proprietary, SAS-dominated era of the 1980s and 90s, through the pain of rising R&D costs and global health crises, to the emergence of collaborative models and open-source innovation.\nThe story highlights how industry challenges catalyzed a shift toward openness and collaboration. Key milestones include the FDAâ€™s clarification on software neutrality, the rise of R and open-source platforms, and GSKâ€™s commitment to writing at least 50% of code in open-source languages. The COVID-19 pandemic accelerated this transformation, driving rapid adoption of data science platforms and collaborative tools.\nGSKâ€™s multi-wave approach to change is shared and candidly addresses ongoing challenges: technical validation, regulatory uncertainty, cultural resistance, and the need for robust governance and training.\nUltimately, the journey demonstrates that embracing open-source technologies enables greater automation, and innovation in clinical study reporting. Success depends on a growth mindset, adaptability, and a vision for collaborative transformationâ€”qualities that GSK continues to foster as it leads the industry into a new era of transparency and shared progress.\n\nEurope/US Session #6\n\n\n\nLeveraging ellmer and GPT to Integrate AI Agents into Shiny Applications for Accelerating Trials\nXing Chen\nModerna\nXiaolin Chang\nModerna\nBackground:\nArtificial intelligence is transforming drug development, but its integration into biostatistics and clinical workflows remains limited. Clinical and biometrics teams often face barriers in exploring high-dimensional, multi-view trial datasets, requiring technical expertise to extract insights and generate visualizations.\nMethods:\nWe developed an AI-enhanced R Shiny application that integrates ellmer with ChatGPTs. The app connects to Cellular-Mediated Immunogenicity (CMI) data across multiple mRNA infectious disease programs. Natural language queries are translated into structured R operations, through which users can interactively explore and visualize data, eliminating the need for manual coding.\nResults:\nThe application successfully handled unstructured user queries, generated targeted outputs, and produced customizable visualizations. Pilot deployment across infectious disease programs demonstrated faster extraction of trial insights, reduced dependency on ad-hoc programming\nsupport, and invigorates collaboration between statisticians, clinicians, and translational researchers.\nConclusions:\nEmbedding AI agents via ellmer within Shiny applications provides a scalable, user-friendly framework for accelerating exploratory analyses in vaccine development. This approach demonstrates how AI-assisted analytics can increase efficiency, strengthen cross-functional decision-making, and support broader adaptation of GenAI into clinical development workflows.\n\n\nautoslideR: Streamlining slide deck generation for clinical reporting events\nYolanda Zhou (Roche) and Joe Zhu (Roche).\nThe standard process of developing the slide deck for clinical reporting events includes manually populating numbers from static outputs and a separate QC. The process is time-consuming, resource-intensive, and error-prone. To address these issues, we created â€œautoslideRâ€, an R package to automate the slide deck generation for multiple clinical reporting events. autoslideR has successfully supported slide creation for several study endorsement meetings, IMC meetings, as well as dose escalation meetings for early development, saving teams 0.5 to 4 days for the slide deck preparation compared to the time it traditionally takes.\nThis talk explores the key features and technical advantages of autoslideR, such as supporting customized layout creations from existing templates, as well as adding placeholder slides to accelerate final slide preparation.\nhttps://pharmaverse.github.io/examples/digit_files/autoslider.html\n\n\nPutting the â€˜Râ€™ in RWD\nSachin Heerah (Pfizer) and Darren Jeng (Pfizer).\nThe Pfizer Real World Data (RWD) programming team has leveraged R and Posit services to enhance the capabilities of its programmers. We have designed an R package, Shiny apps and even a Quarto website to support all programmers with varying backgrounds, including those with only SAS experience. Our R package is designed to simplify database queries, utilize both R and SAS variable syntax, and standardize deliverables. We have leveraged Positâ€™s RStudio features such as code snippets to make code templates readily accessible for all users within the IDE. Code guides are also presented as snippets to allow all users to load example data and explore standard RWD programming workflows. Overall, embracing and leveraging the features available to us in R and Posit is enhancing our workflow through integrated resources and documentation.\n\n\nGenerating Synthetic Data with synthpop in R\nSophie Furlow (Abbot Diagnostics).\nThis talk will introduce synthetic data as an emerging tool for research and production. We will discuss the differences between synthetic, simulated, and resampled data and cover the current state of the art of synthetic data in healthcare, paying special attention to applications in pharma and diagnostics. We will walk through the basics of synthpop, an R package designed to generate synthetic and anonymous data at the individual level using various machine learning algorithms. Viewers will learn how to create entirely synthetic datasets with minimal statistical distortion their original data, making it suitable for software testing, data sharing, and model training. The talk will end with a demonstration of synthpopâ€™s quality evaluation features and major caveats to consider during the generation process.\n\nEurope/US Session #7\n\n\n\nGenAI in Production - moving beyond prototypes\nDevin Pastoor\nChief Technology and Product Officer, A2-Ai\nCreation of AI applications has become increasingly commoditized through a rich ecosystem of R and python packages. However, there are significant hurdles to take a prototype application into â€œproductionâ€ and keep it operating. Traditional testing and validation approaches do not always apply directly and the flexibility from which users can interact with the application can be much larger than a traditional application. This talk will discuss how to successfully (and unsuccessfully) develop, test, release, and maintain Generative AI based applications in GxP contexts.\n\n\nBuilding the Ultimate R AI Assistant\nPawel Rucki\nPrincipal Data Scientist, Roche\nUsing R for clinical programming can be challenging, particularly with the specialized, often internal, packages used for reporting. To tackle these issues, we built our own AI assistant.\nIn this talk, Iâ€™ll share our story of building a multi-agent R co-pilot with the LangGraph framework, including the lessons we learned and a few tricks we picked up. A key decision was giving each R package its own AI agent. This was a game-changer for getting good help on our internal packages where general-purpose LLMs just canâ€™t keep up.\nIâ€™ll discuss our agent network architecture and the practical techniques used to give agents the right context to write, debug, and explain complex R code for clinical trials. Iâ€™ll wrap up with a live demo showing how our design solves real-world programming problems, speeding up development and leading to better, more reliable code.\n(agents, cursor via mcp, webR extenstion, admiralroche, air AI assistant for programming in R)\n\nEurope/US Session #8\n\n\n\nThe Dependency Whisperer: AI That Sees What You Might Miss\nMing Yan\nEli Lilly\nVina Ro\nSr.Â Clinical Data Analyst, Eli Lilly\nAn ongoing challenge in clinical programming is ensuring that downstream analyses are accurately refreshed following updates to SDTM or ADaM datasets. Traditional tools like AstroGrep can locate references to specific datasets or variables, but they lack the ability to distinguish between active code and commented text. Moreover, identifying indirect dependencies often requires multiple searches and manual effort to document all affected areas, which is time-consuming and increases the risk of missing updatesâ€”potentially leading to incorrect outputs being shared with external parties.\nThis paper introduces an AI-powered tool designed to automatically identify and visualize all datasets and variables impacted by upstream changes. By parsing SDTM, ADaM, and TFL specifications or programs, the tool learns the structure of dependencies across the analysis pipeline. When a user specifies an updated variable and dataset, the tool generates a graphical report highlighting all affected elements. This capability streamlines the refresh process, reduces manual effort, and ensures the accuracy and completeness of deliverables.\n\n\nBayesERtools: R package for exposure-response analysis with Bayesian approaches\nKenta Yoshida\nClinical Pharmacology Modeling & Simulation, Genentech\nExposure-response (ER) analysis is a critical component of clinical drug development for decisions such as dose selection. The new R package, BayesERtools (https://genentech.github.io/BayesERtools/), is designed to make Bayesian ER analysis more accessible. It provides a user-friendly interface for common tasks such as model development, simulation, and plotting, streamlining the entire workflow. The package currently supports linear and Emax models for continuous and binary endpoints. To further support users, we have also developed BayesERbook (https://genentech.github.io/BayesERbook/), a comprehensive online book that documents the workflow with typical examples. This open-source project, based on the Stan ecosystem, aims to expand the use of Bayesian methods in pharma, providing a powerful tool for quantitative decision-making in drug development.\n\n\nAdapting to Regulatory Guidance: Covariate Adjustment and R-enabled Submissions\nAlex Przybylski\nData scientist @ Novartis\nFDA guidance released in 2023 advocates for covariate adjustment as a non-controversial means to enhance the efficiency of statistical analyses. This includes specific recommendations featuring recently proposed methods from academic groups, providing an opportunity for sponsors to realize the practical benefits of innovative approaches. However, they also present challenges for trial teams regarding practical implementation. Bridging the gap between academia, regulation and industry adoption is essential.\nThis talk presents a case study from Novartis, where FDA feedback referencing the new guidance prompted a targeted and strategic response. We will discuss how we responded to this feedback, our preparation for future regulatory expectations, and the role of R in enabling change and impact.\nThe case study will highlight lightweight R package solutions ({beeca}) and the work of the ASA-BIOP Covariate Adjustment Working Group ({RobinCar2}). We will discuss the importance of software that is suited for integration with trial analysis workflows in regulated environments and the benefits of collaborating within the open-source community.\n\n\nR library validation using Acceptance-Test Driven Development\nBrian Repko\nRetired (ex-Novartis Biomedical Research Oncology Data Science), Retired (ex-Novartis)\nEurope/US\nWhile unit tests are a key component of a single R packageâ€™s quality, we need similar ways to test / validate a library of R packages or potentially Shiny applications. This session will walk through frameworks used for Acceptance-Test Driven Development (ATDD) and Behavior Driven Development (BDD) outside of R - cucumber, JBehave, etc. - how they work and their value-add to system quality. In short, tests are written in Quarto markdown, in a â€œgiven-when-thenâ€ format that is regex-matched against annotated functions that drive the testing and asserting of the system under test. The industry can write tests in plain-language and share those tests and feature code not only amongst themselves but also potentially as part of a validation effort with regulators. Feature code can also make use of packages like {chromote} to drive Shiny applications as well. This is bringing my experience as a contributor to JBehave to R and the pharmaverse.\n\n\nWORKSHOP: R Validation Discussion: Metric Repos for Open Quality Assessment\nDoug Kelkhoff\nOver the past year, the R Validation Hub has been hard at work to build out a Metric Repository - a pre-built database of metrics to support the software validation process. Weâ€™d like to invite you to discuss the industry outlook for this open validation database. How do we standardize our compute environments to make metrics useful? What do we do when a package isnâ€™t living up to our standards? What can we do to ensure that you can supplement our database with the validation of in-house packages? Weâ€™d love to share our answers and hear your thoughts to help guide our work. Come join us and discuss the future of R package validation.\nWorkshop related packages:\nriskmetric, riskassessment, riskscore\nWorkshop related links:\nhttps://github.com/pharmaR/regulatory-r-repo-wg\nhttps://github.com/pharmaR/val.meter\nhttps://github.com/pharmaR/val.criterion\n\n\nWORKSHOP: datasetjson - Read and write CDISC Dataset JSON formatted datasets in R and Python\nMichael Stackhouse Chief Innovation Officer, Atorus Sam Hume Research Data Engineer | Open Source | Data Exchange Standards, CDSIC Nick Masel Associate Director Innovation Team Lead, Johnson & Johnson Eli Miller Senior Manager, Cloud Solutions, Atorus What? Join us for an engaging workshop designed to introduce Dataset-JSON, a powerful format for sharing datasets. Weâ€™ll start with an environment setup and explore the motivation behind choosing Dataset-JSON over other formats like Parquet. The session will include a detailed walkthrough of the Dataset-JSON specification, followed by hands-on demonstrations and exercises in both R and Python.\nDiscover how to implement Dataset-JSON in your workflows, learn about upcoming adoption plans, and explore future roadmap and API integrations. This workshop is perfect for data professionals interested in improving dataset interoperability and sharing standards.\nhttps://atorus-research.github.io/datasetjson/\nhttps://atorus-research.github.io/datasetjson_workshop/\n\n\nWORKSHOP: SDTM programming in R using {sdtm.oak} package\nRammprasad Ganapathy\nPrincipal Data Scientist\nThe {sdtm.oak} package is an EDC (Electronic Data Capture) and data standard-agnostic solution designed to empower the pharmaceutical programming community to develop CDISC SDTM datasets using R. By leveraging reusable algorithms, {sdtm. oak} offers a modular programming framework that can potentially automate the creation of SDTM datasets based on standard SDTM specifications. In this workshop, we will cover the fundamentals of the V0.1 {sdtm.oak} package and provide detailed guidance for programmers to begin utilizing it effectively.\nUsers will be given access to an R environment for the session. Users do not need to install R or other tools/packages prior to the session.\nWorkshop related link:\nhttps://pharmaverse.github.io/rinpharma-SDTM-workshop/#/title-slide\n\n\nWORKSHOP: Python for Clinical Study Report and Submission\nNan Xiao\nStatistician at Merck, Merck\nYilong Zhang\nBiostatistician at Meta, Meta\nOpen-source programming languages are rapidly transforming drug discovery, research, and development, offering powerful capabilities for study design, data analysis, visualization, and clinical reporting. This workshop introduces practical strategies for using Python to prepare tables, listings, and figures (TLFs) in a clinical study report (CSR) and to assemble submission-ready electronic Common Technical Document (eCTD) packages that include both source code and deliverables.\nThis workshop is designed for clinical programmers, statisticians, and data scientists who are interested in exploring Python as an alternative approach for clinical trial analysis and reporting. Participants will gain hands-on experience with reproducible workflows, clinical data engineering, and end-to-end project management using the modern Python toolchain. By the end of the session, attendees will have a clear roadmap to start a Python project for clinical trial analysis and reporting.\nThe workshop is based on the open source bookÂ Python for Clinical Study Reports and SubmissionÂ (https://pycsr.org/) and is organized into four modules:\n\nPython environment setup: UseÂ uvÂ to create and manage reproducible Python projects. Develop and collaborate in GitHub Codespaces, Visual Studio Code, or Positron.\nPython packages for clinical reporting: A guided tour of essential packages such asÂ polars,Â plotnine, andÂ rtflite, with demonstrations of creating TLFs commonly used in clinical trials.\nManage a clinical trial A&R project: Practical project structure, conventions, and execution from data to deliverables.\nPreparing eCTD submission packages: An example workflow for assembling submission-ready source code and outputs usingÂ py-pkglite, aligned with eCTD requirements.\n\nhttps://pycsr.org/slides/workshop-slides.html#/datasets\nPublicly available CDISC pilot study data located at the CDISC GitHub repository.\nThe dataset structure follows the CDISC Analysis Data Model (ADAM).\nSource data: https://github.com/elong0527/r4csr/tree/main/data-adam\nConverted parquet data: https://github.com/nanxstats/pycsr/tree/main/data\nWorkshop Slides:\nhttps://pycsr.org/slides/workshop-slides.html#\nhttps://github.com/nanxstats/pycsr\n\n\nWORKSHOP: Supercharge your shiny app by offloading computations to a HPC cluster\nMichael Mayer\nPrincipal Solution Engineer at Posit , Posit PBC\nWith ever increasing complexity of data and analysis, application developers are tempted to put in serious computations into their shiny applications. Given that those shiny applications typically run on infrastructure that has certain resource limitations or sometimes even shared with other shiny apps, more often than not this leads to crashes affecting the overall stability of the system. As a consequence, either other approaches are pursued, or the approach simplified to the point that good science is prevented from happening.Â in this workshop you will learn how to interact with a remote HPC cluster straight from your laptop. You will run a shiny app locally and remote submit pieces of code to the HPC cluster. By leveraging a multiple of your locally available compute power for a short period of time, you will reduceÂ  time-to-result considerably keeping the app fairly interactive. This approach also can be used to connect applications hosted on a Shiny Hosting platform such as Posit Connect to a HPC cluster.Â In the second part of the workshop you will be able to discuss with the workshop instructor(s) your own use cases and get insights on how to address those.\n\n\nWORKSHOP: Bayesian Survival and Multistate Models using R and Stan\nEric Novik\nFounder & CEO @ Generable, Generable\nJacqueline Buros-Novik\nGenerable\nJuho Timonen\nComputational Scientist, Generable\nBayesian inference and Stan offer many advantages for analyzing time-to-event data, including incorporating prior knowledge into the model, propagating all sources of uncertainty to produce well-calibrated predictions, and integrating arbitrary utility functions for optimal decision-making, such as patientsâ€™ preferences for different types of risks. The latter point is particularly relevant in multistate models where people may differ in their preferences towards multiple competing events.In this workshop, we will briefly introduce Bayesian workflow â€“ the typical steps in Bayesian analysis and basic (single-event) survival models in the Bayesian context. We will then proceed to introduce multistate models where we are tracking multiple event types, such as bleeding and stroke in cardiovascular trials, or stable disease, progressive disease, and death in oncology trials. Time permitting, we will demonstrate how to incorporate the patientâ€™s utility function into a decision analysis.\nWorkshop related link:\nhttps://github.com/generable/bmstate\n\n\nWORKSHOP: From Data to Insights: A Hands-On Workshop with {teal} for Clinical Data Exploration\nNina Qi\nPrincipal Data Scientist at Genentech, Roche/Genentech\nDony Unardi\nData Scientist at Genentech, Roche/Genentech\n{teal} is an innovative open-source R-Shiny framework that has transformed how clinical trial data is analyzed and visualized in recent years. By streamlining the creation of interactive web applications, it enables R programmers and data scientists to deliver insights faster while promoting efficiency, transparency, and reproducibility in data exploration.Â This hands-on workshop, built on the latest {teal} 1.0 release, will start from the basics and progressively cover practical topics for building {teal} applications. Together, we will explore key features of {teal} and work through step-by-step exercises designed to build confidence and proficiency in {teal} programming. No prior experience with {teal} or attendance at previous workshops is required - all R users are welcome.Â Designed to deepen participantsâ€™ understanding of the {teal} framework, this session will equip attendees with the skills to leverage the {teal} ecosystem to create scalable, reproducible applications that accelerate insight generation in clinical research.\nWorkshop related link:\nhttps://github.com/pharmaverse/tealworkshop-rinpharma2025/tree/main\n\n\n\nPresentations\n\n[Open-Source Culture and Data Strategy in SHIONOGI: A New Value-Creation Model for Pharma]\n\n[Yoshitake Kitanishi]\n\n\n\n[Strategically Assisting Statistical programmers To succeed in R (SAS2R)]\n\n[KuenHung Lin]\n\n\n\n[Side-by-side by Design: Pharma Data Handling with Merge, Join, Match, and Hash in R]\n\n[Yutaka Morioka]\n[Yuki Nakagawa]\n\n\n\n[Risk Assessment Deep Dive]\n\n[Ryo Nakaya]\n\n\n\n[Leverage template-based automated reporting on DMC materials preparation]\n\n[Nina Han]\n[Peng Zhang]\n\n\n\n[Dynamic Creation of Kaplan-Meier Plots and Summary Measure Tables for Survival Data with R Shiny]\n\n[Takumi Imamura]\n[Takahiro Hasegawa]\n\n\n\n[autoslideR: Streamlining slide deck generation for clinical reporting events]\n\n[Yolanda Zhou]\n[Joe Zhu]\n\n\n\n[From Harmony to Hybrid: Charting a Practical Course for R Adoption in Pharma]\n\n[Joe Zhu]\n\n\n\n[Emerging trend of LLM development in R and implementation]\n\n[Mia Chen]\n[Peng Zhang]\n\n\n\n[An R package to consolidate pretest probability models and guidelines for CAD]\n\n[Jeremy Selva]\n\n\n\n[Enhancing Efficiency in e-CRT Creation for PMDA Through R Shiny App Development Using Vibe Coding]\n\n[Naoki Yoshida]\n\n\n\n[{meRlin} - context-aware AI assistant for clinical programming]\n\n[Steven Brooks]\n[Pietro Mascheroni]\n[Xiecheng Gu]\n\n\n\n[Improving precision healthcare for under-represented and genetically diverse global populations]\n\n[Jimmy Breen]\n\n\n\n[Exploring AI tools in clinical trial data analysis]\n\n[Terry Zhang]\n\n\n\n[An R Package cgmguru for Automated Glycemic Event Detection From Continuous Glucose Monitoring Data]\n\n[Sang Ho Park]\n\n\n\n[Interactive and Reproducible Reports with Quarto]\n\n[Jaspreet Pabla]\n\n\n\n[Using Analysis Results Data using {cards} for PMDA Oncology inqueries]\n\n[Shunsuke Goto]\n\n\n\n[Reach for R Low Hanging Fruit for Faster Results]\n\n[Sunil Gupta]\n\n\n\n[MedxR Package - Bridging Regulatory Drug Data from the FDA and Health Canada into R]\n\n[Renzo CÃ¡ceres Rossi]\n\n\n\n[risk.assessr: extending its use in the package validation process]\n\n[Hugo Bottois]\n\n\n\n[Code Review for Compliance: Best Practices for Validated R Workflows in Pharma]\n\n[Alexandros Kouretsis]\n\n\n\n[A Web-Based R Application for Forecasting Patient Enrollment in Clinical Trials]\n\n[Akifumi Okayama]\n[Motoki Oe]\n[Nobushige Matsuoka]"
  },
  {
    "objectID": "workshops/clinical-reporting/polars-python.html",
    "href": "workshops/clinical-reporting/polars-python.html",
    "title": "Polars: Blazing Fast Python Framework for Clinical Trial Data",
    "section": "",
    "text": "Intermediate Python Performance\nPolars is a cutting-edge Python DataFrame library with high-performance backend and Apache Arrow columnar format for blazingly fast data manipulation. Learn how it accelerates clinical trial workflows from database querying to TFL preparation.\n\n\n\nâš¡ Polars fundamentals - Lightning-fast data operations\nğŸ—ï¸ Apache Arrow - Columnar data format\nğŸ“Š Clinical workflows - CDISC data processing\nğŸ”— Pharmaverse-py integration\nğŸ“ˆ Great Tables - Data presentation",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Polars: Blazing Fast Python Framework for Clinical Trial Data"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/polars-python.html#overview",
    "href": "workshops/clinical-reporting/polars-python.html#overview",
    "title": "Polars: Blazing Fast Python Framework for Clinical Trial Data",
    "section": "",
    "text": "Intermediate Python Performance\nPolars is a cutting-edge Python DataFrame library with high-performance backend and Apache Arrow columnar format for blazingly fast data manipulation. Learn how it accelerates clinical trial workflows from database querying to TFL preparation.\n\n\n\nâš¡ Polars fundamentals - Lightning-fast data operations\nğŸ—ï¸ Apache Arrow - Columnar data format\nğŸ“Š Clinical workflows - CDISC data processing\nğŸ”— Pharmaverse-py integration\nğŸ“ˆ Great Tables - Data presentation",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Polars: Blazing Fast Python Framework for Clinical Trial Data"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/polars-python.html#prerequisites",
    "href": "workshops/clinical-reporting/polars-python.html#prerequisites",
    "title": "Polars: Blazing Fast Python Framework for Clinical Trial Data",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRequired Knowledge:\n\nIntermediate Python\nBasic pandas experience helpful\nUnderstanding of clinical trial data",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Polars: Blazing Fast Python Framework for Clinical Trial Data"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/polars-python.html#key-technologies",
    "href": "workshops/clinical-reporting/polars-python.html#key-technologies",
    "title": "Polars: Blazing Fast Python Framework for Clinical Trial Data",
    "section": "Key Technologies",
    "text": "Key Technologies\n\nPolars\n\n\nApache Arrow\n\n\nGreat Tables\n\n\npharmaverse-py",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Polars: Blazing Fast Python Framework for Clinical Trial Data"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/polars-python.html#workshop-materials",
    "href": "workshops/clinical-reporting/polars-python.html#workshop-materials",
    "title": "Polars: Blazing Fast Python Framework for Clinical Trial Data",
    "section": "Workshop Materials",
    "text": "Workshop Materials\n\n\n\n\n\n\nNoteResources\n\n\n\nGitHub Examples: github.com/machow/examples-great-tables-pharma",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Polars: Blazing Fast Python Framework for Clinical Trial Data"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/polars-python.html#why-polars",
    "href": "workshops/clinical-reporting/polars-python.html#why-polars",
    "title": "Polars: Blazing Fast Python Framework for Clinical Trial Data",
    "section": "Why Polars?",
    "text": "Why Polars?\n\nPerformance Advantages\n\nğŸš€ 10-100x faster than pandas for large datasets\nğŸ’¾ Memory efficient with lazy evaluation\nğŸ”„ Parallel processing out of the box\nğŸ“¦ Apache Arrow native format\n\n\n\nPolars vs Pandas\n# Pandas (traditional)\ndf = pd.read_csv(\"adsl.csv\")\nresult = df[df['AGE'] &gt; 65].groupby('ARM')['AGE'].mean()\n\n# Polars (fast)\nresult = (\n    pl.scan_csv(\"adsl.csv\")\n    .filter(pl.col('AGE') &gt; 65)\n    .group_by('ARM')\n    .agg(pl.col('AGE').mean())\n    .collect()  # Lazy evaluation\n)",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Polars: Blazing Fast Python Framework for Clinical Trial Data"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/polars-python.html#clinical-trial-applications",
    "href": "workshops/clinical-reporting/polars-python.html#clinical-trial-applications",
    "title": "Polars: Blazing Fast Python Framework for Clinical Trial Data",
    "section": "Clinical Trial Applications",
    "text": "Clinical Trial Applications\n\n1. Database Querying\n\nFast SQL-like operations\nEfficient joins across SDTM domains\nLazy evaluation for large queries\n\n\n\n2. Complex Data Wrangling\n\nGrouping and aggregation\nWindow functions for time-series\nPivoting and reshaping\n\n\n\n3. TFL Preparation\n\nData summarization\nCreating analysis datasets\nIntegration with Great Tables",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Polars: Blazing Fast Python Framework for Clinical Trial Data"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/polars-python.html#learning-outcomes",
    "href": "workshops/clinical-reporting/polars-python.html#learning-outcomes",
    "title": "Polars: Blazing Fast Python Framework for Clinical Trial Data",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nâœ… Master Polars DataFrame operations\nâœ… Leverage Apache Arrow for performance\nâœ… Process clinical trial data efficiently\nâœ… Integrate with pharmaverse-py ecosystem\nâœ… Create TFLs with Great Tables\n\n\nSimilar Workshops\n\nPython for CSR and Submission - Complete Python workflow\npointblank: Data Validation - R equivalent for data quality\n\n\n\nRelated Presentations\n\nPolars workshop presentation - Context\n\n\n\nNext Steps\n\nFull Python workflow: Python CSR workshop\nHigh-performance in R: See duckplyr presentation\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "Polars: Blazing Fast Python Framework for Clinical Trial Data"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/pointblank.html",
    "href": "workshops/clinical-reporting/pointblank.html",
    "title": "How to use pointblank to understand, validate, and document your data",
    "section": "",
    "text": "Intermediate Data Quality Validation\nMaster data quality and documentation workflows with {pointblank}. From quick dataset understanding to enterprise-scale validation of 35+ database tables daily.\n\n\n\nğŸ” Quick dataset understanding\nâœ… Data validation with expectation-based rules\nğŸ“ Complete documentation of tables and variables\nğŸ“Š Scaling validation from small to large\nğŸ¯ Beautiful documentation generation",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "How to use pointblank to understand, validate, and document your data"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/pointblank.html#overview",
    "href": "workshops/clinical-reporting/pointblank.html#overview",
    "title": "How to use pointblank to understand, validate, and document your data",
    "section": "",
    "text": "Intermediate Data Quality Validation\nMaster data quality and documentation workflows with {pointblank}. From quick dataset understanding to enterprise-scale validation of 35+ database tables daily.\n\n\n\nğŸ” Quick dataset understanding\nâœ… Data validation with expectation-based rules\nğŸ“ Complete documentation of tables and variables\nğŸ“Š Scaling validation from small to large\nğŸ¯ Beautiful documentation generation",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "How to use pointblank to understand, validate, and document your data"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/pointblank.html#prerequisites",
    "href": "workshops/clinical-reporting/pointblank.html#prerequisites",
    "title": "How to use pointblank to understand, validate, and document your data",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRequired Knowledge:\n\nIntermediate R programming\nBasic understanding of data validation concepts\nFamiliarity with dplyr helpful",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "How to use pointblank to understand, validate, and document your data"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/pointblank.html#key-package",
    "href": "workshops/clinical-reporting/pointblank.html#key-package",
    "title": "How to use pointblank to understand, validate, and document your data",
    "section": "Key Package",
    "text": "Key Package\n\n{pointblank}\n\n\n{dplyr}\n\n\n{DBI}",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "How to use pointblank to understand, validate, and document your data"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/pointblank.html#workshop-materials",
    "href": "workshops/clinical-reporting/pointblank.html#workshop-materials",
    "title": "How to use pointblank to understand, validate, and document your data",
    "section": "Workshop Materials",
    "text": "Workshop Materials\n\n\n\n\n\n\nNoteResources\n\n\n\nGitHub Workshop: github.com/rich-iannone/pointblank-workshop",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "How to use pointblank to understand, validate, and document your data"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/pointblank.html#three-core-workflows",
    "href": "workshops/clinical-reporting/pointblank.html#three-core-workflows",
    "title": "How to use pointblank to understand, validate, and document your data",
    "section": "Three Core Workflows",
    "text": "Three Core Workflows\n\n1. Understanding New Datasets\nQuickly scan and explore unknown data:\nlibrary(pointblank)\n\n# Get comprehensive data overview\nscan_data(my_dataset)\n\n\n2. Validating Data\nCreate validation rules based on expectations:\n# Create validation agent\nagent &lt;- \n  create_agent(\n    tbl = clinical_data,\n    label = \"Clinical Data Validation\"\n  ) %&gt;%\n  # Age should be positive\n  col_vals_gt(vars(AGE), value = 0) %&gt;%\n  # Sex should be M or F\n  col_vals_in_set(vars(SEX), set = c(\"M\", \"F\")) %&gt;%\n  # No missing subject IDs\n  col_vals_not_null(vars(SUBJID)) %&gt;%\n  # Date consistency\n  col_vals_lte(vars(RANDDT), vars(STUDYDT)) %&gt;%\n  interrogate()\n\n# View results\nagent\n\n\n3. Documenting Tables\nCreate informative data dictionaries:\n# Create informant\ninformant &lt;- \n  create_informant(\n    tbl = clinical_data,\n    label = \"ADSL Dataset\"\n  ) %&gt;%\n  info_tabular(\n    Description = \"Analysis dataset for subject-level data\"\n  ) %&gt;%\n  info_columns(\n    columns = \"SUBJID\",\n    `Description` = \"Unique subject identifier\"\n  ) %&gt;%\n  info_columns(\n    columns = \"AGE\",\n    `Description` = \"Age at randomization (years)\",\n    `Valid Range` = \"18-85\"\n  ) %&gt;%\n  incorporate()\n\n# Generate beautiful HTML documentation\ninformant",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "How to use pointblank to understand, validate, and document your data"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/pointblank.html#scaling-validation",
    "href": "workshops/clinical-reporting/pointblank.html#scaling-validation",
    "title": "How to use pointblank to understand, validate, and document your data",
    "section": "Scaling Validation",
    "text": "Scaling Validation\n\nFrom Small to Enterprise\nSmall Problems:\n# Quick check before analysis\nstopifnot_inform(\n  ~ col_vals_not_null(., vars(SUBJID)),\n  ~ col_vals_gt(., vars(AGE), 0)\n)\nEnterprise Scale:\n# Daily validation of 35 database tables\nmultiagent &lt;- \n  create_multiagent(\n    agent_1, agent_2, ..., agent_35\n  )\n\n# Automated email reports\nmultiagent %&gt;%\n  email_blast(\n    to = \"data_quality_team@pharma.com\",\n    when = has_any_sev_issues()\n  )",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "How to use pointblank to understand, validate, and document your data"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/pointblank.html#practical-applications",
    "href": "workshops/clinical-reporting/pointblank.html#practical-applications",
    "title": "How to use pointblank to understand, validate, and document your data",
    "section": "Practical Applications",
    "text": "Practical Applications\n\nClinical Trial Data Validation\n\nSDTM compliance checks\nADaM dataset verification\nCross-domain consistency\nLongitudinal data integrity\n\n\n\nData Documentation\n\nAutomated data dictionaries\nVariable descriptions\nValid ranges and constraints\nChange tracking\n\n\n\nQuality Monitoring\n\nDaily validation pipelines\nAlert systems for issues\nTrend analysis of data quality\nAudit trail generation",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "How to use pointblank to understand, validate, and document your data"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/pointblank.html#example-complete-validation-pipeline",
    "href": "workshops/clinical-reporting/pointblank.html#example-complete-validation-pipeline",
    "title": "How to use pointblank to understand, validate, and document your data",
    "section": "Example: Complete Validation Pipeline",
    "text": "Example: Complete Validation Pipeline\n# Define validation for ADSL\nvalidate_adsl &lt;- function(adsl_data) {\n  create_agent(adsl_data, label = \"ADSL Validation\") %&gt;%\n    # Demographics\n    col_vals_not_null(vars(SUBJID, AGE, SEX)) %&gt;%\n    col_vals_gt(vars(AGE), 18) %&gt;%\n    col_vals_lt(vars(AGE), 90) %&gt;%\n    col_vals_in_set(vars(SEX), c(\"M\", \"F\")) %&gt;%\n    # Dates\n    col_vals_not_null(vars(RANDDT)) %&gt;%\n    col_vals_regex(vars(RANDDT), \"^\\\\d{4}-\\\\d{2}-\\\\d{2}$\") %&gt;%\n    # Treatment\n    col_vals_in_set(vars(ARM), c(\"Placebo\", \"Treatment\")) %&gt;%\n    # Execute\n    interrogate()\n}\n\n# Run daily\nagent &lt;- validate_adsl(read_data(\"adsl.csv\"))\n\n# Check results\nif (has_any_issues(agent)) {\n  send_alert(agent)\n}",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "How to use pointblank to understand, validate, and document your data"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/pointblank.html#learning-outcomes",
    "href": "workshops/clinical-reporting/pointblank.html#learning-outcomes",
    "title": "How to use pointblank to understand, validate, and document your data",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nâœ… Quickly scan and understand new datasets\nâœ… Create robust validation rules\nâœ… Generate beautiful data documentation\nâœ… Scale validation from small to enterprise\nâœ… Automate data quality monitoring\nâœ… Build audit-ready validation pipelines",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "How to use pointblank to understand, validate, and document your data"
    ]
  },
  {
    "objectID": "workshops/clinical-reporting/pointblank.html#integration-with-other-tools",
    "href": "workshops/clinical-reporting/pointblank.html#integration-with-other-tools",
    "title": "How to use pointblank to understand, validate, and document your data",
    "section": "Integration with Other Tools",
    "text": "Integration with Other Tools\n\nDatabases: Works with DBI-compatible connections\nArrow: Validate Parquet files\nShiny: Interactive validation dashboards\nGitHub Actions: Automated validation in CI/CD\n\n\n\nSimilar Workshops\n\nR Validation Discussion - Package validation\nBuilding R Packages - Testing best practices\n\n\n\nRelated Presentations\n\nData Quality discussions\n\n\n\nNext Steps\n\nFor validation: R Validation workshop\nCareer skills: Data Validation expertise\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ“Š Clinical Reporting & Analysis",
      "How to use pointblank to understand, validate, and document your data"
    ]
  },
  {
    "objectID": "workshops/index.html",
    "href": "workshops/index.html",
    "title": "Workshops",
    "section": "",
    "text": "Explore all workshops from the R/Pharma 2025 conference, organized by theme. Each workshop includes detailed descriptions, prerequisites, key learning outcomes, and links to materials.",
    "crumbs": [
      "Workshops"
    ]
  },
  {
    "objectID": "workshops/index.html#ai-large-language-models",
    "href": "workshops/index.html#ai-large-language-models",
    "title": "Workshops",
    "section": "ğŸ¤– AI & Large Language Models",
    "text": "ğŸ¤– AI & Large Language Models\n\n\n\nGetting Started with LLM APIs in R\nSara Altman (Posit PBC)\nBeginner\nLearn to integrate LLMs into R workflows using {ellmer}. Build chatbots, implement tool calling, and design effective system prompts. No AI background required!\nKey Tools: {ellmer}, {shinychat}\n\n\n\nGuided Tour to Building LLM-Based Tooling\nDevin Pastoor, Xu Fei, Aathira Anil Kumar (A2-AI)\nIntermediate\nFrom prototypes to production: Build enterprise AI solutions bridging R and Python. GxP-compliant workflows with AWS Bedrock and MCP servers.\nKey Tools: {ellmer}, AWS Bedrock, MCP\n\n\n\n\nIntegrating LLM with Clinical Data Review\nZhen Wu, Peng Zhang (CIMS Global)\nIntermediate\nBuild {DataChat} - an R Shiny app for conversational clinical data exploration. Emphasis on data privacy, statistical validity, and RAG implementation.\nKey Tools: {ellmer}, {shinychat}, {ragnar}",
    "crumbs": [
      "Workshops"
    ]
  },
  {
    "objectID": "workshops/index.html#clinical-reporting-analysis",
    "href": "workshops/index.html#clinical-reporting-analysis",
    "title": "Workshops",
    "section": "ğŸ“Š Clinical Reporting & Analysis",
    "text": "ğŸ“Š Clinical Reporting & Analysis\n\n\n\nCardinal: Harmonizing Clinical Reporting\nAbinaya Yogasekaram, Emily De La Rua\nBeginner\nHands-on with standardized TLG templates using {gtsummary}. CDISC-aligned outputs for meta-analysis and regulatory submissions.\nKey Tools: {cardinal}, {gtsummary}, {cards}\n\n\n\nAdvanced Clinical Reporting\nDavid Gohel (Ardata)\nIntermediate\nCreate sophisticated Word reports programmatically. Complex tables, ggplot2 integration, and complete CSR generation.\nKey Tools: {officer}, {flextable}\n\n\n\n\n\n\nPolished, Branded Documents with Quarto\nIsabella VelÃ¡squez (Posit)\nBeginner\nCreate websites, PDFs, presentations, and dashboards with consistent branding using brand.yml.\nKey Tools: Quarto, brand.yml\n\n\n\nPolars: Blazing Fast Python Framework\nMichael Chow, Jeroen Janssens (Posit)\nIntermediate\nHigh-performance clinical trial data exploration with Apache Arrow and Great Tables integration.\nKey Tools: Polars, Apache Arrow, Great Tables\n\n\n\n\npointblank: Understand, Validate, Document Data\nRich Iannone (Posit)\nIntermediate\nData quality workflows that scale from quick checks to validating 35+ database tables daily. Beautiful automated documentation.\nKey Tools: {pointblank}",
    "crumbs": [
      "Workshops"
    ]
  },
  {
    "objectID": "workshops/index.html#development-validation",
    "href": "workshops/index.html#development-validation",
    "title": "Workshops",
    "section": "ğŸ”§ Development & Validation",
    "text": "ğŸ”§ Development & Validation\n\n\n\nIntroduction to Building R Packages\nNicola Rennie\nBeginner\nTransform scripts into packages! Learn structure, documentation, testing, and sharing. Youâ€™ll build your own package!\nKey Tools: {devtools}, {usethis}, {testthat}\n\n\n\nR Validation Discussion\nDoug Kelkhoff (R Validation Hub)\nAdvanced\nDiscussion on Metric Repository for open quality assessment. Standardization and industry adoption strategies.\nKey Tools: {riskmetric}, {riskassessment}\n\n\n\n\ndatasetjson: Read and Write CDISC Dataset JSON\nMichael Stackhouse, Sam Hume, Nick Masel, Eli Miller\nIntermediate\nWork with Dataset-JSON format in R and Python. Modern data sharing for pharma workflows.\nKey Tools: {datasetjson}, Python datasetjson",
    "crumbs": [
      "Workshops"
    ]
  },
  {
    "objectID": "workshops/index.html#statistical-methods-modeling",
    "href": "workshops/index.html#statistical-methods-modeling",
    "title": "Workshops",
    "section": "ğŸ“ˆ Statistical Methods & Modeling",
    "text": "ğŸ“ˆ Statistical Methods & Modeling\n\n\n\nFlexible Trial Design with rpact\nDaniel Sabanes Bove, Friedrich Pahlke\nIntermediate\nClinical trial planning and simulation. Group sequential, adaptive designs, and p-value combination tests.\nKey Tools: {rpact}, RPACT Cloud\n\n\n\nDebugging Stan Programs\nDaniel Lee\nAdvanced\nPractical strategies for debugging Stan models. From non-identifiable parameters to runtime performance.\nKey Tools: Stan, CmdStan\n\n\n\n\n\n\nR Classification with tidymodels\nHarshavardhan Bajoria\nBeginner\nMachine learning classification using the tidymodels framework. Preprocessing, training, and evaluation.\nKey Tools: {tidymodels}, {recipes}, {parsnip}\n\n\n\nBayesian Survival & Multistate Models\nEric Novik, Jacqueline Buros-Novik, Juho Timonen (Generable)\nAdvanced\nBayesian time-to-event analysis with Stan. Multistate models for competing risks in clinical trials.\nKey Tools: Stan, {bmstate}",
    "crumbs": [
      "Workshops"
    ]
  },
  {
    "objectID": "workshops/index.html#specialized-applications",
    "href": "workshops/index.html#specialized-applications",
    "title": "Workshops",
    "section": "ğŸ§¬ Specialized Applications",
    "text": "ğŸ§¬ Specialized Applications\n\n\n\nSDTM Programming with {sdtm.oak}\nRammprasad Ganapathy\nIntermediate\nEDC-agnostic SDTM dataset creation. Modular programming framework with reusable algorithms.\nKey Tools: {sdtm.oak}\n\n\n\nPython for Clinical Study Report\nNan Xiao, Yilong Zhang\nIntermediate\nPython workflows for TFLs and eCTD packages. Modern toolchain with uv, polars, and plotnine.\nKey Tools: Python, polars, plotnine, rtflite\n\n\n\n\n\n\nSupercharge Shiny with HPC Cluster\nMichael Mayer (Posit)\nAdvanced\nOffload computations to HPC clusters from Shiny apps. Remote job submission and resource management.\nKey Tools: Shiny, HPC, Posit Connect\n\n\n\nFrom Data to Insights with {teal}\nNina Qi, Dony Unardi (Genentech)\nIntermediate\nInteractive clinical trial data exploration with the {teal} 1.0 framework. Build scalable, reproducible apps.\nKey Tools: {teal}, Shiny",
    "crumbs": [
      "Workshops"
    ]
  },
  {
    "objectID": "workshops/index.html#workshop-statistics",
    "href": "workshops/index.html#workshop-statistics",
    "title": "Workshops",
    "section": "ğŸ“‹ Workshop Statistics",
    "text": "ğŸ“‹ Workshop Statistics\n\n\n\n3\nAI/LLM\n\n\n5\nClinical Reporting\n\n\n3\nDevelopment\n\n\n8\nStatistical/Specialized",
    "crumbs": [
      "Workshops"
    ]
  },
  {
    "objectID": "workshops/index.html#find-your-workshop",
    "href": "workshops/index.html#find-your-workshop",
    "title": "Workshops",
    "section": "ğŸ¯ Find Your Workshop",
    "text": "ğŸ¯ Find Your Workshop\nBy Experience Level:\n\nğŸŸ¢ Beginner (5 workshops) - No prior specialized knowledge required\nğŸŸ¡ Intermediate (10 workshops) - Some experience with topic area\nğŸ”´ Advanced (4 workshops) - Significant experience recommended\n\nBy Topic:\n\nLooking for AI/ML? â†’ Start with Getting Started with LLM APIs\nNeed clinical reporting? â†’ Try Cardinal TLGs or officer/flextable\nWant to build packages? â†’ Building R Packages\nInterested in Bayesian methods? â†’ Bayesian Survival Models\n\nPopular Combinations:\n\nAI Pathway: Getting Started â†’ Guided Tour â†’ Clinical Data Privacy\nReporting Pathway: Cardinal â†’ officer/flextable â†’ Quarto\nDeveloper Pathway: Building Packages â†’ Validation Discussion\n\n\n\n\n\n\n\n\nTipPro Tip\n\n\n\nMany workshops have associated GitHub repositories with code examples and exercises. Look for the â€œResourcesâ€ section in each workshop page for direct links!\n\n\n\nAll workshops from R/Pharma 2025 Conference | Last updated: November 2025",
    "crumbs": [
      "Workshops"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/bayesian-survival.html",
    "href": "workshops/statistical-methods/bayesian-survival.html",
    "title": "Bayesian Survival and Multistate Models using R and Stan",
    "section": "",
    "text": "Advanced Bayesian Survival Analysis\nBayesian inference and Stan offer many advantages for time-to-event data: incorporating prior knowledge, propagating uncertainty, and integrating patient utility functions for optimal decision-making. Learn multistate models for competing events in cardiovascular and oncology trials.\n\n\n\nğŸ”„ Bayesian workflow - Typical analysis steps\nâ±ï¸ Survival models - Single and competing events\nğŸ”€ Multistate models - Bleeding, stroke, death pathways\nğŸ¯ Patient preferences - Utility function integration\nğŸ“Š Decision analysis - Optimal treatment selection",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Bayesian Survival and Multistate Models using R and Stan"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/bayesian-survival.html#overview",
    "href": "workshops/statistical-methods/bayesian-survival.html#overview",
    "title": "Bayesian Survival and Multistate Models using R and Stan",
    "section": "",
    "text": "Advanced Bayesian Survival Analysis\nBayesian inference and Stan offer many advantages for time-to-event data: incorporating prior knowledge, propagating uncertainty, and integrating patient utility functions for optimal decision-making. Learn multistate models for competing events in cardiovascular and oncology trials.\n\n\n\nğŸ”„ Bayesian workflow - Typical analysis steps\nâ±ï¸ Survival models - Single and competing events\nğŸ”€ Multistate models - Bleeding, stroke, death pathways\nğŸ¯ Patient preferences - Utility function integration\nğŸ“Š Decision analysis - Optimal treatment selection",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Bayesian Survival and Multistate Models using R and Stan"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/bayesian-survival.html#prerequisites",
    "href": "workshops/statistical-methods/bayesian-survival.html#prerequisites",
    "title": "Bayesian Survival and Multistate Models using R and Stan",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRequired Knowledge:\n\nIntermediate R programming\nBasic survival analysis concepts\nSome Bayesian exposure helpful\n\nRecommended:\n\nStan familiarity (not required)\nClinical trial experience",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Bayesian Survival and Multistate Models using R and Stan"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/bayesian-survival.html#key-tools",
    "href": "workshops/statistical-methods/bayesian-survival.html#key-tools",
    "title": "Bayesian Survival and Multistate Models using R and Stan",
    "section": "Key Tools",
    "text": "Key Tools\n\nStan\n\n\n{bmstate}\n\n\n{rstan}\n\n\n{cmdstanr}",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Bayesian Survival and Multistate Models using R and Stan"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/bayesian-survival.html#workshop-materials",
    "href": "workshops/statistical-methods/bayesian-survival.html#workshop-materials",
    "title": "Bayesian Survival and Multistate Models using R and Stan",
    "section": "Workshop Materials",
    "text": "Workshop Materials\n\n\n\n\n\n\nNoteResources\n\n\n\nGitHub Package: github.com/generable/bmstate",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Bayesian Survival and Multistate Models using R and Stan"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/bayesian-survival.html#why-bayesian-for-survival-analysis",
    "href": "workshops/statistical-methods/bayesian-survival.html#why-bayesian-for-survival-analysis",
    "title": "Bayesian Survival and Multistate Models using R and Stan",
    "section": "Why Bayesian for Survival Analysis?",
    "text": "Why Bayesian for Survival Analysis?\n\nAdvantages\n1. Prior Information Integration\n\nUse historical trial data\nExpert knowledge incorporation\nBorrowing strength across studies\n\n2. Uncertainty Quantification\n\nFull posterior distributions\nCredible intervals\nProbability statements\n\n3. Patient Preferences\n\nUtility function integration\nTrade-offs between outcomes\nPersonalized decision-making\n\n4. Complex Models\n\nMultistate transitions\nCompeting risks\nTime-varying effects",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Bayesian Survival and Multistate Models using R and Stan"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/bayesian-survival.html#bayesian-workflow",
    "href": "workshops/statistical-methods/bayesian-survival.html#bayesian-workflow",
    "title": "Bayesian Survival and Multistate Models using R and Stan",
    "section": "Bayesian Workflow",
    "text": "Bayesian Workflow\n\nStep 1: Prior Specification\nparameters {\n  real log_baseline_hazard;\n  real treatment_effect;\n}\n\nmodel {\n  // Priors\n  log_baseline_hazard ~ normal(-2, 1);\n  treatment_effect ~ normal(0, 0.5);\n}\n\n\nStep 2: Prior Predictive Checks\nSimulate data from priors to verify reasonableness.\n\n\nStep 3: Fit Model\nlibrary(bmstate)\n\nfit &lt;- fit_survival_model(\n  formula = Surv(time, status) ~ treatment,\n  data = clinical_data,\n  prior = prior_spec\n)\n\n\nStep 4: Posterior Diagnostics\n\nCheck convergence (R-hat)\nEffective sample size\nTrace plots\nPosterior predictive checks\n\n\n\nStep 5: Inference\nExtract posterior summaries and make decisions.",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Bayesian Survival and Multistate Models using R and Stan"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/bayesian-survival.html#single-event-survival-models",
    "href": "workshops/statistical-methods/bayesian-survival.html#single-event-survival-models",
    "title": "Bayesian Survival and Multistate Models using R and Stan",
    "section": "Single-Event Survival Models",
    "text": "Single-Event Survival Models\n\nExponential Model\nConstant hazard over time.\n\n\nWeibull Model\nFlexible hazard shape (increasing/decreasing).\n\n\nPiecewise Exponential\nDifferent hazards in time intervals.\n\n\nCox-Like Models\nSemi-parametric baseline with covariates.",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Bayesian Survival and Multistate Models using R and Stan"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/bayesian-survival.html#multistate-models",
    "href": "workshops/statistical-methods/bayesian-survival.html#multistate-models",
    "title": "Bayesian Survival and Multistate Models using R and Stan",
    "section": "Multistate Models",
    "text": "Multistate Models\n\nExample: Cardiovascular Trial\nStates:\n\nHealthy\nBleeding event\nStroke event\nDeath\n\nTransitions:\n\nHealthy â†’ Bleeding\nHealthy â†’ Stroke\nHealthy â†’ Death\nBleeding â†’ Death\nStroke â†’ Death\n\n\n\nOncology Trial\nStates:\n\nStable disease\nProgressive disease\nDeath\n\nAllows:\n\nDifferent treatment effects on each transition\nCompeting risks modeling\nIllness-death models",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Bayesian Survival and Multistate Models using R and Stan"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/bayesian-survival.html#patient-utility-functions",
    "href": "workshops/statistical-methods/bayesian-survival.html#patient-utility-functions",
    "title": "Bayesian Survival and Multistate Models using R and Stan",
    "section": "Patient Utility Functions",
    "text": "Patient Utility Functions\n\nIncorporating Preferences\nPatients may value outcomes differently:\n\nPrefer minor bleeding over stroke\nTrade survival time for quality\nDifferent risk tolerances\n\n\n\nExample Utility\n# Patient preferences\nutility &lt;- function(state, time) {\n  switch(state,\n    \"healthy\" = 1.0,\n    \"bleeding\" = 0.8,  # 20% quality loss\n    \"stroke\" = 0.3,    # 70% quality loss\n    \"death\" = 0.0\n  )\n}\n\n\nDecision Analysis\nUse utilities to:\n\nCompare treatment strategies\nCalculate quality-adjusted survival\nOptimize individual decisions",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Bayesian Survival and Multistate Models using R and Stan"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/bayesian-survival.html#workshop-exercises",
    "href": "workshops/statistical-methods/bayesian-survival.html#workshop-exercises",
    "title": "Bayesian Survival and Multistate Models using R and Stan",
    "section": "Workshop Exercises",
    "text": "Workshop Exercises\n\nExercise 1: Single-Event Model\nFit Weibull survival model to trial data.\n\n\nExercise 2: Multistate Model\nCardiovascular trial with bleeding/stroke competing events.\n\n\nExercise 3: Utility Integration\nIncorporate patient preferences into decision analysis.",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Bayesian Survival and Multistate Models using R and Stan"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/bayesian-survival.html#practical-example",
    "href": "workshops/statistical-methods/bayesian-survival.html#practical-example",
    "title": "Bayesian Survival and Multistate Models using R and Stan",
    "section": "Practical Example",
    "text": "Practical Example\nlibrary(bmstate)\n\n# Fit multistate model\nfit &lt;- fit_multistate(\n  formula = list(\n    bleeding ~ treatment + age,\n    stroke ~ treatment + age + prior_stroke,\n    death ~ treatment + age\n  ),\n  data = cardio_data,\n  transitions = transition_matrix\n)\n\n# Extract hazard ratios\nhr_bleeding &lt;- exp(posterior_summary(fit, \"treatment_bleeding\"))\nhr_stroke &lt;- exp(posterior_summary(fit, \"treatment_stroke\"))\n\n# Decision analysis with utilities\nqaly &lt;- calculate_qaly(\n  fit = fit,\n  utilities = patient_utilities,\n  horizon = 5  # years\n)",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Bayesian Survival and Multistate Models using R and Stan"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/bayesian-survival.html#advantages-in-pharma",
    "href": "workshops/statistical-methods/bayesian-survival.html#advantages-in-pharma",
    "title": "Bayesian Survival and Multistate Models using R and Stan",
    "section": "Advantages in Pharma",
    "text": "Advantages in Pharma\n\nRegulatory\n\nExplicit uncertainty quantification\nTransparent assumptions (priors)\nFlexible for complex designs\n\n\n\nScientific\n\nAccumulate knowledge across trials\nHandle small samples better\nNatural borrowing of information\n\n\n\nPatient-Centered\n\nIndividual risk-benefit\nPreference integration\nPersonalized medicine",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Bayesian Survival and Multistate Models using R and Stan"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/bayesian-survival.html#learning-outcomes",
    "href": "workshops/statistical-methods/bayesian-survival.html#learning-outcomes",
    "title": "Bayesian Survival and Multistate Models using R and Stan",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nâœ… Understand Bayesian workflow for survival data\nâœ… Fit single-event survival models in Stan\nâœ… Build multistate models for competing risks\nâœ… Integrate patient utility functions\nâœ… Perform Bayesian decision analysis\nâœ… Interpret and communicate Bayesian results",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Bayesian Survival and Multistate Models using R and Stan"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/bayesian-survival.html#advanced-topics-time-permitting",
    "href": "workshops/statistical-methods/bayesian-survival.html#advanced-topics-time-permitting",
    "title": "Bayesian Survival and Multistate Models using R and Stan",
    "section": "Advanced Topics (Time Permitting)",
    "text": "Advanced Topics (Time Permitting)\n\nCure models\nRecurrent events\nJoint models (longitudinal + survival)\nHierarchical models",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Bayesian Survival and Multistate Models using R and Stan"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/bayesian-survival.html#resources-1",
    "href": "workshops/statistical-methods/bayesian-survival.html#resources-1",
    "title": "Bayesian Survival and Multistate Models using R and Stan",
    "section": "Resources",
    "text": "Resources\n\n{bmstate} documentation\nBayesian Survival Analysis (Ibrahim et al.)\nStan Userâ€™s Guide - Survival chapter\nStatistical Rethinking (McElreath)\n\n\n\nSimilar Workshops\n\nDebugging Stan - Stan troubleshooting skills\nrpact Trial Design - Frequentist designs\n\n\n\nRelated Presentations\n\nBayesERtools - Exposure-response with Stan\n\n\n\nNext Steps\n\nPrerequisites: Debugging Stan workshop highly recommended\nCareer value: Bayesian expertise opens doors\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Bayesian Survival and Multistate Models using R and Stan"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/rpact-trial-design.html",
    "href": "workshops/statistical-methods/rpact-trial-design.html",
    "title": "Flexible Clinical Trial Design, Simulation, and Analysis with rpact",
    "section": "",
    "text": "Intermediate Trial Design Statistics\nExplore the capabilities of {rpact} - a comprehensive, validated, open-source R package for clinical trial planning, design simulation, and data analysis. Under continuous development since 2017 with extensive documentation.\n\n\n\nğŸ“Š Trial design basics - Group sequential and adaptive designs\nğŸ² Simulation - Design characteristics assessment\nğŸ”¬ P-value combination tests\nğŸ“ˆ Adaptive designs - Multi-armed and enrichment\nğŸ’» RPACT Cloud - User-friendly platform",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Flexible Clinical Trial Design, Simulation, and Analysis with rpact"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/rpact-trial-design.html#overview",
    "href": "workshops/statistical-methods/rpact-trial-design.html#overview",
    "title": "Flexible Clinical Trial Design, Simulation, and Analysis with rpact",
    "section": "",
    "text": "Intermediate Trial Design Statistics\nExplore the capabilities of {rpact} - a comprehensive, validated, open-source R package for clinical trial planning, design simulation, and data analysis. Under continuous development since 2017 with extensive documentation.\n\n\n\nğŸ“Š Trial design basics - Group sequential and adaptive designs\nğŸ² Simulation - Design characteristics assessment\nğŸ”¬ P-value combination tests\nğŸ“ˆ Adaptive designs - Multi-armed and enrichment\nğŸ’» RPACT Cloud - User-friendly platform",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Flexible Clinical Trial Design, Simulation, and Analysis with rpact"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/rpact-trial-design.html#prerequisites",
    "href": "workshops/statistical-methods/rpact-trial-design.html#prerequisites",
    "title": "Flexible Clinical Trial Design, Simulation, and Analysis with rpact",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRequired Knowledge:\n\nIntermediate statistics\nClinical trial basics\nR programming fundamentals\n\nRecommended:\n\nExperience with trial design\nKnowledge of group sequential methods",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Flexible Clinical Trial Design, Simulation, and Analysis with rpact"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/rpact-trial-design.html#key-tools",
    "href": "workshops/statistical-methods/rpact-trial-design.html#key-tools",
    "title": "Flexible Clinical Trial Design, Simulation, and Analysis with rpact",
    "section": "Key Tools",
    "text": "Key Tools\n\n{rpact}\n\n\nRPACT Cloud",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Flexible Clinical Trial Design, Simulation, and Analysis with rpact"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/rpact-trial-design.html#workshop-materials",
    "href": "workshops/statistical-methods/rpact-trial-design.html#workshop-materials",
    "title": "Flexible Clinical Trial Design, Simulation, and Analysis with rpact",
    "section": "Workshop Materials",
    "text": "Workshop Materials\n\n\n\n\n\n\nNoteResources\n\n\n\nPresentation: rpharma.presentation.2025.rpact.com\nDocumentation: www.rpact.org\nRPACT Cloud (Free): cloud.rpact.com",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Flexible Clinical Trial Design, Simulation, and Analysis with rpact"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/rpact-trial-design.html#package-features",
    "href": "workshops/statistical-methods/rpact-trial-design.html#package-features",
    "title": "Flexible Clinical Trial Design, Simulation, and Analysis with rpact",
    "section": "Package Features",
    "text": "Package Features\n\nDesign Types\nGroup Sequential Designs:\n\nMultiple interim analyses\nEarly stopping rules\nAlpha spending functions\nFutility boundaries\n\nAdaptive Designs:\n\nSample size reassessment\nTreatment selection\nPopulation enrichment\nCombination tests\n\n\n\nData Types Supported\n\nâœ… Continuous endpoints\nâœ… Binary outcomes\nâœ… Survival data\nâœ… Count data",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Flexible Clinical Trial Design, Simulation, and Analysis with rpact"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/rpact-trial-design.html#rpact-cloud",
    "href": "workshops/statistical-methods/rpact-trial-design.html#rpact-cloud",
    "title": "Flexible Clinical Trial Design, Simulation, and Analysis with rpact",
    "section": "RPACT Cloud",
    "text": "RPACT Cloud\nUser-friendly web platform for trial design:\nFeatures:\n\nNo coding required\nInteractive design exploration\nAutomatic documentation\nExport to R code\nFree version available\n\nUse Cases:\n\nQuick design evaluation\nStakeholder presentations\nTeaching and training\nProposal preparation",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Flexible Clinical Trial Design, Simulation, and Analysis with rpact"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/rpact-trial-design.html#practical-applications",
    "href": "workshops/statistical-methods/rpact-trial-design.html#practical-applications",
    "title": "Flexible Clinical Trial Design, Simulation, and Analysis with rpact",
    "section": "Practical Applications",
    "text": "Practical Applications\n\nSample Size Calculation\nlibrary(rpact)\n\n# Group sequential design\ndesign &lt;- getDesignGroupSequential(\n  kMax = 3,  # 3 interim analyses\n  alpha = 0.025,\n  beta = 0.2,\n  typeOfDesign = \"asOF\"  # O'Brien-Fleming\n)\n\n# Calculate sample size\nsampleSize &lt;- getSampleSizeMeans(\n  design = design,\n  meanRatio = 1,\n  normalApproximation = FALSE,\n  alternative = 0.5,  # Treatment effect\n  stDev = 1\n)\n\n\nSimulation\n# Simulate adaptive design\nsimulation &lt;- getSimulationMultiArmMeans(\n  design = design,\n  activeArms = 3,\n  plannedSubjects = c(50, 100, 150),\n  meanRatio = 1,\n  stDev = 1,\n  maxNumberOfIterations = 10000\n)",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Flexible Clinical Trial Design, Simulation, and Analysis with rpact"
    ]
  },
  {
    "objectID": "workshops/statistical-methods/rpact-trial-design.html#learning-outcomes",
    "href": "workshops/statistical-methods/rpact-trial-design.html#learning-outcomes",
    "title": "Flexible Clinical Trial Design, Simulation, and Analysis with rpact",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nâœ… Design group sequential trials\nâœ… Implement adaptive designs\nâœ… Simulate trial characteristics\nâœ… Use RPACT Cloud platform\nâœ… Apply {rpact} to real studies\n\n\nSimilar Workshops\n\nBayesian Survival Models - Alternative statistical approach\nDebugging Stan - For Bayesian designs\n\n\n\nNext Steps\n\nFor Bayesian designs: Try Bayesian Survival workshop\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ“ˆ Statistical Methods & Modeling",
      "Flexible Clinical Trial Design, Simulation, and Analysis with rpact"
    ]
  },
  {
    "objectID": "workshops/ai-llm/guided-tour-llm-tooling.html",
    "href": "workshops/ai-llm/guided-tour-llm-tooling.html",
    "title": "A Guided Tour to Building and Integrating LLM Based Tooling with R",
    "section": "",
    "text": "Intermediate AI/LLM Enterprise GxP\nA practical, 2-hour workshop demonstrating how to integrate Generative AI (GenAI) into pharmaceutical workflows. This session focuses on bridging the R and Python ecosystems to deliver scalable, GxP-compliant solutions.\n\n\n\nğŸ—ï¸ Architecture patterns for LLM-enabled applications\nğŸ” Enterprise integration with AWS Bedrock and internal systems\nğŸ¤– MCP servers for reproducible analytics\nâœ… GxP compliance in AI application deployment\nğŸ”— R-Python interoperability for GenAI workflows",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "A Guided Tour to Building and Integrating LLM Based Tooling with R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/guided-tour-llm-tooling.html#overview",
    "href": "workshops/ai-llm/guided-tour-llm-tooling.html#overview",
    "title": "A Guided Tour to Building and Integrating LLM Based Tooling with R",
    "section": "",
    "text": "Intermediate AI/LLM Enterprise GxP\nA practical, 2-hour workshop demonstrating how to integrate Generative AI (GenAI) into pharmaceutical workflows. This session focuses on bridging the R and Python ecosystems to deliver scalable, GxP-compliant solutions.\n\n\n\nğŸ—ï¸ Architecture patterns for LLM-enabled applications\nğŸ” Enterprise integration with AWS Bedrock and internal systems\nğŸ¤– MCP servers for reproducible analytics\nâœ… GxP compliance in AI application deployment\nğŸ”— R-Python interoperability for GenAI workflows",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "A Guided Tour to Building and Integrating LLM Based Tooling with R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/guided-tour-llm-tooling.html#prerequisites",
    "href": "workshops/ai-llm/guided-tour-llm-tooling.html#prerequisites",
    "title": "A Guided Tour to Building and Integrating LLM Based Tooling with R",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRequired Knowledge:\n\nIntermediate R programming\nBasic understanding of APIs\nFamiliarity with clinical trial workflows\n\nRecommended:\n\nExperience with Python (helpful but not required)\nKnowledge of cloud services (AWS)\nUnderstanding of GxP requirements",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "A Guided Tour to Building and Integrating LLM Based Tooling with R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/guided-tour-llm-tooling.html#key-technologies",
    "href": "workshops/ai-llm/guided-tour-llm-tooling.html#key-technologies",
    "title": "A Guided Tour to Building and Integrating LLM Based Tooling with R",
    "section": "Key Technologies",
    "text": "Key Technologies\n\n{ellmer}\n\n\n{mcpr}\n\n\nAWS Bedrock\n\n\nPython\n\n\nLangChain\n\n\nMCP (Model Context Protocol)",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "A Guided Tour to Building and Integrating LLM Based Tooling with R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/guided-tour-llm-tooling.html#workshop-content",
    "href": "workshops/ai-llm/guided-tour-llm-tooling.html#workshop-content",
    "title": "A Guided Tour to Building and Integrating LLM Based Tooling with R",
    "section": "Workshop Content",
    "text": "Workshop Content\n\n1. From Prototype to Production\nThe Reality Check:\n\nWhy most AI prototypes fail in production\nCommon pitfalls in enterprise AI deployment\nTesting and validation approaches for GenAI\nMaintaining AI applications over time\n\n\n\n2. Architecture Patterns\nBuilding Scalable AI Systems:\n# Example: MCP Server for Clinical Data\nlibrary(mcpr)\n\n# Define a clinical data tool\nclinical_server &lt;- mcp_server() %&gt;%\n  add_tool(\n    name = \"query_adverse_events\",\n    description = \"Query adverse events from clinical database\",\n    parameters = list(\n      study_id = \"string\",\n      severity = \"string\"\n    ),\n    handler = function(study_id, severity) {\n      # Connect to database and query\n      query_clinical_db(study_id, severity)\n    }\n  )\n\n\n3. Real-World Applications\n\nA. Interactive Chatbots for Clinical Study Reporting\n\nConversational interfaces for study data exploration\nNatural language queries on CDISC datasets\nAutomated report generation from templates\n\n\n\nB. SOP Management Systems\n\nDocument retrieval and summarization\nCompliance checking against SOPs\nVersion control and change tracking\n\n\n\nC. MCP Server Implementations\n\nReproducible analytics workflows\nTool registration and management\nCross-language interoperability (R â†”ï¸ Python)\n\n\n\n\n4. AWS Bedrock Integration\nEnterprise LLM Deployment:\n\nModel selection and configuration\nSecurity and access control\nCost optimization strategies\nMonitoring and logging\n\n# Python example: AWS Bedrock with LangChain\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(\n    model_id=\"anthropic.claude-3-sonnet\",\n    region_name=\"us-east-1\"\n)\n\n\n5. GxP Compliance Strategies\nMaking AI Production-Ready:\n\nâœ… Validation approaches for LLM applications\nğŸ“ Documentation requirements\nğŸ” Audit trails and logging\nğŸ§ª Testing strategies (unit, integration, UAT)\nğŸ“Š Performance monitoring\n\n\n\n6. Navigating IT Constraints\nReal-World Enterprise Challenges:\n\nAir-gapped environments\nData privacy requirements\nIntegration with legacy systems\nRegulatory approval processes",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "A Guided Tour to Building and Integrating LLM Based Tooling with R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/guided-tour-llm-tooling.html#hands-on-exercises",
    "href": "workshops/ai-llm/guided-tour-llm-tooling.html#hands-on-exercises",
    "title": "A Guided Tour to Building and Integrating LLM Based Tooling with R",
    "section": "Hands-On Exercises",
    "text": "Hands-On Exercises\n\nExercise 1: Build a Clinical Data Chatbot\nCreate an interactive chatbot that can:\n\nQuery SDTM/ADaM datasets\nGenerate summary statistics\nCreate basic visualizations\nAnswer questions about study design\n\n\n\nExercise 2: Implement an MCP Server\nBuild a reusable MCP server for:\n\nData validation\nStatistical computations\nReport generation\n\n\n\nExercise 3: AWS Bedrock Integration\nConnect to AWS Bedrock and:\n\nConfigure Claude for pharma-specific tasks\nImplement rate limiting and error handling\nAdd logging for audit purposes",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "A Guided Tour to Building and Integrating LLM Based Tooling with R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/guided-tour-llm-tooling.html#practical-applications-in-pharma",
    "href": "workshops/ai-llm/guided-tour-llm-tooling.html#practical-applications-in-pharma",
    "title": "A Guided Tour to Building and Integrating LLM Based Tooling with R",
    "section": "Practical Applications in Pharma",
    "text": "Practical Applications in Pharma\n\nClinical Study Reporting\n\nAutomated CSR generation\nTable/Listing/Figure creation from natural language\nCross-referencing and consistency checking\n\n\n\nRegulatory Submissions\n\nDocument preparation assistance\nCompliance verification\nResponse to regulatory queries\n\n\n\nData Analysis\n\nExploratory data analysis via natural language\nStatistical model selection guidance\nResults interpretation and explanation",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "A Guided Tour to Building and Integrating LLM Based Tooling with R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/guided-tour-llm-tooling.html#workshop-instructors",
    "href": "workshops/ai-llm/guided-tour-llm-tooling.html#workshop-instructors",
    "title": "A Guided Tour to Building and Integrating LLM Based Tooling with R",
    "section": "Workshop Instructors",
    "text": "Workshop Instructors\n\nDevin Pastoor - Chief Technology and Product Officer at A2-AI, expert in GxP-compliant AI systems and pharmaceutical DevOps.\nXu Fei - Senior Solutions Engineer at A2-AI, specializes in LLM-enabled applications across R and Python stacks, with focus on enterprise DevOps and cloud APIs.\nAathira Anil Kumar - Engineer at A2-AI, extensive experience with life-science organizations and GenAI integration.",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "A Guided Tour to Building and Integrating LLM Based Tooling with R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/guided-tour-llm-tooling.html#tools-frameworks-covered",
    "href": "workshops/ai-llm/guided-tour-llm-tooling.html#tools-frameworks-covered",
    "title": "A Guided Tour to Building and Integrating LLM Based Tooling with R",
    "section": "Tools & Frameworks Covered",
    "text": "Tools & Frameworks Covered\n\nR Ecosystem\n\n{ellmer} - LLM integration\n{mcpr} - Model Context Protocol\n{shinychat} - Chatbot interfaces\n\n\n\nPython Ecosystem\n\nLangChain - LLM application framework\nLangGraph - Multi-agent orchestration\nAWS SDK - Cloud integration\n\n\n\nInfrastructure\n\nAWS Bedrock - Managed LLM service\nDocker - Containerization\nGitHub Actions - CI/CD",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "A Guided Tour to Building and Integrating LLM Based Tooling with R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/guided-tour-llm-tooling.html#learning-outcomes",
    "href": "workshops/ai-llm/guided-tour-llm-tooling.html#learning-outcomes",
    "title": "A Guided Tour to Building and Integrating LLM Based Tooling with R",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of this workshop, you will be able to:\nâœ… Design architecture for production GenAI applications\nâœ… Integrate LLMs with enterprise pharmaceutical systems\nâœ… Implement GxP-compliant AI workflows\nâœ… Build MCP servers for reproducible analytics\nâœ… Navigate IT constraints in regulated environments\nâœ… Bridge R and Python ecosystems for AI solutions",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "A Guided Tour to Building and Integrating LLM Based Tooling with R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/guided-tour-llm-tooling.html#real-world-case-studies",
    "href": "workshops/ai-llm/guided-tour-llm-tooling.html#real-world-case-studies",
    "title": "A Guided Tour to Building and Integrating LLM Based Tooling with R",
    "section": "Real-World Case Studies",
    "text": "Real-World Case Studies\n\nCase Study 1: Clinical Study Report Automation\nHow A2-AI helped a pharma client reduce CSR preparation time by 60% using LLM-powered automation while maintaining GxP compliance.\n\n\nCase Study 2: SOP Management System\nImplementation of an enterprise-wide SOP chatbot serving 500+ users across multiple departments.\n\n\nCase Study 3: Data Quality Checks\nAutomated data validation using LLMs to identify anomalies and suggest corrections in clinical trial data.",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "A Guided Tour to Building and Integrating LLM Based Tooling with R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/guided-tour-llm-tooling.html#next-steps",
    "href": "workshops/ai-llm/guided-tour-llm-tooling.html#next-steps",
    "title": "A Guided Tour to Building and Integrating LLM Based Tooling with R",
    "section": "Next Steps",
    "text": "Next Steps\nAfter this workshop:\n\nGetting Started with LLM APIs - For foundational knowledge\nLLM-Powered Clinical Data Review - Privacy considerations\nExplore A2-AIâ€™s GitHub for example implementations",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "A Guided Tour to Building and Integrating LLM Based Tooling with R"
    ]
  },
  {
    "objectID": "workshops/ai-llm/guided-tour-llm-tooling.html#additional-resources",
    "href": "workshops/ai-llm/guided-tour-llm-tooling.html#additional-resources",
    "title": "A Guided Tour to Building and Integrating LLM Based Tooling with R",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nAWS Bedrock documentation: aws.amazon.com/bedrock\nMCP specification: modelcontextprotocol.io\nA2-AI blog: Industry insights and case studies\n\n\n\n\n\n\n\nTipWorkshop Materials\n\n\n\nThis is a hands-on workshop with extensive code examples and exercises. All materials will be provided during the session, including:\n\nStarter code templates\nExample datasets (CDISC SDTM/ADaM)\nAWS sandbox environment access\nReference documentation\n\n\n\n\n\nSimilar Workshops\n\nGetting Started with LLM APIs - Foundation concepts\nIntegrating LLM with Clinical Data - Privacy focus\n\n\n\nRelated Presentations\n\n{llumen}: Agentic Framework - Merckâ€™s internal solution\nBuilding the Ultimate R AI Assistant - Rocheâ€™s multi-agent system\n\n\n\nNext Steps\n\nPrerequisites: Start with Getting Started with LLM APIs\nCareer path: AI Specialist Track\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ¤– AI & Large Language Models",
      "A Guided Tour to Building and Integrating LLM Based Tooling with R"
    ]
  },
  {
    "objectID": "workshops/development-validation/building-r-packages.html",
    "href": "workshops/development-validation/building-r-packages.html",
    "title": "Introduction to Building (Better) R Packages",
    "section": "",
    "text": "Beginner Friendly Package Development Best Practices\nTransform your R scripts into professional packages! Learn the fundamentals of package development, from basic structure to documentation, testing, and sharing. By the end of this workshop, you will have made an R package!\n\n\n\nğŸ“¦ Package structure - Essential components\nğŸ”§ Writing functions - Package-friendly style\nğŸ“ Documentation - roxygen2 and examples\nâœ… Testing - Ensuring code quality\nğŸŒ Sharing - GitHub and CRAN\nğŸ’¡ Best practices - Professional development",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "Introduction to Building (Better) R Packages"
    ]
  },
  {
    "objectID": "workshops/development-validation/building-r-packages.html#overview",
    "href": "workshops/development-validation/building-r-packages.html#overview",
    "title": "Introduction to Building (Better) R Packages",
    "section": "",
    "text": "Beginner Friendly Package Development Best Practices\nTransform your R scripts into professional packages! Learn the fundamentals of package development, from basic structure to documentation, testing, and sharing. By the end of this workshop, you will have made an R package!\n\n\n\nğŸ“¦ Package structure - Essential components\nğŸ”§ Writing functions - Package-friendly style\nğŸ“ Documentation - roxygen2 and examples\nâœ… Testing - Ensuring code quality\nğŸŒ Sharing - GitHub and CRAN\nğŸ’¡ Best practices - Professional development",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "Introduction to Building (Better) R Packages"
    ]
  },
  {
    "objectID": "workshops/development-validation/building-r-packages.html#prerequisites",
    "href": "workshops/development-validation/building-r-packages.html#prerequisites",
    "title": "Introduction to Building (Better) R Packages",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRequired Knowledge:\n\nBasic R programming\nNo prior package development experience needed!\nNo function writing experience required!\n\nSetup:\n\nR and RStudio\n{devtools} and {usethis} packages",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "Introduction to Building (Better) R Packages"
    ]
  },
  {
    "objectID": "workshops/development-validation/building-r-packages.html#key-packages",
    "href": "workshops/development-validation/building-r-packages.html#key-packages",
    "title": "Introduction to Building (Better) R Packages",
    "section": "Key Packages",
    "text": "Key Packages\n\n{devtools}\n\n\n{usethis}\n\n\n{roxygen2}\n\n\n{testthat}\n\n\n{pkgdown}",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "Introduction to Building (Better) R Packages"
    ]
  },
  {
    "objectID": "workshops/development-validation/building-r-packages.html#workshop-materials",
    "href": "workshops/development-validation/building-r-packages.html#workshop-materials",
    "title": "Introduction to Building (Better) R Packages",
    "section": "Workshop Materials",
    "text": "Workshop Materials\n\n\n\n\n\n\nNoteResources\n\n\n\nSlides: nrennie.rbind.io/r-pharma-2025-r-packages",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "Introduction to Building (Better) R Packages"
    ]
  },
  {
    "objectID": "workshops/development-validation/building-r-packages.html#why-build-packages",
    "href": "workshops/development-validation/building-r-packages.html#why-build-packages",
    "title": "Introduction to Building (Better) R Packages",
    "section": "Why Build Packages?",
    "text": "Why Build Packages?\n\nBenefits\n\nâ™»ï¸ Reusability - Use your code across projects\nğŸ¤ Sharing - Collaborate easily with others\nğŸ“š Documentation - Built-in help pages\nâœ… Testing - Ensure code reliability\nğŸ¯ Organization - Structured project layout",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "Introduction to Building (Better) R Packages"
    ]
  },
  {
    "objectID": "workshops/development-validation/building-r-packages.html#workshop-structure",
    "href": "workshops/development-validation/building-r-packages.html#workshop-structure",
    "title": "Introduction to Building (Better) R Packages",
    "section": "Workshop Structure",
    "text": "Workshop Structure\n\nPart 1: Package Basics (45 min)\nCreating Your First Package:\nlibrary(usethis)\n\n# Create package structure\ncreate_package(\"~/mypackage\")\n\n# Your package now has:\n# - DESCRIPTION file\n# - NAMESPACE file\n# - R/ directory\n# - .Rproj file\nEssential Components:\n\nDESCRIPTION - Package metadata\nNAMESPACE - Function exports\nR/ - Function code\nman/ - Documentation (auto-generated)\n\n\n\nPart 2: Writing Functions (45 min)\nPackage-Friendly Function Style:\n# R/my_function.R\n\n#' Calculate Summary Statistics\n#'\n#' @param x A numeric vector\n#' @param na.rm Logical. Should missing values be removed?\n#'\n#' @return A named vector of summary statistics\n#' @export\n#'\n#' @examples\n#' calculate_summary(c(1, 2, 3, 4, 5))\n#' calculate_summary(c(1, 2, NA, 4), na.rm = TRUE)\ncalculate_summary &lt;- function(x, na.rm = FALSE) {\n  if (!is.numeric(x)) {\n    stop(\"`x` must be numeric\")\n  }\n  \n  c(\n    mean = mean(x, na.rm = na.rm),\n    sd = sd(x, na.rm = na.rm),\n    min = min(x, na.rm = na.rm),\n    max = max(x, na.rm = na.rm)\n  )\n}\nBest Practices:\n\nâœ… Clear function names\nâœ… Input validation\nâœ… Informative error messages\nâœ… Consistent parameter names\nâœ… Return predictable output\n\n\n\nPart 3: Documentation (30 min)\nroxygen2 Documentation:\n# Use roxygen2 comments above functions\n#' (starts with #')\n#' \n#' @param parameter_name Description\n#' @return What the function returns\n#' @export Makes function available to users\n#' @examples Usage examples\n\n# Generate documentation\ndevtools::document()\nCreating README:\nuse_readme_md()\n\n\nPart 4: Testing (30 min)\nSetting Up Tests:\n# Initialize testing infrastructure\nusethis::use_testthat()\n\n# Create test file\nusethis::use_test(\"my_function\")\nWriting Tests:\n# tests/testthat/test-my_function.R\n\ntest_that(\"calculate_summary works correctly\", {\n  x &lt;- c(1, 2, 3, 4, 5)\n  result &lt;- calculate_summary(x)\n  \n  expect_equal(result[\"mean\"], 3)\n  expect_equal(result[\"sd\"], sd(x))\n  expect_length(result, 4)\n})\n\ntest_that(\"calculate_summary handles NA values\", {\n  x &lt;- c(1, 2, NA, 4)\n  \n  expect_true(is.na(calculate_summary(x)[\"mean\"]))\n  expect_false(is.na(calculate_summary(x, na.rm = TRUE)[\"mean\"]))\n})\n\ntest_that(\"calculate_summary validates input\", {\n  expect_error(calculate_summary(\"not numeric\"))\n})\n\n\nPart 5: Sharing Your Package (30 min)\nGitHub:\n# Initialize git\nusethis::use_git()\n\n# Connect to GitHub\nusethis::use_github()\nPackage Website:\n# Create pkgdown site\nusethis::use_pkgdown()\npkgdown::build_site()\nCRAN Preparation:\n# Check package\ndevtools::check()\n\n# Spell check\ndevtools::spell_check()\n\n# Check for CRAN policies\nrcmdcheck::rcmdcheck()",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "Introduction to Building (Better) R Packages"
    ]
  },
  {
    "objectID": "workshops/development-validation/building-r-packages.html#pharmaceutical-package-considerations",
    "href": "workshops/development-validation/building-r-packages.html#pharmaceutical-package-considerations",
    "title": "Introduction to Building (Better) R Packages",
    "section": "Pharmaceutical Package Considerations",
    "text": "Pharmaceutical Package Considerations\n\nValidation Requirements\n\nğŸ“‹ Documentation - Comprehensive function docs\nâœ… Testing - High test coverage (aim for 80%+)\nğŸ”’ Version control - Track all changes\nğŸ“ Change log - Document updates\nğŸ·ï¸ Releases - Tagged versions\n\n\n\nExample Structure for Pharma Package\nmypharmapackage/\nâ”œâ”€â”€ DESCRIPTION          # Package metadata\nâ”œâ”€â”€ NAMESPACE            # Auto-generated\nâ”œâ”€â”€ R/                   # Function code\nâ”‚   â”œâ”€â”€ data_processing.R\nâ”‚   â”œâ”€â”€ statistical_tests.R\nâ”‚   â””â”€â”€ plotting.R\nâ”œâ”€â”€ tests/\nâ”‚   â””â”€â”€ testthat/       # Unit tests\nâ”œâ”€â”€ vignettes/          # Long-form documentation\nâ”œâ”€â”€ data/               # Example datasets\nâ”œâ”€â”€ inst/\nâ”‚   â””â”€â”€ validation/     # Validation documents\nâ””â”€â”€ README.md",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "Introduction to Building (Better) R Packages"
    ]
  },
  {
    "objectID": "workshops/development-validation/building-r-packages.html#hands-on-exercise",
    "href": "workshops/development-validation/building-r-packages.html#hands-on-exercise",
    "title": "Introduction to Building (Better) R Packages",
    "section": "Hands-On Exercise",
    "text": "Hands-On Exercise\nBuild a Clinical Trial Utility Package:\nCreate a package with functions for:\n\nSubject disposition summary\nAdverse event frequency table\nDemographic summary table\n\nInclude:\n\nProper documentation\nInput validation\nUnit tests\nExamples\nREADME",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "Introduction to Building (Better) R Packages"
    ]
  },
  {
    "objectID": "workshops/development-validation/building-r-packages.html#best-practices-summary",
    "href": "workshops/development-validation/building-r-packages.html#best-practices-summary",
    "title": "Introduction to Building (Better) R Packages",
    "section": "Best Practices Summary",
    "text": "Best Practices Summary\n\nDoâ€™s âœ…\n\nWrite clear, focused functions\nDocument everything with roxygen2\nTest your functions thoroughly\nUse meaningful names\nInclude examples\nKeep dependencies minimal\nVersion control with git\n\n\n\nDonâ€™ts âŒ\n\nDonâ€™t use library() inside functions\nDonâ€™t modify userâ€™s options/settings\nDonâ€™t write to userâ€™s file system without permission\nDonâ€™t use non-standard evaluation carelessly\nDonâ€™t skip documentation",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "Introduction to Building (Better) R Packages"
    ]
  },
  {
    "objectID": "workshops/development-validation/building-r-packages.html#advanced-topics-time-permitting",
    "href": "workshops/development-validation/building-r-packages.html#advanced-topics-time-permitting",
    "title": "Introduction to Building (Better) R Packages",
    "section": "Advanced Topics (Time Permitting)",
    "text": "Advanced Topics (Time Permitting)\n\nVignettes - Long-form documentation\nData in packages - Including example datasets\nPackage dependencies - Managing imports\nS3 methods - Object-oriented programming\nGitHub Actions - Automated checks",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "Introduction to Building (Better) R Packages"
    ]
  },
  {
    "objectID": "workshops/development-validation/building-r-packages.html#learning-outcomes",
    "href": "workshops/development-validation/building-r-packages.html#learning-outcomes",
    "title": "Introduction to Building (Better) R Packages",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of this workshop, you will be able to:\nâœ… Create a basic R package structure\nâœ… Write package-friendly functions\nâœ… Document functions with roxygen2\nâœ… Write unit tests with {testthat}\nâœ… Share your package on GitHub\nâœ… Understand best practices for package development\nâœ… Have created your own R package!",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "Introduction to Building (Better) R Packages"
    ]
  },
  {
    "objectID": "workshops/development-validation/building-r-packages.html#resources-for-continued-learning",
    "href": "workshops/development-validation/building-r-packages.html#resources-for-continued-learning",
    "title": "Introduction to Building (Better) R Packages",
    "section": "Resources for Continued Learning",
    "text": "Resources for Continued Learning\n\nR Packages book (2nd ed): r-pkgs.org\nusethis documentation: usethis.r-lib.org\nWriting R Extensions: Official CRAN manual\nPharmaverse packages: Examples of pharma-specific packages",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "Introduction to Building (Better) R Packages"
    ]
  },
  {
    "objectID": "workshops/development-validation/building-r-packages.html#next-steps",
    "href": "workshops/development-validation/building-r-packages.html#next-steps",
    "title": "Introduction to Building (Better) R Packages",
    "section": "Next Steps",
    "text": "Next Steps\nAfter this workshop:\n\nPractice by packaging your existing code\nContribute to open-source pharma packages\nExplore {pkgdown} for beautiful websites\nLearn about continuous integration (GitHub Actions)\nJoin the pharmaverse community!\n\n\n\nSimilar Workshops\n\nR Validation Discussion - Validation for packages\ndatasetjson - Example of well-built package\n\n\n\nRelated Presentations\n\nGSKâ€™s Open Source Journey - Industry adoption\n\n\n\nNext Steps\n\nAfter building: Learn validation\nCareer impact: Package Development skills\nContribute: Join pharmaverse\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "Introduction to Building (Better) R Packages"
    ]
  },
  {
    "objectID": "workshops/development-validation/datasetjson.html",
    "href": "workshops/development-validation/datasetjson.html",
    "title": "datasetjson: Read and Write CDISC Dataset JSON",
    "section": "",
    "text": "Intermediate CDISC Data Exchange\nJoin us for an engaging workshop on Dataset-JSON, a powerful format for sharing datasets. Learn why Dataset-JSON is preferable to formats like Parquet, explore the specification in detail, and get hands-on experience implementing it in both R and Python.\n\n\n\nğŸ“‹ Dataset-JSON specification - Format details\nğŸ†š Comparison - vs Parquet, XPT, and other formats\nğŸ R and Python - Implementation in both languages\nğŸ”„ Adoption plans - Industry roadmap\nğŸš€ API integration - Future developments\nğŸ”§ Practical implementation - Real-world usage",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "datasetjson: Read and Write CDISC Dataset JSON"
    ]
  },
  {
    "objectID": "workshops/development-validation/datasetjson.html#overview",
    "href": "workshops/development-validation/datasetjson.html#overview",
    "title": "datasetjson: Read and Write CDISC Dataset JSON",
    "section": "",
    "text": "Intermediate CDISC Data Exchange\nJoin us for an engaging workshop on Dataset-JSON, a powerful format for sharing datasets. Learn why Dataset-JSON is preferable to formats like Parquet, explore the specification in detail, and get hands-on experience implementing it in both R and Python.\n\n\n\nğŸ“‹ Dataset-JSON specification - Format details\nğŸ†š Comparison - vs Parquet, XPT, and other formats\nğŸ R and Python - Implementation in both languages\nğŸ”„ Adoption plans - Industry roadmap\nğŸš€ API integration - Future developments\nğŸ”§ Practical implementation - Real-world usage",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "datasetjson: Read and Write CDISC Dataset JSON"
    ]
  },
  {
    "objectID": "workshops/development-validation/datasetjson.html#prerequisites",
    "href": "workshops/development-validation/datasetjson.html#prerequisites",
    "title": "datasetjson: Read and Write CDISC Dataset JSON",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRequired Knowledge:\n\nBasic R or Python programming\nUnderstanding of clinical trial data\nFamiliarity with CDISC standards helpful\n\nFor Hands-on:\n\nLaptop with R or Python installed\nWorkshop materials (provided)",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "datasetjson: Read and Write CDISC Dataset JSON"
    ]
  },
  {
    "objectID": "workshops/development-validation/datasetjson.html#key-tools",
    "href": "workshops/development-validation/datasetjson.html#key-tools",
    "title": "datasetjson: Read and Write CDISC Dataset JSON",
    "section": "Key Tools",
    "text": "Key Tools\n\n{datasetjson} (R)\n\n\ndatasetjson (Python)\n\n\nCDISC Dataset-JSON",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "datasetjson: Read and Write CDISC Dataset JSON"
    ]
  },
  {
    "objectID": "workshops/development-validation/datasetjson.html#workshop-materials",
    "href": "workshops/development-validation/datasetjson.html#workshop-materials",
    "title": "datasetjson: Read and Write CDISC Dataset JSON",
    "section": "Workshop Materials",
    "text": "Workshop Materials\n\n\n\n\n\n\nNoteResources\n\n\n\nPackage Documentation: atorus-research.github.io/datasetjson\nWorkshop Materials: atorus-research.github.io/datasetjson_workshop",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "datasetjson: Read and Write CDISC Dataset JSON"
    ]
  },
  {
    "objectID": "workshops/development-validation/datasetjson.html#what-is-dataset-json",
    "href": "workshops/development-validation/datasetjson.html#what-is-dataset-json",
    "title": "datasetjson: Read and Write CDISC Dataset JSON",
    "section": "What is Dataset-JSON?",
    "text": "What is Dataset-JSON?\n\nCDISC Standard Format\nDataset-JSON is an emerging CDISC standard for representing clinical trial datasets in JSON format.\nKey Features:\n\nğŸ“Š Self-describing - Metadata included\nğŸ” Human-readable - Text-based format\nğŸŒ Web-friendly - Native JSON support\nğŸ”— Linked data - Relationships preserved\nâœ… Validated - Schema-based validation\n\n\n\nExample Structure\n{\n  \"datasetJSONVersion\": \"1.0.0\",\n  \"fileOID\": \"example.adsl\",\n  \"datasetName\": \"ADSL\",\n  \"datasetLabel\": \"Subject-Level Analysis Dataset\",\n  \"records\": 254,\n  \"columns\": [\n    {\n      \"name\": \"USUBJID\",\n      \"label\": \"Unique Subject Identifier\",\n      \"dataType\": \"string\",\n      \"length\": 20\n    },\n    {\n      \"name\": \"AGE\",\n      \"label\": \"Age\",\n      \"dataType\": \"integer\"\n    }\n  ],\n  \"rows\": [\n    [\"ABC-001\", 65],\n    [\"ABC-002\", 72]\n  ]\n}",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "datasetjson: Read and Write CDISC Dataset JSON"
    ]
  },
  {
    "objectID": "workshops/development-validation/datasetjson.html#why-dataset-json",
    "href": "workshops/development-validation/datasetjson.html#why-dataset-json",
    "title": "datasetjson: Read and Write CDISC Dataset JSON",
    "section": "Why Dataset-JSON?",
    "text": "Why Dataset-JSON?\n\nAdvantages Over Other Formats\n\nvs XPT (SAS Transport)\n\n\n\nFeature\nDataset-JSON\nXPT\n\n\n\n\nHuman-readable\nâœ… Yes\nâŒ No\n\n\nSelf-describing\nâœ… Yes\nâš ï¸ Limited\n\n\nWeb APIs\nâœ… Native\nâŒ Poor\n\n\nModern tools\nâœ… Excellent\nâš ï¸ Legacy\n\n\nSize efficiency\nâš ï¸ Larger\nâœ… Smaller\n\n\n\n\n\nvs Parquet\n\n\n\nFeature\nDataset-JSON\nParquet\n\n\n\n\nCDISC standard\nâœ… Yes\nâŒ No\n\n\nMetadata\nâœ… Rich\nâš ï¸ Basic\n\n\nHuman-readable\nâœ… Yes\nâŒ Binary\n\n\nQuery performance\nâš ï¸ Slower\nâœ… Very fast\n\n\nInteroperability\nâœ… Excellent\nâš ï¸ Tool-specific\n\n\n\n\n\n\nUse Cases\nIdeal for:\n\nâœ… Regulatory submissions\nâœ… Data exchange between organizations\nâœ… Web APIs and services\nâœ… Documentation and review\nâœ… Long-term archival\n\nNot ideal for:\n\nâŒ Big data analytics (use Parquet)\nâŒ Real-time processing\nâŒ Embedded systems",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "datasetjson: Read and Write CDISC Dataset JSON"
    ]
  },
  {
    "objectID": "workshops/development-validation/datasetjson.html#workshop-structure",
    "href": "workshops/development-validation/datasetjson.html#workshop-structure",
    "title": "datasetjson: Read and Write CDISC Dataset JSON",
    "section": "Workshop Structure",
    "text": "Workshop Structure\n\nPart 1: Introduction (30 min)\nTopics:\n\nDataset-JSON motivation\nFormat specification walkthrough\nComparison with alternatives\nIndustry adoption status\n\n\n\nPart 2: R Implementation (45 min)\nHands-on with R package:\nlibrary(datasetjson)\n\n# Read Dataset-JSON\nadsl &lt;- read_dataset_json(\"adsl.json\")\n\n# Standard R data frame\nhead(adsl)\nstr(adsl)\n\n# Access metadata\nattributes(adsl)$column_labels\nattributes(adsl)$column_types\n\n# Write Dataset-JSON\nwrite_dataset_json(adsl, \"output.json\")\n\n# Validation\nvalidate_dataset_json(\"adsl.json\")\nFeatures:\n\nRead/write Dataset-JSON files\nAutomatic metadata handling\nSchema validation\nIntegration with tidyverse\n\n\n\nPart 3: Python Implementation (45 min)\nHands-on with Python package:\nimport datasetjson as dsj\nimport pandas as pd\n\n# Read Dataset-JSON\nadsl = dsj.read_json(\"adsl.json\")\n\n# Pandas DataFrame\nprint(adsl.head())\nprint(adsl.info())\n\n# Access metadata\nprint(dsj.get_metadata(adsl))\n\n# Write Dataset-JSON\ndsj.write_json(adsl, \"output.json\")\n\n# Validation\ndsj.validate(\"adsl.json\")\nFeatures:\n\nPandas integration\nMetadata preservation\nSchema validation\nType handling\n\n\n\nPart 4: Advanced Topics (45 min)\nTopics:\n\nCustom metadata\nLarge file handling\nCompression options\nStreaming implementations\nAPI integration patterns\n\n\n\nPart 5: Future Directions (30 min)\nDiscussion:\n\nRegulatory acceptance timeline\nTool support roadmap\nCommunity adoption\nAPI standards\nYour organizationâ€™s plans",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "datasetjson: Read and Write CDISC Dataset JSON"
    ]
  },
  {
    "objectID": "workshops/development-validation/datasetjson.html#practical-exercises",
    "href": "workshops/development-validation/datasetjson.html#practical-exercises",
    "title": "datasetjson: Read and Write CDISC Dataset JSON",
    "section": "Practical Exercises",
    "text": "Practical Exercises\n\nExercise 1: Read and Explore\nLoad Dataset-JSON file and explore structure.\nTasks:\n\nRead ADSL dataset\nExamine metadata\nCompare with XPT\nValidate format\n\n\n\nExercise 2: Convert Formats\nConvert between Dataset-JSON and other formats.\nTasks:\n\nXPT â†’ Dataset-JSON\nCSV â†’ Dataset-JSON\nDataset-JSON â†’ Parquet\nPreserve metadata\n\n\n\nExercise 3: Create from Scratch\nBuild Dataset-JSON from raw data.\nTasks:\n\nDefine column metadata\nSet data types\nAdd labels and descriptions\nValidate output\n\n\n\nExercise 4: API Integration\nServe Dataset-JSON via REST API.\nR (Plumber):\nlibrary(plumber)\n\n#* Get ADSL dataset\n#* @get /datasets/adsl\n#* @serializer json\nfunction() {\n  read_dataset_json(\"data/adsl.json\")\n}\nPython (FastAPI):\nfrom fastapi import FastAPI\nimport datasetjson as dsj\n\napp = FastAPI()\n\n@app.get(\"/datasets/adsl\")\ndef get_adsl():\n    return dsj.read_json(\"data/adsl.json\")",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "datasetjson: Read and Write CDISC Dataset JSON"
    ]
  },
  {
    "objectID": "workshops/development-validation/datasetjson.html#cdisc-dataset-json-specification",
    "href": "workshops/development-validation/datasetjson.html#cdisc-dataset-json-specification",
    "title": "datasetjson: Read and Write CDISC Dataset JSON",
    "section": "CDISC Dataset-JSON Specification",
    "text": "CDISC Dataset-JSON Specification\n\nKey Components\nFile Metadata:\n\ndatasetJSONVersion\nfileOID\ndatasetName\ndatasetLabel\nrecords (row count)\n\nColumn Definitions:\n\nname\nlabel\ndataType (string, integer, float, datetime)\nlength\ndisplayFormat\n\nData Rows:\n\nArray of arrays (efficient)\nColumn order matches definitions\nNull handling\n\nAdditional Metadata:\n\nStudy information\nVariable relationships\nCode lists\nValue-level metadata",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "datasetjson: Read and Write CDISC Dataset JSON"
    ]
  },
  {
    "objectID": "workshops/development-validation/datasetjson.html#industry-adoption",
    "href": "workshops/development-validation/datasetjson.html#industry-adoption",
    "title": "datasetjson: Read and Write CDISC Dataset JSON",
    "section": "Industry Adoption",
    "text": "Industry Adoption\n\nCurrent Status\nCDISC:\n\nâœ… Published specification\nğŸ”„ Active development\nğŸ“¢ Industry engagement\n\nTools:\n\nâœ… R package (Atorus)\nâœ… Python package (CDISC/Atorus)\nğŸ”„ SAS support in development\nğŸ”„ Validator tools emerging\n\nPharma:\n\nğŸ” Pilot projects underway\nğŸ“‹ Evaluation phase\nğŸ¯ 2026+ adoption expected\n\n\n\nRoadmap\n2025:\n\nFinalize specification v1.0\nExpand tool support\nPilot submissions\n\n2026+:\n\nBroader industry adoption\nRegulatory acceptance\nReplace XPT in workflows",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "datasetjson: Read and Write CDISC Dataset JSON"
    ]
  },
  {
    "objectID": "workshops/development-validation/datasetjson.html#best-practices",
    "href": "workshops/development-validation/datasetjson.html#best-practices",
    "title": "datasetjson: Read and Write CDISC Dataset JSON",
    "section": "Best Practices",
    "text": "Best Practices\n\nCreating Dataset-JSON\nâœ… Do:\n\nInclude comprehensive metadata\nUse CDISC terminology\nValidate against schema\nDocument conventions\nVersion your files\n\nâŒ Donâ€™t:\n\nOmit critical metadata\nUse inconsistent naming\nSkip validation\nIgnore data types\nCreate huge files (split if needed)\n\n\n\nIntegration\nAPI Design:\n\nRESTful endpoints\nPagination for large datasets\nCaching strategies\nError handling\nAuthentication\n\nStorage:\n\nCompress for archival\nIndex for search\nBackup metadata separately\nVersion control friendly",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "datasetjson: Read and Write CDISC Dataset JSON"
    ]
  },
  {
    "objectID": "workshops/development-validation/datasetjson.html#learning-outcomes",
    "href": "workshops/development-validation/datasetjson.html#learning-outcomes",
    "title": "datasetjson: Read and Write CDISC Dataset JSON",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nâœ… Understand Dataset-JSON specification\nâœ… Read/write Dataset-JSON in R\nâœ… Read/write Dataset-JSON in Python\nâœ… Convert between formats\nâœ… Integrate with APIs\nâœ… Plan adoption strategy\nâœ… Contribute to development",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "datasetjson: Read and Write CDISC Dataset JSON"
    ]
  },
  {
    "objectID": "workshops/development-validation/datasetjson.html#resources-1",
    "href": "workshops/development-validation/datasetjson.html#resources-1",
    "title": "datasetjson: Read and Write CDISC Dataset JSON",
    "section": "Resources",
    "text": "Resources\nOfficial:\n\nCDISC Dataset-JSON specification\nCDISC website and wiki\nPackage documentation\n\nCommunity:\n\nGitHub repositories\nCDISC Dataset-JSON working group\nStack Overflow tags\n\nLearning:\n\nWorkshop materials\nExample datasets\nTutorial notebooks",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "datasetjson: Read and Write CDISC Dataset JSON"
    ]
  },
  {
    "objectID": "workshops/development-validation/datasetjson.html#getting-involved",
    "href": "workshops/development-validation/datasetjson.html#getting-involved",
    "title": "datasetjson: Read and Write CDISC Dataset JSON",
    "section": "Getting Involved",
    "text": "Getting Involved\nContribute:\n\nTest packages\nReport issues\nSubmit use cases\nJoin working groups\n\nStay Informed:\n\nCDISC newsletters\nPackage releases\nConference presentations\n\n\n\nSimilar Workshops\n\nSDTM Programming - SDTM dataset creation\nPython for CSR - Modern data formats\n\n\n\nRelated Presentations\n\nMosaic: ARS-Driven Automation - CDISC standards\n\n\n\nNext Steps\n\nFor SDTM: See {sdtm.oak} workshop\nStandards evolution: ARS/ARM trends\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ”§ Development & Validation",
      "datasetjson: Read and Write CDISC Dataset JSON"
    ]
  },
  {
    "objectID": "workshops/specialized/hpc-cluster-shiny.html",
    "href": "workshops/specialized/hpc-cluster-shiny.html",
    "title": "Supercharge Your Shiny App by Offloading Computations to HPC Cluster",
    "section": "",
    "text": "Advanced Shiny HPC\nWith increasing data and analysis complexity, developers are tempted to put serious computations into Shiny apps. Given resource limitations and shared infrastructure, this often leads to crashes. This workshop teaches you to interact with remote HPC clusters from your laptop or Posit Connect, offloading computations while keeping apps interactive.\n\n\n\nğŸ–¥ï¸ HPC cluster interaction from laptop\nğŸš€ Remote job submission from Shiny\nâš¡ Parallel computing for faster results\nğŸ“Š Resource management and optimization\nğŸ”„ App stability while leveraging HPC power",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Supercharge Your Shiny App by Offloading Computations to HPC Cluster"
    ]
  },
  {
    "objectID": "workshops/specialized/hpc-cluster-shiny.html#overview",
    "href": "workshops/specialized/hpc-cluster-shiny.html#overview",
    "title": "Supercharge Your Shiny App by Offloading Computations to HPC Cluster",
    "section": "",
    "text": "Advanced Shiny HPC\nWith increasing data and analysis complexity, developers are tempted to put serious computations into Shiny apps. Given resource limitations and shared infrastructure, this often leads to crashes. This workshop teaches you to interact with remote HPC clusters from your laptop or Posit Connect, offloading computations while keeping apps interactive.\n\n\n\nğŸ–¥ï¸ HPC cluster interaction from laptop\nğŸš€ Remote job submission from Shiny\nâš¡ Parallel computing for faster results\nğŸ“Š Resource management and optimization\nğŸ”„ App stability while leveraging HPC power",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Supercharge Your Shiny App by Offloading Computations to HPC Cluster"
    ]
  },
  {
    "objectID": "workshops/specialized/hpc-cluster-shiny.html#prerequisites",
    "href": "workshops/specialized/hpc-cluster-shiny.html#prerequisites",
    "title": "Supercharge Your Shiny App by Offloading Computations to HPC Cluster",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRequired Knowledge:\n\nIntermediate Shiny development\nBasic understanding of HPC concepts\nR programming proficiency\n\nHelpful:\n\nExperience with computational bottlenecks\nSSH and remote systems familiarity",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Supercharge Your Shiny App by Offloading Computations to HPC Cluster"
    ]
  },
  {
    "objectID": "workshops/specialized/hpc-cluster-shiny.html#key-technologies",
    "href": "workshops/specialized/hpc-cluster-shiny.html#key-technologies",
    "title": "Supercharge Your Shiny App by Offloading Computations to HPC Cluster",
    "section": "Key Technologies",
    "text": "Key Technologies\n\nShiny\n\n\nHPC Cluster\n\n\nPosit Connect\n\n\nJob Schedulers\n\n\n{future}",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Supercharge Your Shiny App by Offloading Computations to HPC Cluster"
    ]
  },
  {
    "objectID": "workshops/specialized/hpc-cluster-shiny.html#the-problem",
    "href": "workshops/specialized/hpc-cluster-shiny.html#the-problem",
    "title": "Supercharge Your Shiny App by Offloading Computations to HPC Cluster",
    "section": "The Problem",
    "text": "The Problem\n\nTypical Scenario\n# BAD: Heavy computation in Shiny\nserver &lt;- function(input, output, session) {\n  output$result &lt;- renderPlot({\n    # This crashes the app!\n    heavy_simulation(\n      iterations = 1000000,\n      samples = input$samples\n    )\n  })\n}\nIssues:\n\nâŒ App becomes unresponsive\nâŒ Other users affected (shared server)\nâŒ Memory limits exceeded\nâŒ Timeouts and crashes",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Supercharge Your Shiny App by Offloading Computations to HPC Cluster"
    ]
  },
  {
    "objectID": "workshops/specialized/hpc-cluster-shiny.html#the-solution",
    "href": "workshops/specialized/hpc-cluster-shiny.html#the-solution",
    "title": "Supercharge Your Shiny App by Offloading Computations to HPC Cluster",
    "section": "The Solution",
    "text": "The Solution\n\nOffload to HPC\n# GOOD: Offload to cluster\nserver &lt;- function(input, output, session) {\n  observeEvent(input$run, {\n    # Submit job to HPC\n    job_id &lt;- submit_hpc_job(\n      script = \"simulation.R\",\n      params = list(samples = input$samples)\n    )\n    \n    # Poll for results\n    result &lt;- poll_job_result(job_id)\n    \n    output$result &lt;- renderPlot(result)\n  })\n}\nBenefits:\n\nâœ… App stays responsive\nâœ… Leverage massive compute power\nâœ… No local resource limits\nâœ… Parallel processing\nâœ… Other users unaffected",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Supercharge Your Shiny App by Offloading Computations to HPC Cluster"
    ]
  },
  {
    "objectID": "workshops/specialized/hpc-cluster-shiny.html#workshop-content",
    "href": "workshops/specialized/hpc-cluster-shiny.html#workshop-content",
    "title": "Supercharge Your Shiny App by Offloading Computations to HPC Cluster",
    "section": "Workshop Content",
    "text": "Workshop Content\n\nPart 1: HPC Cluster Basics\nJob Schedulers:\n\nSLURM\nPBS\nSGE\nLSF\n\nKey Concepts:\n\nJob submission\nResource requests (cores, memory, time)\nQueue management\nJob monitoring\n\n\n\nPart 2: Connecting Shiny to HPC\nArchitecture:\nUser â†’ Shiny App â†’ SSH Connection â†’ HPC Cluster\n                        â†“\n                   Job Scheduler\n                        â†“\n                   Compute Nodes\n                        â†“\n                   Results â† Back to Shiny\nAuthentication:\n\nSSH keys\nCertificates\nSecure credential management\n\n\n\nPart 3: Job Submission from R\nlibrary(ssh)\n\n# Connect to cluster\nsession &lt;- ssh_connect(\"user@hpc.university.edu\")\n\n# Submit job\nssh_exec_wait(session, command = \"\n  sbatch --job-name=shiny_analysis \\\n         --ntasks=16 \\\n         --mem=64G \\\n         --time=01:00:00 \\\n         analysis_script.R\n\")\n\n# Check status\nstatus &lt;- ssh_exec_internal(session, \"squeue --user=myuser\")\n\n# Retrieve results\nssh_scp_download(session, \n                 files = \"results/output.rds\",\n                 to = \"local_results/\")\n\n\nPart 4: Building Reactive HPC Shiny App\nKey Components:\n\nJob submission UI\n\nui &lt;- fluidPage(\n  numericInput(\"n_samples\", \"Samples:\", 1000000),\n  numericInput(\"n_cores\", \"Cores:\", 16),\n  actionButton(\"submit\", \"Run on HPC\"),\n  textOutput(\"status\"),\n  plotOutput(\"results\")\n)\n\nJob management\n\nserver &lt;- function(input, output, session) {\n  job_status &lt;- reactiveVal(\"Idle\")\n  \n  observeEvent(input$submit, {\n    job_status(\"Submitting...\")\n    \n    # Submit to HPC\n    job_id &lt;- submit_slurm_job(\n      cores = input$n_cores,\n      memory = \"64G\",\n      script = generate_script(input$n_samples)\n    )\n    \n    job_status(paste(\"Running - Job ID:\", job_id))\n    \n    # Poll for completion\n    observe({\n      if (is_job_complete(job_id)) {\n        results &lt;- fetch_results(job_id)\n        output$results &lt;- renderPlot(results)\n        job_status(\"Complete\")\n      }\n    })\n  })\n  \n  output$status &lt;- renderText(job_status())\n}\n\n\nPart 5: Advanced Patterns\nParallel Workflows:\n\nMultiple simultaneous jobs\nParameter sweeps\nSensitivity analysis\n\nResult Caching:\n\nStore results in shared filesystem\nAvoid re-computation\nSession persistence\n\nProgress Tracking:\n\nReal-time job monitoring\nLog file streaming\nEstimated completion",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Supercharge Your Shiny App by Offloading Computations to HPC Cluster"
    ]
  },
  {
    "objectID": "workshops/specialized/hpc-cluster-shiny.html#practical-example-pharmacokinetic-simulation",
    "href": "workshops/specialized/hpc-cluster-shiny.html#practical-example-pharmacokinetic-simulation",
    "title": "Supercharge Your Shiny App by Offloading Computations to HPC Cluster",
    "section": "Practical Example: Pharmacokinetic Simulation",
    "text": "Practical Example: Pharmacokinetic Simulation\n# Heavy PK simulation\npk_simulation &lt;- function(n_subjects, n_doses, n_cores) {\n  # This takes 30 minutes locally\n  # But 2 minutes on HPC with 64 cores!\n  \n  results &lt;- parallel_pk_sim(\n    subjects = n_subjects,\n    doses = n_doses,\n    cores = n_cores\n  )\n  \n  return(results)\n}\n\n# Shiny app\nui &lt;- fluidPage(\n  titlePanel(\"PK Simulation (HPC-Powered)\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"subjects\", \"Subjects:\", 100, 10000, 1000),\n      sliderInput(\"doses\", \"Doses:\", 1, 20, 5),\n      sliderInput(\"cores\", \"HPC Cores:\", 1, 128, 64),\n      actionButton(\"run_hpc\", \"Run on HPC\"),\n      hr(),\n      textOutput(\"job_status\")\n    ),\n    \n    mainPanel(\n      plotOutput(\"concentration_time\"),\n      plotOutput(\"exposure_distribution\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  # HPC job management logic\n  # (See workshop materials for full code)\n}",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Supercharge Your Shiny App by Offloading Computations to HPC Cluster"
    ]
  },
  {
    "objectID": "workshops/specialized/hpc-cluster-shiny.html#use-cases-in-pharma",
    "href": "workshops/specialized/hpc-cluster-shiny.html#use-cases-in-pharma",
    "title": "Supercharge Your Shiny App by Offloading Computations to HPC Cluster",
    "section": "Use Cases in Pharma",
    "text": "Use Cases in Pharma\n\nClinical Trial Simulations\n\nDesign optimization\nPower calculations\nScenario analysis\n\n\n\nPharmacokinetic Modeling\n\nPopulation PK\nPBPK simulations\nDose optimization\n\n\n\nGenomics Analysis\n\nVariant calling\nPathway analysis\nBiomarker discovery\n\n\n\nReal-World Evidence\n\nLarge database queries\nPropensity matching\nSurvival modeling",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Supercharge Your Shiny App by Offloading Computations to HPC Cluster"
    ]
  },
  {
    "objectID": "workshops/specialized/hpc-cluster-shiny.html#best-practices",
    "href": "workshops/specialized/hpc-cluster-shiny.html#best-practices",
    "title": "Supercharge Your Shiny App by Offloading Computations to HPC Cluster",
    "section": "Best Practices",
    "text": "Best Practices\n\nâœ… Doâ€™s\n\nUse HPC for genuinely heavy computations\nImplement proper error handling\nCache results when possible\nProvide user feedback (progress bars)\nSet reasonable timeout limits\n\n\n\nâŒ Donâ€™ts\n\nDonâ€™t submit jobs for trivial computations\nDonâ€™t ignore job failures\nDonâ€™t hardcode credentials\nDonâ€™t monopolize cluster resources\nDonâ€™t forget to clean up temp files",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Supercharge Your Shiny App by Offloading Computations to HPC Cluster"
    ]
  },
  {
    "objectID": "workshops/specialized/hpc-cluster-shiny.html#deployment-considerations",
    "href": "workshops/specialized/hpc-cluster-shiny.html#deployment-considerations",
    "title": "Supercharge Your Shiny App by Offloading Computations to HPC Cluster",
    "section": "Deployment Considerations",
    "text": "Deployment Considerations\n\nLocal Development\n\nSSH to institutional HPC\nTest with small jobs\nDebug locally when possible\n\n\n\nPosit Connect Deployment\n\nConfigure SSH keys\nNetwork access to HPC\nService account credentials\nMonitor resource usage\n\n\n\nSecurity\n\nEncrypted connections\nCredential management\nAudit trails\nResource quotas",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Supercharge Your Shiny App by Offloading Computations to HPC Cluster"
    ]
  },
  {
    "objectID": "workshops/specialized/hpc-cluster-shiny.html#learning-outcomes",
    "href": "workshops/specialized/hpc-cluster-shiny.html#learning-outcomes",
    "title": "Supercharge Your Shiny App by Offloading Computations to HPC Cluster",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nâœ… Connect Shiny apps to HPC clusters\nâœ… Submit and manage remote jobs\nâœ… Build responsive compute-heavy apps\nâœ… Implement parallel workflows\nâœ… Deploy HPC-enabled apps to Posit Connect\nâœ… Optimize resource usage",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Supercharge Your Shiny App by Offloading Computations to HPC Cluster"
    ]
  },
  {
    "objectID": "workshops/specialized/hpc-cluster-shiny.html#workshop-format",
    "href": "workshops/specialized/hpc-cluster-shiny.html#workshop-format",
    "title": "Supercharge Your Shiny App by Offloading Computations to HPC Cluster",
    "section": "Workshop Format",
    "text": "Workshop Format\nPart 1 (1 hour):\n\nHPC basics and setup\nSimple job submission from R\nMonitoring and results\n\nPart 2 (1 hour):\n\nBuilding HPC-enabled Shiny app\nLive demo and coding\nTroubleshooting\n\nPart 3 (30 min):\n\nParticipantsâ€™ use cases discussion\nQ&A and problem-solving\nDeployment strategies",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Supercharge Your Shiny App by Offloading Computations to HPC Cluster"
    ]
  },
  {
    "objectID": "workshops/specialized/hpc-cluster-shiny.html#next-steps",
    "href": "workshops/specialized/hpc-cluster-shiny.html#next-steps",
    "title": "Supercharge Your Shiny App by Offloading Computations to HPC Cluster",
    "section": "Next Steps",
    "text": "Next Steps\n\nSet up SSH access to your HPC\nTry example app from workshop\nIdentify compute bottlenecks in your apps\nConsider HPC for heavy workloads\n\n\n\nSimilar Workshops\n\nFrom Data to Insights with {teal} - Shiny framework\nGetting Started with LLM APIs - Another compute-heavy use case\n\n\n\nNext Steps\n\nBuild Shiny apps first: {teal} workshop\nAdvanced Shiny: Validating Shiny Apps\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Supercharge Your Shiny App by Offloading Computations to HPC Cluster"
    ]
  },
  {
    "objectID": "workshops/specialized/python-csr-submission.html",
    "href": "workshops/specialized/python-csr-submission.html",
    "title": "Python for Clinical Study Report and Submission",
    "section": "",
    "text": "Intermediate Python Clinical Reporting\nOpen-source Python offers powerful capabilities for clinical trial analysis and reporting. This workshop introduces practical strategies for preparing tables, listings, and figures (TLFs) in a Clinical Study Report (CSR) and assembling submission-ready eCTD packages.\n\n\n\nğŸ Python environment setup with uv\nğŸ“Š Clinical data engineering with polars\nğŸ“ˆ TLF creation with plotnine and rtflite\nğŸ“¦ eCTD packages with py-pkglite\nğŸ”„ Reproducible workflows end-to-end",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Python for Clinical Study Report and Submission"
    ]
  },
  {
    "objectID": "workshops/specialized/python-csr-submission.html#overview",
    "href": "workshops/specialized/python-csr-submission.html#overview",
    "title": "Python for Clinical Study Report and Submission",
    "section": "",
    "text": "Intermediate Python Clinical Reporting\nOpen-source Python offers powerful capabilities for clinical trial analysis and reporting. This workshop introduces practical strategies for preparing tables, listings, and figures (TLFs) in a Clinical Study Report (CSR) and assembling submission-ready eCTD packages.\n\n\n\nğŸ Python environment setup with uv\nğŸ“Š Clinical data engineering with polars\nğŸ“ˆ TLF creation with plotnine and rtflite\nğŸ“¦ eCTD packages with py-pkglite\nğŸ”„ Reproducible workflows end-to-end",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Python for Clinical Study Report and Submission"
    ]
  },
  {
    "objectID": "workshops/specialized/python-csr-submission.html#prerequisites",
    "href": "workshops/specialized/python-csr-submission.html#prerequisites",
    "title": "Python for Clinical Study Report and Submission",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRequired Knowledge:\n\nBasic Python programming\nUnderstanding of clinical trial analysis\nFamiliarity with TFLs\n\nHelpful:\n\nR experience (for comparison)\nCDISC standards knowledge",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Python for Clinical Study Report and Submission"
    ]
  },
  {
    "objectID": "workshops/specialized/python-csr-submission.html#key-tools",
    "href": "workshops/specialized/python-csr-submission.html#key-tools",
    "title": "Python for Clinical Study Report and Submission",
    "section": "Key Tools",
    "text": "Key Tools\n\nPython\n\n\nuv\n\n\npolars\n\n\nplotnine\n\n\nrtflite\n\n\npy-pkglite",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Python for Clinical Study Report and Submission"
    ]
  },
  {
    "objectID": "workshops/specialized/python-csr-submission.html#workshop-materials",
    "href": "workshops/specialized/python-csr-submission.html#workshop-materials",
    "title": "Python for Clinical Study Report and Submission",
    "section": "Workshop Materials",
    "text": "Workshop Materials\n\n\n\n\n\n\nNoteResources\n\n\n\nWorkshop Slides: pycsr.org/slides/workshop-slides.html\nOnline Book: Python for Clinical Study Reports and Submission - pycsr.org\nGitHub: github.com/nanxstats/pycsr\nDevelopment: GitHub Codespaces, VS Code, or Positron",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Python for Clinical Study Report and Submission"
    ]
  },
  {
    "objectID": "workshops/specialized/python-csr-submission.html#workshop-modules",
    "href": "workshops/specialized/python-csr-submission.html#workshop-modules",
    "title": "Python for Clinical Study Report and Submission",
    "section": "Workshop Modules",
    "text": "Workshop Modules\n\nModule 1: Python Environment Setup\nUsing uv for project management:\n# Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create project\nuv init my-clinical-trial\ncd my-clinical-trial\n\n# Add dependencies\nuv add polars plotnine rtflite\n\n# Run scripts\nuv run analysis.py\nBenefits:\n\nâœ… Fast dependency resolution\nâœ… Reproducible environments\nâœ… No conda/virtualenv complexity\nâœ… Lock files for exact versions\n\n\n\nModule 2: Clinical Reporting Packages\n\npolars - High-Performance DataFrames\nimport polars as pl\n\n# Read CDISC data\nadsl = pl.read_parquet(\"data/adsl.parquet\")\n\n# Fast data manipulation\ndemographics = (\n    adsl\n    .filter(pl.col(\"SAFFL\") == \"Y\")\n    .group_by(\"ARM\")\n    .agg([\n        pl.count().alias(\"N\"),\n        pl.col(\"AGE\").mean().alias(\"Age_Mean\"),\n        pl.col(\"AGE\").std().alias(\"Age_SD\")\n    ])\n)\nWhy polars?\n\n10-100x faster than pandas\nLazy evaluation\nBetter memory efficiency\nExpressive API\n\n\n\nplotnine - Grammar of Graphics\nfrom plotnine import *\n\n# Kaplan-Meier plot\n(\n    ggplot(survival_data, aes(x=\"time\", y=\"survival\", color=\"arm\")) +\n    geom_step(size=1) +\n    geom_ribbon(aes(ymin=\"lower\", ymax=\"upper\", fill=\"arm\"), alpha=0.2) +\n    labs(title=\"Overall Survival\", x=\"Time (months)\", y=\"Probability\") +\n    theme_minimal()\n)\nggplot2 equivalent for Python!\n\n\nrtflite - RTF Generation\nfrom rtflite import *\n\n# Create RTF document\ndoc = RtfDocument()\n\n# Add table\ntable = create_table(demographics_df)\ndoc.add_table(table)\n\n# Save\ndoc.save(\"demographics.rtf\")\n\n\n\nModule 3: Complete Project Management\nProject Structure:\nmy-trial/\nâ”œâ”€â”€ data/           # CDISC datasets\nâ”œâ”€â”€ src/            # Python scripts\nâ”‚   â”œâ”€â”€ tables/\nâ”‚   â”œâ”€â”€ listings/\nâ”‚   â””â”€â”€ figures/\nâ”œâ”€â”€ outputs/        # Generated TFLs\nâ”œâ”€â”€ pyproject.toml  # Dependencies\nâ””â”€â”€ README.md\nExecution:\n# main.py\nfrom src.tables import demographics, adverse_events\nfrom src.figures import km_plot\n\n# Generate all outputs\ndemographics.create()\nadverse_events.create()\nkm_plot.create()\n\n\nModule 4: eCTD Submission Packages\npy-pkglite for packaging:\nfrom pkglite import *\n\n# Create submission package\npkg = Package()\npkg.add_directory(\"src/\", pattern=\"*.py\")\npkg.add_directory(\"outputs/\", pattern=\"*.rtf\")\npkg.pack(\"submission.txt\")\n\n# Includes source code + outputs\n# Aligned with eCTD requirements",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Python for Clinical Study Report and Submission"
    ]
  },
  {
    "objectID": "workshops/specialized/python-csr-submission.html#practical-exercises",
    "href": "workshops/specialized/python-csr-submission.html#practical-exercises",
    "title": "Python for Clinical Study Report and Submission",
    "section": "Practical Exercises",
    "text": "Practical Exercises\n\nExercise 1: Demographics Table\nCreate standard demographics table:\n\nAge (mean, SD)\nSex (n, %)\nRace (n, %)\nBy treatment arm\n\n\n\nExercise 2: Adverse Events Listing\nGenerate AE listing with:\n\nSubject ID\nAE term\nStart/end dates\nSeverity\nRelationship\n\n\n\nExercise 3: Survival Analysis Figure\nCreate Kaplan-Meier plot:\n\nSurvival curves by arm\nConfidence intervals\nRisk table\nPublication quality\n\n\n\nExercise 4: Full Submission Package\nAssemble eCTD package:\n\nAll TFLs\nSource code\nDocumentation\nValidation artifacts",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Python for Clinical Study Report and Submission"
    ]
  },
  {
    "objectID": "workshops/specialized/python-csr-submission.html#data-sources",
    "href": "workshops/specialized/python-csr-submission.html#data-sources",
    "title": "Python for Clinical Study Report and Submission",
    "section": "Data Sources",
    "text": "Data Sources\nCDISC Pilot Study:\n\nPublicly available\nStandard structure (SDTM/ADaM)\nRealistic scenarios\nPre-converted to Parquet\n\nLocation: github.com/nanxstats/pycsr/tree/main/data",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Python for Clinical Study Report and Submission"
    ]
  },
  {
    "objectID": "workshops/specialized/python-csr-submission.html#python-vs-r",
    "href": "workshops/specialized/python-csr-submission.html#python-vs-r",
    "title": "Python for Clinical Study Report and Submission",
    "section": "Python vs R",
    "text": "Python vs R\n\nWhen to Use Python\nâœ… Large datasets (polars performance)\nâœ… ML/AI integration needed\nâœ… Team already uses Python\nâœ… Cloud-native deployments\n\n\nWhen to Use R\nâœ… Statistical depth required\nâœ… Established R workflows\nâœ… Pharmaverse ecosystem\nâœ… Regulatory precedent\n\n\nBest Approach\nUse both! Many organizations adopting hybrid:\n\nR for statistical analysis\nPython for data engineering\nShared CDISC data formats",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Python for Clinical Study Report and Submission"
    ]
  },
  {
    "objectID": "workshops/specialized/python-csr-submission.html#learning-outcomes",
    "href": "workshops/specialized/python-csr-submission.html#learning-outcomes",
    "title": "Python for Clinical Study Report and Submission",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nâœ… Set up reproducible Python projects with uv\nâœ… Process clinical data efficiently with polars\nâœ… Create TFLs with plotnine and rtflite\nâœ… Manage A&R projects professionally\nâœ… Prepare eCTD submission packages\nâœ… Understand Pythonâ€™s role in clinical trials",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Python for Clinical Study Report and Submission"
    ]
  },
  {
    "objectID": "workshops/specialized/python-csr-submission.html#resources-1",
    "href": "workshops/specialized/python-csr-submission.html#resources-1",
    "title": "Python for Clinical Study Report and Submission",
    "section": "Resources",
    "text": "Resources\nBook Chapters:\n\nPython Setup and Environment\nEssential Packages for Clinical Reporting\nProject Structure and Workflow\nCreating Tables\nCreating Listings\nCreating Figures\nSubmission Package Assembly\n\nCommunity:\n\npharmaverse-py initiative\nPython in Pharma meetups\nStack Overflow [python] + [clinical-trials]",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Python for Clinical Study Report and Submission"
    ]
  },
  {
    "objectID": "workshops/specialized/python-csr-submission.html#next-steps",
    "href": "workshops/specialized/python-csr-submission.html#next-steps",
    "title": "Python for Clinical Study Report and Submission",
    "section": "Next Steps",
    "text": "Next Steps\n\nComplete pycsr.org tutorials\nTry on your own data\nExplore pharmaverse-py\nContribute to open-source Python clinical tools\n\n\n\nSimilar Workshops\n\nPolars: Python Framework - Deep dive on polars\ndatasetjson - Data exchange\n\n\n\nNext Steps\n\nR equivalent: See officer/flextable and Cardinal\n\n\nLast updated: November 2025 | R/Pharma 2025 Conference",
    "crumbs": [
      "Workshops",
      "ğŸ§¬ Specialized Applications",
      "Python for Clinical Study Report and Submission"
    ]
  }
]