# R-Pharma-2025-Workshops

### **WORKSHOP: Flexible Clinical Trial Design, Simulation, and Analysis with the R Package rpact**

**Daniel Sabanes Bove** Entrepreneur in Biostatistics Consulting and Engineering, RCONIS

**Friedrich Pahlke** Tech Entrepreneur \| Driving Innovation in R/Shiny

In this workshop we will explore the capabilities of the validated open-source R package 'rpact', which is available on CRAN and GitHub. rpact is a comprehensive validated, open source, free-of-charge R software package for clinical trial planning, design simulation, and data analysis. rpact is under continuous full-time development since 2017, and comprehensive documentation is available at [www.rpact.org](http://www.rpact.org/). We will provide an introduction to the package and illustrate the usage by several examples. The focus of the package is on group sequential and adaptive designs with p-value combination tests, but also fixed sample designs can be considered. The software can specifically be used to assess design characteristics of popular group sequential designs. However, going beyond those, adaptive designs are a strength of rpact: Adaptive multi-armed and population enrichment designs that are based on closed combination tests can be assessed by simulation. The application of the designs for simulation, real data, and estimation is possible for continuous, binary, survival, and count data. Furthermore, we introduce 'RPACT Cloud', a user-friendly platform designed to simplify and enhance the process of clinical trial design and simulations for researchers and practitioners. A free version of RPACT Cloud is available at <https://cloud.rpact.com/> and workshop participants will be able to try out the software themselves following the workshop.

Link to the presentation: <https://rpharma.presentation.2025.rpact.com/>

### **WORKSHOP: Debugging Stan Programs**

**Daniel Lee**

Data Scientist, Bayesian statistician, Stan developer

Having trouble getting your Stan models to behave? This hands-on workshop will explore practical strategies for debugging Stan code, from identifying non-identifiable parameters to improving runtime performance. Participants will gain tools and intuition for building more robust models—skills that extend to a wide range of statistical and stochastic programming tasks.Prerequisite: Familiarity with Stan is recommended.

Workshop related link: <https://github.com/bayesianops/stan-tutorials/tree/main/debugging-2025-11-03>

### **WORKSHOP: Introduction to building (better) R packages**

**Nicola Rennie**

Data Visualisation Specialist

There are many benefits to turning your R scripts or functions into a package, like making your code easier to re-use, easier to share with others, easier to document, and easier to test. But the process of writing a package can feel intimidating, especially if you haven't done it before. But it doesn't need to! In this interactive workshop, we'll discuss:\* What things you need to make a package and how to create them\* How to write functions (in a package-friendly way) and add them to a package\* How to write documentation and examples for functions\* Best practices for package development\* How to share your package with other people\* Useful resources. By the end of this workshop, you will have made an R package! The session aims to be introductory, so you don't need any previous experience of building R packages (or even writing functions!) but some basic knowledge of R will be useful. If you're already a seasoned R package developer, you're also welcome to attend and hopefully you'll still pick up a few package development tips!

Workshop related link: <https://nrennie.rbind.io/r-pharma-2025-r-packages/#/title-slide>

### **WORKSHOP: Getting Started with LLM APIs in R**

**Sara Altman**

Data Science Educator

Posit PBC

LLMs are transforming how we write code, build tools, and analyze data, but getting started with directly working with LLM APIs can feel daunting. This workshop will introduce participants to programming with LLM APIs in R using ellmer, an open-source package that makes it easy to work with LLMs from R. We’ll cover the basics of calling LLMs from R, as well as system prompt design, tool calling, and building basic chatbots.

No AI or machine learning background is required—just basic R familiarity. Participants will leave with example scripts they can adapt to their own projects.

Workshop related link: <https://skaltman.github.io/r-pharma-llm/>

### **WORKSHOP: Hands on with Cardinal: Harmonizing Clinical Reporting with Open‑Source TLGs**

**Abinaya Yogasekaram**

Bioinformatics \| Software Development \| Data Science

Amaris Consulting

**Emily De La Rua**

Data Scientist at Roche

Cardinal is an open-source collection of standardized table, listing, and graph (TLG) templates designed to streamline the process of clinical output review, comparison, and meta-analysis—promoting efficient communication to stakeholders while aligning with CDISC’s ARD/ARM efforts.

In this workshop, we’ll introduce the {gtsummary} package for table creation and walk through Cardinal’s growing catalog of TLGs. Participants will get hands-on experience generating key outputs—ideally working on examples that are directly relevant to your own work.

Whether you're new to Cardinal, looking to integrate it into your own workflow, or interested in contributing to the project, this workshop will provide practical insights and real-world applications for modern clinical reporting.

Workshop related link: <https://pharmaverse.github.io/cardinal/>

### **WORKSHOP: Advanced Clinical Reporting with officer and flextable**

**David Gohel**

[**Advanced Clinical Reporting with officer and flextable**](https://rinpharma.com/docs/RPH2025/#)

In this workshop you will be working with the officer and flextable packages to create sophisticated clinical reports in Word format using R with a reproducible approach. Going from clinical data all the way through to complete pharmaceutical report generation. Specifically, we will walk through an end-to-end example focusing on advanced document structure management, complex table creation following pharmaceutical standards, and integration of ggplot2 visualizations. The workshop is divided into two parts: first, discovering the fundamentals of officer and flextable packages, then moving to advanced clinical reporting techniques including section management, headers and footers customization, and cross-reference handling for complete pharmaceutical report composition.

Workshop related links:

<https://ardata.fr/r-in-pharma-2025/>

<https://github.com/ardata-fr/r-in-pharma-2025-codes>

<https://github.com/ardata-fr/r-in-pharma-reporting-with-officer-flextable>

### **WORKSHOP: Creating Polished, Branded Documents with Quarto**

**Isabella Velásquez**

Sr. Product Marketing Manager

Posit PBC

Join us for a hands-on, one-day workshop at R/Phama, where we’ll explore the versatility of Quarto output formats. You will learn how to create dynamic websites, professional PDF documents, engaging presentations, and interactive dashboards using Quarto. This workshop highlights Quarto's powerful theming capabilities, including the new support for brand.yml, which ensures that your work maintains a professional and cohesive style across all formats.

Workshop related links:

<https://019a4f2d-6b79-72c1-834b-c2a9488f9ec8.share.connect.posit.cloud/>

<https://github.com/ivelasq/2025-11-04_branded-quarto>

### **WORKSHOP: Polars: The Blazing Fast Python Framework for Modern Clinical Trial Data Exploration**

**Michael Chow**

Principal Software Engineer at Posit (Open Source)

Posit

**Jeroen Janssens**

Head of Developer Relations

Posit PBC

Clinical trials generate complex and standards driven datasets that can slow down traditional data processing tools. This workshop introduces Polars, a cutting-edge Python DataFrame library engineered with a high-performance backend and the Apache Arrow columnar format for blazingly fast data manipulation. Attendees will learn how Polars lays the foundation for the pharmaverse-py, streamlining the data clinical workflow from database querying and complex data wrangling to the potential task of prepping data for regulatory Tables, Figures, and Listings (TFLs). Discover the 'delightful' Polars API and how its speed dramatically accelerates both exploratory and regid data tasks in pharmaceutical drug development. The workshop is led by Michael Chow, a Python developer at Posit who is a key contributor to open-source data tools, notably helping to launch the data presentation library Great Tables, and focusing on bringing efficient data analysis patterns to Python.

Workshop related link:

<https://github.com/machow/examples-great-tables-pharma>

### **WORKSHOP: How to use pointblank to understand, validate, and document your data**

**Rich Iannone**

Software Engineer at Posit PBC

This workshop will focus on the data quality and data documentation workflows that the pointblank package makes possible. We will use functions that allow us to:

\(1\) quickly understand a new dataset

\(2\) validate tabular data using rules that are based on our understanding of the data

\(3\) fully document a table by describing its variables and other important details. The pointblank package was created to scale from small validation problems (“Let’s make certain this table fits my expectations before moving on”) to very large (“Let’s validate these 35 database tables every day and ensure data quality is maintained”) and we’ll delve into all sorts of data quality scenarios so you’ll be comfortable using this package in your organization. Data documentation is seemingly and unfortunately less common in organizations (maybe even less than the practice of data validation). We’ll learn all about how this doesn’t have to be a tedious chore. The pointblank package allows you to create informative and beautiful data documentation that will help others understand what’s in all those tables that are so vital to an organization.

Workshop related link:

<https://github.com/rich-iannone/pointblank-workshop>

#### **Europe/US Session #1**

### **Practical AI for data science**

Simon Couch (Posit). Practical AI for data science

While most discourse about AI focuses on glamorous, ungrounded applications, data scientists spend most of their days tackling unglamorous problems in sensitive data. Integrated thoughtfully, LLMs are quite useful in practice for all sorts of everyday data science tasks, even when restricted to secure deployments that protect proprietary information. At Posit, our work on ellmer and related R packages has focused on enabling these practical uses. This talk will outline three practical AI use-cases—structured data extraction, tool calling, and coding—and offer guidance on getting started with LLMs when your data and code is confidential.

<https://github.com/simonpcouch>

#### **Europe/US Session #2**

### **duckplyr: Analyze large data with DuckDB and dplyr compatibility**

Kirill Muller (Cynkra).

The duckplyr package is now stable; version 1.1.2 is on CRAN. It builds on top of DuckDB, a fast and flexible analytical engine that can work with larger-than-memory data from disk or cloud storage. All these features are available to duckplyr, with syntax and semantics much closer to R and the tidyverse. Use it to speed up existing code, to analyze Parquet or CSV files directly, or to access the plethora of exciting functionality provided by DuckDB.

### **Beyond Training: Evolving Strategies to Teach and Support R Adoption in Pharma**

Alanah Jonas (GSK).

Teaching R effectively is just the starting point for lasting adoption in pharma. At GSK, we began by developing two core R courses written in bookdown and delivered interactively, an approach that enabled high completion rates.

Recognizing that training alone isn't enough, we expanded support through options like self-certification, intermediate courses, and a Resource Hub. We also launched AccelerateR to provide tailored support to early adopters during their transition.

Today, those efforts have scaled and evolved into Rburst, a cross-functional initiative designed not just to teach R, but to embed it into our ways of working across all of Biostatistics.

This talk will explore how our training strategy has matured into a broader support model, enabling wide-scale, integrated adoption of R in everyday work.

### **Validating Shiny Apps in Regulated Environments**

Pedro Silva (Jumping Rivers).

Shiny apps are increasingly used to deliver interactive tools in clinical and healthcare settings. But when these tools are used in regulated environments, validation becomes essential. How can we ensure that our Shiny apps are trustworthy, without stifling innovation?

In this talk we’ll explore practical approaches to validating Shiny applications in regulated contexts, drawing on principles from software engineering, quality assurance, and risk based validation. We’ll discuss key challenges like traceability, documentation, and versioning, as well as share techniques for building apps that are easier to validate from the start.

I’ll highlight some of the tools and packages used in Jumping Rivers that can support validation workflows that satisfies both internal reviewers and external regulators, including the Litmusverse, a suite of R packages designed to assess code quality and generate validation evidence.

By the end of the session, you’ll understand: - Why validation is critical for Shiny apps in regulated contexts; - What elements make a Shiny app more or less validatable; - How to incorporate validation strategies into your development process.

This session is ideal for R users in pharma, clinical research, and healthcare who want to build confidence in their dashboards, while maintaining flexibility in how they work.

### **Beyond {gtsummary}: How the {crane} Package Extends the Framework for Pharmaceutical Reporting**

Daniel Sjoberg (Genentech) and Davide Garolini (Genentech).

{gtsummary} has become the most-used R package for clinical tables in the R ecosystem, winning awards from the American Statistical Association and Posit. Building on this foundation, we created {crane}, an open-source extension that facilitates reporting requirements in the pharmaceutical space. Because {crane} and {gtsummary} are built upon Analysis Results Datasets (ARDs), study teams can take advantage of this foundation that streamlines the QC process and allows for simple re-use of calculated results in subsequent reporting.

In this session we show how: (1) a vanilla {gtsummary} script instantly upgrades when {crane} is loaded; (2) ARD-based outputs make QC a straightforward operation; (3) LLMs can summarise the language-agnostic ARD results for medical writers in seconds. You’ll leave with a blueprint of how to adapt {crane} for your reporting needs or (or roll your own) and shorten table turnaround on day one.

<https://www.danieldsjoberg.com/RinPharma-crane-2025/>

<https://github.com/ddsjoberg/RinPharma-crane-2025/tree/main>

<https://github.com/cynkra/r-in-pharma-2025/>

<https://docs.google.com/presentation/d/1atV5rBLQF2vcCg2lraiutg_Xx2BOEklZzf9LJt842ig/edit?usp=sharing>

#### **Europe/US Session #3**

### **{llumen}: An agentic LLM framework for biomedical documents, databases & foundation models**

Sven-Eric Schelhorn (lumen).

We introduce {llumen}, an R package used at Merck KGaA, Darmstadt, Germany that implements an agentic framework bases on {ellmer} and enables pharmaceutical researchers to work with biomedical documents, large real-world evidence and biomarker databases, as well as with biological foundation models using large language models as orchestrators.

{llumen} currently supports the following functionalities:

-   *Large language models*\*

-   Runs on standard company laptops (Windows, Linux, or macOS).

-   Allows using a wide range of private (Azure), public (OpenAI, Anthropic, Bedrock), and local (llama.cpp) LLMs.

-   Provides both programmatic and fully interactive (chatbot) ways to interact with the LLM.

-   Supports chain-of-thoughts, tree-of-thoughts, and ReAct reasoning strategies.

-   Supports context management to maintain conversation history and state.

-   *Text analysis*\*

-   Supports embeddings (Azure, public, or local) using a local, high-performance vector DB (based on {ragnar}).

-   Implements chunking office documents (Word, Excel, Powerpoint, RTF, PDF) into the vector DB.

-   While reading office documents, supports extracting tables with summary data.

-   Enables retrieval-augmented generation (RAG) using the vector DB, either explicitly or by using a RAG tool that the LLM calls.

-   Emulates the Future House's PaperQA2 approach for summarizing and re-ranking parts of semantically related documents so that answer quality and comprehensiveness is significantly boosted.

-   Extracts text from documents to fill structured templates, such as the JSON-based {llumen} Analysis Results Schema (LARS). Extracted JSONs are directly validated by the LLM using an R tool.

-   Supports automatic diagram generation from texts, for instance to capture the main concepts of a paper in two or three flow charts.

-   Provides advanced text preprocessing and cleaning capabilities for various document types.

-   Offers customizable tokenization and text segmentation methods.

-   *Agentic tool use and codegen*\*

-   Allows the LLM to search the internet via Google and access the textual content of websites in a controlled manner. Also includes specialized query tools for Pubmed, Pubchem, and Wikipedia.

-   Enables the LLM agent to query Parquet files, Neo4j knowledge graphs, and GraphQL APIs using SQL, Cypher and GraphQL codegen in a secure context. The database agents are very capable and can answer complex, open-ended scientific questions using these data sources.

-   Enables the LLM agent to use local foundation models, such as Google's TxGemma model for pharmaceutical applications, for instance for predicting solubility of small molecules.

-   Uses multimodal LLMs to accurately diagnose cancer histopathology images with very comprehensive pathology reports that investigate architectural and cytological Features of H&E slides.

-   Makes it easy to give the LLM new R tool capabilities (basically all R functions), share tools via MCP (built-in pure R MCP server), or utilize tools of external MCP server as internal tools (via a MCP tool wrapper).

-   Supports dynamic tool creation and registration during runtime.

-   Provides a framework for defining custom tools and integrating them seamlessly with the LLM.

We are internally using the package for the following usecases:

-   *Supported usecase*\*

1.  Extracting results of statistical analyses from office documents: The vignettes show how to configure an agentic LLM to extract chunks oftext from documents and tables, semantically query a vector store (agent-based retrieval-augmented generation, RAG), load JSON definitions, extract JSON-structured data from documents, and validate these definitions against a user-defined JSON template. {llumen} can be utilized to extract results data from a large corpus of clinical trial results files. Templates for both CDISC Analysis Result Standard (ARS) and the {llumen} Analysis Result Schema (LARS) are provided.

2.  Querying tabular databases and knowledge graphs: {llumen} excels at SQL codegen, allowing users to query very large Parquet files located on the user's device and perform complex data analyses. It also supports querying Neo4j knowledge graphs and GraphQL APIs, enabling comprehensive data exploration and analysis.

3.  Working with biological foundation models: {llumen}'s ability to use local foundation models specialized for particular biological or pharmaceutical applications, such as Google's GemmaTX, enhances agentic LLMs with specialized capabilities for tasks like predicting molecular properties or analyzing biological pathways.

4.  Automated literature review and summarization: {llumen} can be used to perform comprehensive literature reviews by searching multiple databases (e.g., PubMed, OpenAlex), extracting relevant information, and generating structured summaries of findings.

5.  Histopathology image analysis: The package supports the use of multimodal LLMs for analyzing histopathology images, providing detailed reports on architectural and cytological features of H&E slides, which can aid in cancer diagnosis and research.

6.  Drug discovery and development support: By integrating various tools and databases (e.g., PubChem, TxGemma), {llumen} can assist in various stages of drug discovery, from initial compound screening to predicting drug properties and potential interactions.

7.  Clinical trial data analysis and reporting: {llumen}'s capabilities in extracting and structuring data from various document types make it well-suited for analyzing and reporting on clinical trial data, potentially accelerating the process of deriving insights from trial results.

8.  Biomedical knowledge graph exploration: The package's ability to query knowledge graphs allows researchers to explore complex relationships in biomedical data, potentially uncovering new insights or research directions.

9.  Automated scientific writing assistance: {llumen} can be used to assist in scientific writing tasks, such as generating literature summaries, creating structured abstracts, or even drafting sections of scientific papers based on analyzed data and literature.

10. Regulatory document preparation and analysis: The package's capabilities in extracting and structuring information from various document types can be applied to preparing and analyzing regulatory documents in the pharmaceutical industry

Presentation related link:

<https://drive.google.com/file/d/1g0_O_wSoeBHzeXl6DF3-NIY7gT2Bk62_/view>

### **Build Model Context Protocol servers and clients in R**

John Coene (Opifex).

The mcpr package provides a comprehensive R implementation of the Model Context Protocol (MCP), a standardized JSON-RPC 2.0 interface that enables R applications to expose computational capabilities to AI models. This package bridges the gap between R's statistical computing environment and modern AI assistants by allowing R developers to create MCP servers that expose tools, resources, and prompts as well as client functionality to interact with existing MCP servers.

Key features include schema-based tool definitions, multi-modal response support, and seamless integration with popular AI development environments including Claude Code, Cursor, and VS Code. This implementation democratizes AI-R integration by providing a standards-based approach to exposing R's extensive ecosystem to conversational AI interfaces, opening new possibilities for interactive data analysis and statistical computing workflows.

#### **Europe/US Session #4**

### **Mosaic: Open-Source, ARS-Driven Automation of Standard TFLs**

Conor Moloney (Novartis).

CDISC standards help streamline datasets (e.g. SDTM and ADaM), yet analysis results lack a common model. The emerging Analysis Results Standard (ARS) will help to fill this gap with machine-readable specifications. Mosaic couples ARS with an open-source stack to generate fully validated standard TFLs, fast and reproducibly.

How it works

Mosaic captures the Analysis Results Dataset (ARD) requirement in ARS-aligned YAML; updates and changes are made by editing this metadata, not code. After LinkML validation, a Python layer stores the YAML in a database via a SQL Alchemy ORM engine. The results-generation layer is intentionally language-agnostic . The rules that transform ADaM datasets into the ARD are expressed as metadata that can be rendered in any statistical language. Mosaic’s implementation uses R to derive the ARD.

A React UI reads the validated ARD. Here, users have controlled customisation options. Users can preview each TFL shell in real time and export regulator-ready RTF files suitable for submission packages.

Mosaic replaces ad-hoc programming with a transparent, standards-based pipeline, accelerating TFL delivery while safeguarding traceability and regulatory compliance. This reallocates programmer effort from repetitive coding to high-value scientific review

### **Integrating Collaborative Programming with Automated Traceability and Reproducibility in Pharma**

Jennifer Dusendang (Graticule Inc) and Sundeep Bath (Graticle Inc).

Integrating Collaborative Programming with Automated Traceability and Reproducibility in Pharma Studies and Real-World Data Projects by Adapting DevOps Best-Practices

To enhance integrity of research and study findings, data scientists should ensure that studies are traceable and reproducible, which involves meticulous management of datasets, tracking code changes, and robust storage of results.

Without infrastructure to support reproducibility efforts, documentation, dependency management, and version control processes can be manual, unreliable, and unclear. This creates problems with determining when analysis changes occurred, which version of study results were produced by which version of code, and whether all study steps are processed in proper order and appropriately documented.

Implementing procedures and technical infrastructure helps to maintain and automate reproducibility and traceability. To ensure that code can be executed consistently across multiple compute environments, we structure analysis scripts into parameterized pipelines within an isolated Docker container environment which specifies all versions and dependencies. We integrate Continuous Integration (CI) and Continuous Delivery (CD) into analysis pipelines to enable automatic rerunning of analyses following code modifications and storage of results in the cloud. Our process integrates and improves collaborative programming by providing code reviewers with the validated outputs that are produced by the code. By design, study close-out and compliance activities are incorporated within our infrastructure.

In this paper we will discuss how we implemented and adapted DevOps best-practices like CI/CD in a collaborative coding environment to work for epidemiological studies and real-world data projects. Although the concepts discussed are applicable to many tools, our implementation uses Git, GitHub Actions, SQL, Python, R, Docker, and AWS S3. This content is applicable for all skill levels.

<https://www.lexjansen.com/pharmasug/2025/OS/PharmaSUG-2025-OS-111.pdf>

### **TabPFN: A Deep-Learning Solution for Tabular Data**

Max Kuhn (Posit).

There have been numerous proposals for deep neural networks for tabular data, such as rectangular data sets (e.g., data frames). To date, none have really worked well and take far too long to train. TabPFN is a model that emulates a Bayesian approach and trains a deep learning model on a prior of simulated tabular datasets. Version 2 was released this year and offers several significant advantages, but also has one notable disadvantage. I'll introduce this model and show an example.

<https://topepo.github.io/2025-r-pharma/#/title-slide>

### **The LLM Lounge: Live-coding and conversation on using AI for data science**

**Joe Cheng**

Posit

**Eric Nantz**

Eli Lilly

The number of workflows and tools leveraging large-language-models (LLMs) is growing exponentially across the world, and the life sciences industry is no exception. In this session, Eric Nantz takes the role of the curious and savvy data scientist as he is joined by Posit CTO Joe Cheng in a special live demonstration on using Databot to power an exploratory data science analysis. Throughout the demonstration, Joe will share the fascinating origin story of how Databot came to be, along with a deeper conversation on the recent trends and guiding principles of harnessing AI tools for data analyses and software engineering. This interactive, spontaneous format will allow the audience to submit questions and steer the direction of the session, making it a truly engaging, shared experience.

<https://github.com/posit-dev/querychat>

### **LLM-Powered {gtsummary}: QC-Ready Clinical Tables in Minutes**

**Davide Garolini**

Data Scientist, NEST, Roche

He significantly contributes to the NEST project by improving key tools like {gtsummary} and {rtables} for regulatory compliance.

Open-source R tooling has made table generation easy, yet interpreting the code and guaranteeing bullet-proof QC can still delay submissions. We present a workflow that fuses {gtsummary} for rapid table building with cards for "automatic" validation, topped by a fully offline, model-agnostic LLM helper. Starting on synthetic CDISC-like data, we create a standard summary table, validate it with {cards}, and ask the LLM-helper to explain each step and result and propose, if necessary, next steps—all without revealing trial data. A descriptive log is generated on the fly, combining code, QC results, and LLM explanations in one auditor-friendly document. When real data arrive, the same script reruns unchanged, delivering submission-ready tables in minutes while boosting clarity and compliance.

### **Post-Approval Drug Exposure Estimation Using an R Shiny App**

**Feifei Yang**

AstraZeneca

**Yu Zhang**

AstraZeneca

**Background**

Regulatory authorities require product post-marketing exposure estimates in aggregate safety reports for purposes of signal detection. The calculation of exposure estimates requires multiple data sources between sales data and aggregated de-identified patient data. Traditionally, the calculations were manually handled due to limited countries, products, and indications. With the growth of both markets and products, manual calculations are inefficient to handle the increasing volume of data and regional requests. An approach to automating the entire process has become essential.

**Method**

R Shiny App is a web application framework that allows us to create interactive web applications for data analysis, data visualization, and interactive reports, making it easier to share data insights with others. We converted monthly patient and sales report into an app-readable format and applied an exposure estimate algorithm to perform calculations by region, product, and indication.

**Results**

The time to generate a standard report of post-marketing exposure data for regulatory purposes has been reduced from a week to one day. The QC function of application helps check inconsistent numbers from month to month in sales and patient data. In addition to the exposure report, the visualization provided by the application shows trends in sales, patient utilization of products, and exposure by year and geographic regions. The sharing ability of the application enables other team members to generate customized report directly without any prerequisite training.

**Conclusion**

The application significantly improves the efficiency of delivering post-marketing exposure data and reduces time and resources needed for data management and QC. Accessibility to the application helps disseminate the information to a broader range of teams and functions.

### **Implementing an end-to-end NCA software using Shiny**

**Gerardo Jose Rodriguez**

Lucid Analytics

**Jana Spinner**

Lucid Analytics

Pharmacokinetic studies are essential yet demanding, typically relying on expensive proprietary software for Non-Compartmental Analysis (NCA). aNCA is an alternative developed collaboratively by Roche, Lucid Analytics, Appsilon, and Human Predictions. It is being developed within Pharmaverse, aiming to become a worldwide standard by utilizing open source development to make the process more efficient, economical, and transparent. The app performs an end-to-end approach, using interactive plots for data exploration, half life customization, TLGs, and report generation.aNCA is planned to be released on CRAN soon. It currently reaches 100% of testing coverage on its functions and has successfully passed internal package validations.

aNCA uses PKNCA, a widely used package that calculates over 200 PK parameters, has been tested against other industry-standard NCA software, showing differences within ±0.1% for most tested parameters.

We aim to present aNCA, highlight its innovative software concepts, discuss how we tackled key challenges, and showcase how open source and Shiny achieve industry-standard PK software. However, we welcome suggestions on focus areas for the R/Pharma audience.

If you want to know more about aNCA, feel free to follow the link to our GitHub: <https://github.com/pharmaverse/aNCA>

### **Integrating LLM using R Shiny for Clinical Data Review by Ensuring Data Privacy and Validity**

**Zhen Wu**

CIMS Global

**Peng Zhang**

CIMS Global

The pharmaceutical industry is shifting from traditional SAS-based workflows toward the open-source R ecosystem. R Shiny applications have become a popular solution for visualizing tables, figures, and listings. However, these applications often require a strong understanding of data structures and familiarity with interface components such as dropdowns, which may not be intuitive for clinical reviewers. Recent advancements in artificial intelligence (AI), particularly large language models (LLMs), have opened new possibilities for how users interact with clinical data. In this session, we present an innovative R Shiny application, {DataChat}, that enables users to "chat with data" through a conversational interface. Powered by the {ellmer}, {shinychat}, and {ragnar} packages, along with internal statistical tools and utilities, the app integrates retrieval-augmented generation (RAG) capabilities tailored to the pharmaceutical domain. The solution emphasizes user-friendliness, enabling non-programmers and clinicians to explore datasets and derive insights while maintaining compliance with data privacy requirements and addressing concerns around statistical validity. This approach exemplifies the potential of AI-augmented tools to enhance clinical data review and exploration in a practical and accessible way

### **R we there yet? {admiral}’s journey transitioning from active development to stability**

**Edoardo Mancini**

Roche

If you are attending R/Pharma 2025, you probably enjoy actively contributing to open source R packages for the pharma industry. Fixing bugs, implementing new features and expanding the use-case is work that feels motivating and meaningful. However, what happens if/when your package reaches a point of stability and maturity, where the main use-case is answered and feature completeness is in sight? How can you sustain your development team's momentum without chasing perfection, in a time where the active workload is naturally diminished? What should your new priorities be?

While there is no definitive answer to any of these questions, this talk will discuss the evolution of packages like {admiral} as they transition from active development to mature maintenance, drawing insights from the {admiral} team's experiences in navigating this complex shift.

### **Workshop: R-Classification: Unleashing Predictive Power with tidymodels**

-   **Harshavardhan Bajoria**

Dive into the exciting world of classification with R and the elegant tidymodels framework! This hands-on workshop provides a comprehensive introduction to building, evaluating, and refining machine learning models that predict categorical outcomes. You'll learn to preprocess your data effectively using recipes, split datasets for robust model training and testing with rsample, define and fit various classification algorithms using parsnip, and assess model performance using a suite of metrics from yardstick. Through practical exercises, including a wine classification challenge, you'll gain the skills to tackle real-world predictive problems and make data-driven decisions using the consistent and intuitive tidymodels ecosystem.

### **Workshop: A Guided Tour to Building and Integrating LLM Based Tooling with R**

**Devin Pastoor**

Chief Technology and Product Officer, A2-AI

**Xu Fei**

Xu Fei is a Senior Solutions Engineer at A2-Ai, where he builds AI-powered tools and infrastructure for pharmaceutical research workflows. Working across R and Python stacks, he has developed LLM-enabled applications ranging from interactive chatbots to MCP server implementations, with a focus on making GenAI accessible and practical for scientific computing teams. His work bridges enterprise DevOps, cloud APIs (AWS Bedrock), and domain-specific R and Python packages to help scientists integrate AI capabilities into their existing workflows

**Aathira Anil Kumar**

A2-Ai

Join A2-AI engineers, Aathira Anil Kumar, Devin, and Xu Fei, for a practical, 2-hour workshop demonstrating how to integrate Generative AI (GenAI) into pharmaceutical workflows. This session focuses on bridging the R and Python ecosystems to deliver scalable, GxP-compliant solutions.

You will learn methodology for developing LLM-enabled applications—from interactive chatbots for clinical study reporting and SOP management, to more complex MCP server implementations for reproducible analytics. Drawing on their extensive experience with life-science organizations and tools like AWS Bedrock, the instructors will showcase how to navigate real-world IT constraints while making GenAI accessible and practical for scientific computing teams. This workshop is essential for analysts, developers, and project leaders aiming to stand up GxP-compliant statistical computing environments with integrated AI capabilities.

#### **Europe/US Session #5**

### **GSK’s journey to Clinical Study Reporting Using Open Source**

**Sam Warden**

VP & Global Head, Clinical Programming & Business Excellence

GSK

**Tim Colman**

GSK

GSK’s Journey to Clinical Study Reporting Using Open Source chronicles the transformation of clinical reporting at GSK, the presentation marks pivotal moments in the pharmaceutical industry’s evolution—from the proprietary, SAS-dominated era of the 1980s and 90s, through the pain of rising R&D costs and global health crises, to the emergence of collaborative models and open-source innovation.

The story highlights how industry challenges catalyzed a shift toward openness and collaboration. Key milestones include the FDA’s clarification on software neutrality, the rise of R and open-source platforms, and GSK’s commitment to writing at least 50% of code in open-source languages. The COVID-19 pandemic accelerated this transformation, driving rapid adoption of data science platforms and collaborative tools.

GSK’s multi-wave approach to change is shared and candidly addresses ongoing challenges: technical validation, regulatory uncertainty, cultural resistance, and the need for robust governance and training.

Ultimately, the journey demonstrates that embracing open-source technologies enables greater automation, and innovation in clinical study reporting. Success depends on a growth mindset, adaptability, and a vision for collaborative transformation—qualities that GSK continues to foster as it leads the industry into a new era of transparency and shared progress.

#### **Europe/US Session #6**

### **Leveraging ellmer and GPT to Integrate AI Agents into Shiny Applications for Accelerating Trials**

**Xing Chen**

Moderna

**Xiaolin Chang**

Moderna

Background:

Artificial intelligence is transforming drug development, but its integration into biostatistics and clinical workflows remains limited. Clinical and biometrics teams often face barriers in exploring high-dimensional, multi-view trial datasets, requiring technical expertise to extract insights and generate visualizations.

Methods:

We developed an AI-enhanced R Shiny application that integrates ellmer with ChatGPTs. The app connects to Cellular-Mediated Immunogenicity (CMI) data across multiple mRNA infectious disease programs. Natural language queries are translated into structured R operations, through which users can interactively explore and visualize data, eliminating the need for manual coding.

Results:

The application successfully handled unstructured user queries, generated targeted outputs, and produced customizable visualizations. Pilot deployment across infectious disease programs demonstrated faster extraction of trial insights, reduced dependency on ad-hoc programming

support, and invigorates collaboration between statisticians, clinicians, and translational researchers.

Conclusions:

Embedding AI agents via ellmer within Shiny applications provides a scalable, user-friendly framework for accelerating exploratory analyses in vaccine development. This approach demonstrates how AI-assisted analytics can increase efficiency, strengthen cross-functional decision-making, and support broader adaptation of GenAI into clinical development workflows.

### **autoslideR: Streamlining slide deck generation for clinical reporting events**

Yolanda Zhou (Roche) and Joe Zhu (Roche).

The standard process of developing the slide deck for clinical reporting events includes manually populating numbers from static outputs and a separate QC. The process is time-consuming, resource-intensive, and error-prone. To address these issues, we created “autoslideR”, an R package to automate the slide deck generation for multiple clinical reporting events. autoslideR has successfully supported slide creation for several study endorsement meetings, IMC meetings, as well as dose escalation meetings for early development, saving teams 0.5 to 4 days for the slide deck preparation compared to the time it traditionally takes.

This talk explores the key features and technical advantages of autoslideR, such as supporting customized layout creations from existing templates, as well as adding placeholder slides to accelerate final slide preparation.

<https://pharmaverse.github.io/examples/digit_files/autoslider.html>

### **Putting the 'R' in RWD**

Sachin Heerah (Pfizer) and Darren Jeng (Pfizer).

The Pfizer Real World Data (RWD) programming team has leveraged R and Posit services to enhance the capabilities of its programmers. We have designed an R package, Shiny apps and even a Quarto website to support all programmers with varying backgrounds, including those with only SAS experience. Our R package is designed to simplify database queries, utilize both R and SAS variable syntax, and standardize deliverables. We have leveraged Posit’s RStudio features such as code snippets to make code templates readily accessible for all users within the IDE. Code guides are also presented as snippets to allow all users to load example data and explore standard RWD programming workflows. Overall, embracing and leveraging the features available to us in R and Posit is enhancing our workflow through integrated resources and documentation.

### **Generating Synthetic Data with synthpop in R**

Sophie Furlow (Abbot Diagnostics).

This talk will introduce synthetic data as an emerging tool for research and production. We will discuss the differences between synthetic, simulated, and resampled data and cover the current state of the art of synthetic data in healthcare, paying special attention to applications in pharma and diagnostics. We will walk through the basics of synthpop, an R package designed to generate synthetic and anonymous data at the individual level using various machine learning algorithms. Viewers will learn how to create entirely synthetic datasets with minimal statistical distortion their original data, making it suitable for software testing, data sharing, and model training. The talk will end with a demonstration of synthpop's quality evaluation features and major caveats to consider during the generation process.

#### **Europe/US Session #7**

### **GenAI in Production - moving beyond prototypes**

**Devin Pastoor**

**Chief Technology and Product Officer, A2-Ai**

Creation of AI applications has become increasingly commoditized through a rich ecosystem of R and python packages. However, there are significant hurdles to take a prototype application into "production" and keep it operating. Traditional testing and validation approaches do not always apply directly and the flexibility from which users can interact with the application can be much larger than a traditional application. This talk will discuss how to successfully (and unsuccessfully) develop, test, release, and maintain Generative AI based applications in GxP contexts.

### **Building the Ultimate R AI Assistant**

**Pawel Rucki**

Principal Data Scientist, Roche

Using R for clinical programming can be challenging, particularly with the specialized, often internal, packages used for reporting. To tackle these issues, we built our own AI assistant.

In this talk, I'll share our story of building a multi-agent R co-pilot with the LangGraph framework, including the lessons we learned and a few tricks we picked up. A key decision was giving each R package its own AI agent. This was a game-changer for getting good help on our internal packages where general-purpose LLMs just can't keep up.

I'll discuss our agent network architecture and the practical techniques used to give agents the right context to write, debug, and explain complex R code for clinical trials. I’ll wrap up with a live demo showing how our design solves real-world programming problems, speeding up development and leading to better, more reliable code.

(agents, cursor via mcp, webR extenstion, admiralroche, air AI assistant for programming in R)

#### **Europe/US Session #8**

### **The Dependency Whisperer: AI That Sees What You Might Miss**

**Ming Yan**

Eli Lilly

**Vina Ro**

Sr. Clinical Data Analyst, Eli Lilly

An ongoing challenge in clinical programming is ensuring that downstream analyses are accurately refreshed following updates to SDTM or ADaM datasets. Traditional tools like AstroGrep can locate references to specific datasets or variables, but they lack the ability to distinguish between active code and commented text. Moreover, identifying indirect dependencies often requires multiple searches and manual effort to document all affected areas, which is time-consuming and increases the risk of missing updates—potentially leading to incorrect outputs being shared with external parties.

This paper introduces an AI-powered tool designed to automatically identify and visualize all datasets and variables impacted by upstream changes. By parsing SDTM, ADaM, and TFL specifications or programs, the tool learns the structure of dependencies across the analysis pipeline. When a user specifies an updated variable and dataset, the tool generates a graphical report highlighting all affected elements. This capability streamlines the refresh process, reduces manual effort, and ensures the accuracy and completeness of deliverables.

### **BayesERtools: R package for exposure-response analysis with Bayesian approaches**

**Kenta Yoshida**

Clinical Pharmacology Modeling & Simulation, Genentech

Exposure-response (ER) analysis is a critical component of clinical drug development for decisions such as dose selection. The new R package, BayesERtools (<https://genentech.github.io/BayesERtools/>), is designed to make Bayesian ER analysis more accessible. It provides a user-friendly interface for common tasks such as model development, simulation, and plotting, streamlining the entire workflow. The package currently supports linear and Emax models for continuous and binary endpoints. To further support users, we have also developed BayesERbook (<https://genentech.github.io/BayesERbook/>), a comprehensive online book that documents the workflow with typical examples. This open-source project, based on the Stan ecosystem, aims to expand the use of Bayesian methods in pharma, providing a powerful tool for quantitative decision-making in drug development.

### **Adapting to Regulatory Guidance: Covariate Adjustment and R-enabled Submissions**

**Alex Przybylski**

**Data scientist \@ Novartis**

FDA guidance released in 2023 advocates for covariate adjustment as a non-controversial means to enhance the efficiency of statistical analyses. This includes specific recommendations featuring recently proposed methods from academic groups, providing an opportunity for sponsors to realize the practical benefits of innovative approaches. However, they also present challenges for trial teams regarding practical implementation. Bridging the gap between academia, regulation and industry adoption is essential.

This talk presents a case study from Novartis, where FDA feedback referencing the new guidance prompted a targeted and strategic response. We will discuss how we responded to this feedback, our preparation for future regulatory expectations, and the role of R in enabling change and impact.

The case study will highlight lightweight R package solutions ({beeca}) and the work of the ASA-BIOP Covariate Adjustment Working Group ({RobinCar2}). We will discuss the importance of software that is suited for integration with trial analysis workflows in regulated environments and the benefits of collaborating within the open-source community.

### **R library validation using Acceptance-Test Driven Development**

**Brian Repko**

Retired (ex-Novartis Biomedical Research Oncology Data Science), Retired (ex-Novartis)

Europe/US

While unit tests are a key component of a single R package's quality, we need similar ways to test / validate a library of R packages or potentially Shiny applications. This session will walk through frameworks used for Acceptance-Test Driven Development (ATDD) and Behavior Driven Development (BDD) outside of R - cucumber, JBehave, etc. - how they work and their value-add to system quality. In short, tests are written in Quarto markdown, in a "given-when-then" format that is regex-matched against annotated functions that drive the testing and asserting of the system under test. The industry can write tests in plain-language and share those tests and feature code not only amongst themselves but also potentially as part of a validation effort with regulators. Feature code can also make use of packages like {chromote} to drive Shiny applications as well. This is bringing my experience as a contributor to JBehave to R and the pharmaverse.

### **WORKSHOP: R Validation Discussion: Metric Repos for Open Quality Assessment**

**Doug Kelkhoff**

Over the past year, the R Validation Hub has been hard at work to build out a Metric Repository - a pre-built database of metrics to support the software validation process. We'd like to invite you to discuss the industry outlook for this open validation database. How do we standardize our compute environments to make metrics useful? What do we do when a package isn't living up to our standards? What can we do to ensure that you can supplement our database with the validation of in-house packages? We'd love to share our answers and hear your thoughts to help guide our work. Come join us and discuss the future of R package validation.

Workshop related packages:

riskmetric, riskassessment, riskscore

Workshop related links:

<https://github.com/pharmaR/regulatory-r-repo-wg>

<https://github.com/pharmaR/val.meter>

<https://github.com/pharmaR/val.criterion>

### **WORKSHOP: datasetjson - Read and write CDISC Dataset JSON formatted datasets in R and Python**

Michael Stackhouse Chief Innovation Officer, Atorus Sam Hume Research Data Engineer \| Open Source \| Data Exchange Standards, CDSIC Nick Masel Associate Director Innovation Team Lead, Johnson & Johnson Eli Miller Senior Manager, Cloud Solutions, Atorus What? Join us for an engaging workshop designed to introduce Dataset-JSON, a powerful format for sharing datasets. We'll start with an environment setup and explore the motivation behind choosing Dataset-JSON over other formats like Parquet. The session will include a detailed walkthrough of the Dataset-JSON specification, followed by hands-on demonstrations and exercises in both R and Python.

Discover how to implement Dataset-JSON in your workflows, learn about upcoming adoption plans, and explore future roadmap and API integrations. This workshop is perfect for data professionals interested in improving dataset interoperability and sharing standards.

<https://atorus-research.github.io/datasetjson/>

<https://atorus-research.github.io/datasetjson_workshop/>

### **WORKSHOP: SDTM programming in R using {sdtm.oak} package**

**Rammprasad Ganapathy**

Principal Data Scientist

The {sdtm.oak} package is an EDC (Electronic Data Capture) and data standard-agnostic solution designed to empower the pharmaceutical programming community to develop CDISC SDTM datasets using R. By leveraging reusable algorithms, {sdtm. oak} offers a modular programming framework that can potentially automate the creation of SDTM datasets based on standard SDTM specifications. In this workshop, we will cover the fundamentals of the V0.1 {sdtm.oak} package and provide detailed guidance for programmers to begin utilizing it effectively.

Users will be given access to an R environment for the session. Users do not need to install R or other tools/packages prior to the session.

Workshop related link:

<https://pharmaverse.github.io/rinpharma-SDTM-workshop/#/title-slide>

### **WORKSHOP: Python for Clinical Study Report and Submission**

**Nan Xiao**

Statistician at Merck, Merck

**Yilong Zhang**

Biostatistician at Meta, Meta

Open-source programming languages are rapidly transforming drug discovery, research, and development, offering powerful capabilities for study design, data analysis, visualization, and clinical reporting. This workshop introduces practical strategies for using Python to prepare tables, listings, and figures (TLFs) in a clinical study report (CSR) and to assemble submission-ready electronic Common Technical Document (eCTD) packages that include both source code and deliverables.

This workshop is designed for clinical programmers, statisticians, and data scientists who are interested in exploring Python as an alternative approach for clinical trial analysis and reporting. Participants will gain hands-on experience with reproducible workflows, clinical data engineering, and end-to-end project management using the modern Python toolchain. By the end of the session, attendees will have a clear roadmap to start a Python project for clinical trial analysis and reporting.

The workshop is based on the open source book *Python for Clinical Study Reports and Submission* (<https://pycsr.org/>) and is organized into four modules:

1.  Python environment setup: Use uv to create and manage reproducible Python projects. Develop and collaborate in GitHub Codespaces, Visual Studio Code, or Positron.

2.  Python packages for clinical reporting: A guided tour of essential packages such as polars, plotnine, and rtflite, with demonstrations of creating TLFs commonly used in clinical trials.

3.  Manage a clinical trial A&R project: Practical project structure, conventions, and execution from data to deliverables.

4.  Preparing eCTD submission packages: An example workflow for assembling submission-ready source code and outputs using py-pkglite, aligned with eCTD requirements.

<https://pycsr.org/slides/workshop-slides.html#/datasets>

Publicly available CDISC pilot study data located at the CDISC GitHub repository.

The dataset structure follows the CDISC Analysis Data Model (ADAM).

Source data: <https://github.com/elong0527/r4csr/tree/main/data-adam>

Converted parquet data: <https://github.com/nanxstats/pycsr/tree/main/data>

Workshop Slides:

<https://pycsr.org/slides/workshop-slides.html#>

<https://github.com/nanxstats/pycsr>

### **WORKSHOP: Supercharge your shiny app by offloading computations to a HPC cluster**

**Michael Mayer**

Principal Solution Engineer at Posit , Posit PBC

With ever increasing complexity of data and analysis, application developers are tempted to put in serious computations into their shiny applications. Given that those shiny applications typically run on infrastructure that has certain resource limitations or sometimes even shared with other shiny apps, more often than not this leads to crashes affecting the overall stability of the system. As a consequence, either other approaches are pursued, or the approach simplified to the point that good science is prevented from happening. in this workshop you will learn how to interact with a remote HPC cluster straight from your laptop. You will run a shiny app locally and remote submit pieces of code to the HPC cluster. By leveraging a multiple of your locally available compute power for a short period of time, you will reduce  time-to-result considerably keeping the app fairly interactive. This approach also can be used to connect applications hosted on a Shiny Hosting platform such as Posit Connect to a HPC cluster. In the second part of the workshop you will be able to discuss with the workshop instructor(s) your own use cases and get insights on how to address those.

### **WORKSHOP: Bayesian Survival and Multistate Models using R and Stan**

**Eric Novik**

Founder & CEO \@ Generable, Generable

**Jacqueline Buros-Novik**

Generable

**Juho Timonen**

Computational Scientist, Generable

Bayesian inference and Stan offer many advantages for analyzing time-to-event data, including incorporating prior knowledge into the model, propagating all sources of uncertainty to produce well-calibrated predictions, and integrating arbitrary utility functions for optimal decision-making, such as patients' preferences for different types of risks. The latter point is particularly relevant in multistate models where people may differ in their preferences towards multiple competing [events.In](http://events.In) this workshop, we will briefly introduce Bayesian workflow – the typical steps in Bayesian analysis and basic (single-event) survival models in the Bayesian context. We will then proceed to introduce multistate models where we are tracking multiple event types, such as bleeding and stroke in cardiovascular trials, or stable disease, progressive disease, and death in oncology trials. Time permitting, we will demonstrate how to incorporate the patient’s utility function into a decision analysis.

Workshop related link:

<https://github.com/generable/bmstate>

### **WORKSHOP: From Data to Insights: A Hands-On Workshop with {teal} for Clinical Data Exploration**

**Nina Qi**

Principal Data Scientist at Genentech, Roche/Genentech

**Dony Unardi**

Data Scientist at Genentech, Roche/Genentech

{teal} is an innovative open-source R-Shiny framework that has transformed how clinical trial data is analyzed and visualized in recent years. By streamlining the creation of interactive web applications, it enables R programmers and data scientists to deliver insights faster while promoting efficiency, transparency, and reproducibility in data exploration. This hands-on workshop, built on the latest {teal} 1.0 release, will start from the basics and progressively cover practical topics for building {teal} applications. Together, we will explore key features of {teal} and work through step-by-step exercises designed to build confidence and proficiency in {teal} programming. No prior experience with {teal} or attendance at previous workshops is required - all R users are welcome. Designed to deepen participants’ understanding of the {teal} framework, this session will equip attendees with the skills to leverage the {teal} ecosystem to create scalable, reproducible applications that accelerate insight generation in clinical research.

Workshop related link:

<https://github.com/pharmaverse/tealworkshop-rinpharma2025/tree/main>

# Presentations

### \[**Open-Source Culture and Data Strategy in SHIONOGI: A New Value-Creation Model for Pharma**\]

-   \[**Yoshitake Kitanishi**\]

### \[**Strategically Assisting Statistical programmers To succeed in R (SAS2R)**\]

-   \[**KuenHung Lin**\]

### \[**Side-by-side by Design: Pharma Data Handling with Merge, Join, Match, and Hash in R**\]

-   \[**Yutaka Morioka**\]

-   \[**Yuki Nakagawa**\]

### \[**Risk Assessment Deep Dive**\]

-   \[**Ryo Nakaya**\]

### \[**Leverage template-based automated reporting on DMC materials preparation**\]

-   \[**Nina Han**\]

-   \[**Peng Zhang**\]

### \[**Dynamic Creation of Kaplan-Meier Plots and Summary Measure Tables for Survival Data with R Shiny**\]

-   \[**Takumi Imamura**\]

-   \[**Takahiro Hasegawa**\]

### \[**autoslideR: Streamlining slide deck generation for clinical reporting events**\]

-   \[**Yolanda Zhou**\]

-   \[**Joe Zhu**\]

### \[**From Harmony to Hybrid: Charting a Practical Course for R Adoption in Pharma**\]

-   \[**Joe Zhu**\]

### \[**Emerging trend of LLM development in R and implementation**\]

-   \[**Mia Chen**\]

-   \[**Peng Zhang**\]

### \[**An R package to consolidate pretest probability models and guidelines for CAD**\]

-   \[**Jeremy Selva**\]

### \[**Enhancing Efficiency in e-CRT Creation for PMDA Through R Shiny App Development Using Vibe Coding**\]

-   \[**Naoki Yoshida**\]

### \[**{meRlin} - context-aware AI assistant for clinical programming**\]

-   \[**Steven Brooks**\]

-   \[**Pietro Mascheroni**\]

-   \[**Xiecheng Gu**\]

### \[**Improving precision healthcare for under-represented and genetically diverse global populations**\]

-   \[**Jimmy Breen**\]

### \[**Exploring AI tools in clinical trial data analysis**\]

-   \[**Terry Zhang**\]

### \[**An R Package cgmguru for Automated Glycemic Event Detection From Continuous Glucose Monitoring Data**\]

-   \[**Sang Ho Park**\]

### \[**Interactive and Reproducible Reports with Quarto**\]

-   \[**Jaspreet Pabla**\]

### \[**Using Analysis Results Data using {cards} for PMDA Oncology inqueries**\]

-   \[**Shunsuke Goto**\]

### \[**Reach for R Low Hanging Fruit for Faster Results**\]

-   \[**Sunil Gupta**\]

### \[**MedxR Package - Bridging Regulatory Drug Data from the FDA and Health Canada into R**\]

-   \[**Renzo Cáceres Rossi**\]

### \[**risk.assessr: extending its use in the package validation process**\]

-   \[**Hugo Bottois**\]

### \[**Code Review for Compliance: Best Practices for Validated R Workflows in Pharma**\]

-   \[**Alexandros Kouretsis**\]

### \[**A Web-Based R Application for Forecasting Patient Enrollment in Clinical Trials**\]

-   \[**Akifumi Okayama**\]

-   \[**Motoki Oe**\]

-   \[**Nobushige Matsuoka**\]